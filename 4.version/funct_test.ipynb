{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\learning\\Resume Analyser\\4.version\n"
     ]
    }
   ],
   "source": [
    "cd learning/Resume Analyser/4.version/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Volume in drive C has no label.\n",
      " Volume Serial Number is 7010-C44B\n",
      "\n",
      " Directory of C:\\Users\\Lenovo\\learning\\Resume Analyser\\4.version\n",
      "\n",
      "11/20/2024  01:31 PM    <DIR>          .\n",
      "11/04/2024  08:36 PM    <DIR>          ..\n",
      "11/20/2024  06:34 AM    <DIR>          .ipynb_checkpoints\n",
      "11/20/2024  06:33 AM    <DIR>          __pycache__\n",
      "11/19/2024  02:12 PM             2,031 constants.py\n",
      "11/20/2024  01:12 PM            76,468 funct_test.ipynb\n",
      "11/11/2024  07:34 PM            23,719 HR_analytics.ipynb\n",
      "11/11/2024  07:34 PM         5,213,294 job_final.csv\n",
      "11/04/2024  08:38 PM    <DIR>          marker\n",
      "11/20/2024  01:31 PM               411 prerequisite.py\n",
      "11/19/2024  02:32 PM             3,673 pyresparser.py\n",
      "11/20/2024  01:30 PM               103 requirements.txt\n",
      "11/20/2024  12:39 PM                68 skills.csv\n",
      "11/14/2024  10:13 PM    <DIR>          trainning\n",
      "11/19/2024  02:29 PM            16,227 utils.py\n",
      "               9 File(s)      5,335,994 bytes\n",
      "               6 Dir(s)  193,583,902,720 bytes free\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 33968,
     "status": "ok",
     "timestamp": 1731902950171,
     "user": {
      "displayName": "Giang Le Huynh",
      "userId": "12580968046014402630"
     },
     "user_tz": -420
    },
    "id": "iR8NbgiB9yuA",
    "outputId": "976c1300-b9d9-4618-ee8b-6f3f482e96be",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spacy==3.7.6 in c:\\users\\lenovo\\anaconda3\\envs\\resumeparser\\lib\\site-packages (from -r requirements.txt (line 1)) (3.7.6)\n",
      "Collecting nltk==3.2.4 (from -r requirements.txt (line 2))\n",
      "  Using cached nltk-3.2.4.tar.gz (1.2 MB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting docx2txt (from -r requirements.txt (line 3))\n",
      "  Using cached docx2txt-0.8.tar.gz (2.8 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting spacy_transformers==1.3.5 (from -r requirements.txt (line 4))\n",
      "  Downloading spacy_transformers-1.3.5-cp310-cp310-win_amd64.whl.metadata (7.2 kB)\n",
      "Collecting pdfminer.six==20240706 (from -r requirements.txt (line 5))\n",
      "  Using cached pdfminer.six-20240706-py3-none-any.whl.metadata (4.1 kB)\n",
      "Collecting pandas==2.2.2 (from -r requirements.txt (line 6))\n",
      "  Downloading pandas-2.2.2-cp310-cp310-win_amd64.whl.metadata (19 kB)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\users\\lenovo\\anaconda3\\envs\\resumeparser\\lib\\site-packages (from spacy==3.7.6->-r requirements.txt (line 1)) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\lenovo\\anaconda3\\envs\\resumeparser\\lib\\site-packages (from spacy==3.7.6->-r requirements.txt (line 1)) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\lenovo\\anaconda3\\envs\\resumeparser\\lib\\site-packages (from spacy==3.7.6->-r requirements.txt (line 1)) (1.0.10)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\lenovo\\anaconda3\\envs\\resumeparser\\lib\\site-packages (from spacy==3.7.6->-r requirements.txt (line 1)) (2.0.8)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\lenovo\\anaconda3\\envs\\resumeparser\\lib\\site-packages (from spacy==3.7.6->-r requirements.txt (line 1)) (3.0.9)\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in c:\\users\\lenovo\\anaconda3\\envs\\resumeparser\\lib\\site-packages (from spacy==3.7.6->-r requirements.txt (line 1)) (8.2.5)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\users\\lenovo\\anaconda3\\envs\\resumeparser\\lib\\site-packages (from spacy==3.7.6->-r requirements.txt (line 1)) (1.1.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\lenovo\\anaconda3\\envs\\resumeparser\\lib\\site-packages (from spacy==3.7.6->-r requirements.txt (line 1)) (2.4.8)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\lenovo\\anaconda3\\envs\\resumeparser\\lib\\site-packages (from spacy==3.7.6->-r requirements.txt (line 1)) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in c:\\users\\lenovo\\anaconda3\\envs\\resumeparser\\lib\\site-packages (from spacy==3.7.6->-r requirements.txt (line 1)) (0.4.1)\n",
      "Requirement already satisfied: typer<1.0.0,>=0.3.0 in c:\\users\\lenovo\\anaconda3\\envs\\resumeparser\\lib\\site-packages (from spacy==3.7.6->-r requirements.txt (line 1)) (0.13.1)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\lenovo\\anaconda3\\envs\\resumeparser\\lib\\site-packages (from spacy==3.7.6->-r requirements.txt (line 1)) (4.67.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\lenovo\\anaconda3\\envs\\resumeparser\\lib\\site-packages (from spacy==3.7.6->-r requirements.txt (line 1)) (2.32.3)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in c:\\users\\lenovo\\anaconda3\\envs\\resumeparser\\lib\\site-packages (from spacy==3.7.6->-r requirements.txt (line 1)) (2.9.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\lenovo\\anaconda3\\envs\\resumeparser\\lib\\site-packages (from spacy==3.7.6->-r requirements.txt (line 1)) (3.1.4)\n",
      "Requirement already satisfied: setuptools in c:\\users\\lenovo\\anaconda3\\envs\\resumeparser\\lib\\site-packages (from spacy==3.7.6->-r requirements.txt (line 1)) (75.1.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\lenovo\\anaconda3\\envs\\resumeparser\\lib\\site-packages (from spacy==3.7.6->-r requirements.txt (line 1)) (24.1)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\lenovo\\anaconda3\\envs\\resumeparser\\lib\\site-packages (from spacy==3.7.6->-r requirements.txt (line 1)) (3.5.0)\n",
      "Requirement already satisfied: numpy>=1.19.0 in c:\\users\\lenovo\\anaconda3\\envs\\resumeparser\\lib\\site-packages (from spacy==3.7.6->-r requirements.txt (line 1)) (1.26.4)\n",
      "Requirement already satisfied: six in c:\\users\\lenovo\\anaconda3\\envs\\resumeparser\\lib\\site-packages (from nltk==3.2.4->-r requirements.txt (line 2)) (1.16.0)\n",
      "Collecting transformers<4.37.0,>=3.4.0 (from spacy_transformers==1.3.5->-r requirements.txt (line 4))\n",
      "  Using cached transformers-4.36.2-py3-none-any.whl.metadata (126 kB)\n",
      "Collecting torch>=1.8.0 (from spacy_transformers==1.3.5->-r requirements.txt (line 4))\n",
      "  Downloading torch-2.5.1-cp310-cp310-win_amd64.whl.metadata (28 kB)\n",
      "Collecting spacy-alignments<1.0.0,>=0.7.2 (from spacy_transformers==1.3.5->-r requirements.txt (line 4))\n",
      "  Downloading spacy_alignments-0.9.1-cp310-cp310-win_amd64.whl.metadata (2.7 kB)\n",
      "Requirement already satisfied: charset-normalizer>=2.0.0 in c:\\users\\lenovo\\anaconda3\\envs\\resumeparser\\lib\\site-packages (from pdfminer.six==20240706->-r requirements.txt (line 5)) (3.3.2)\n",
      "Collecting cryptography>=36.0.0 (from pdfminer.six==20240706->-r requirements.txt (line 5))\n",
      "  Using cached cryptography-43.0.3-cp39-abi3-win_amd64.whl.metadata (5.4 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\lenovo\\anaconda3\\envs\\resumeparser\\lib\\site-packages (from pandas==2.2.2->-r requirements.txt (line 6)) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\lenovo\\anaconda3\\envs\\resumeparser\\lib\\site-packages (from pandas==2.2.2->-r requirements.txt (line 6)) (2024.1)\n",
      "Collecting tzdata>=2022.7 (from pandas==2.2.2->-r requirements.txt (line 6))\n",
      "  Using cached tzdata-2024.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: cffi>=1.12 in c:\\users\\lenovo\\anaconda3\\envs\\resumeparser\\lib\\site-packages (from cryptography>=36.0.0->pdfminer.six==20240706->-r requirements.txt (line 5)) (1.17.1)\n",
      "Requirement already satisfied: language-data>=1.2 in c:\\users\\lenovo\\anaconda3\\envs\\resumeparser\\lib\\site-packages (from langcodes<4.0.0,>=3.2.0->spacy==3.7.6->-r requirements.txt (line 1)) (1.3.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\lenovo\\anaconda3\\envs\\resumeparser\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy==3.7.6->-r requirements.txt (line 1)) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in c:\\users\\lenovo\\anaconda3\\envs\\resumeparser\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy==3.7.6->-r requirements.txt (line 1)) (2.23.4)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in c:\\users\\lenovo\\anaconda3\\envs\\resumeparser\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy==3.7.6->-r requirements.txt (line 1)) (4.11.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\lenovo\\anaconda3\\envs\\resumeparser\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy==3.7.6->-r requirements.txt (line 1)) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\lenovo\\anaconda3\\envs\\resumeparser\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy==3.7.6->-r requirements.txt (line 1)) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\lenovo\\anaconda3\\envs\\resumeparser\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy==3.7.6->-r requirements.txt (line 1)) (2024.8.30)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in c:\\users\\lenovo\\anaconda3\\envs\\resumeparser\\lib\\site-packages (from thinc<8.3.0,>=8.2.2->spacy==3.7.6->-r requirements.txt (line 1)) (0.7.11)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\lenovo\\anaconda3\\envs\\resumeparser\\lib\\site-packages (from thinc<8.3.0,>=8.2.2->spacy==3.7.6->-r requirements.txt (line 1)) (0.1.5)\n",
      "Collecting filelock (from torch>=1.8.0->spacy_transformers==1.3.5->-r requirements.txt (line 4))\n",
      "  Using cached filelock-3.16.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting networkx (from torch>=1.8.0->spacy_transformers==1.3.5->-r requirements.txt (line 4))\n",
      "  Using cached networkx-3.4.2-py3-none-any.whl.metadata (6.3 kB)\n",
      "Collecting fsspec (from torch>=1.8.0->spacy_transformers==1.3.5->-r requirements.txt (line 4))\n",
      "  Using cached fsspec-2024.10.0-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting sympy==1.13.1 (from torch>=1.8.0->spacy_transformers==1.3.5->-r requirements.txt (line 4))\n",
      "  Using cached sympy-1.13.1-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting mpmath<1.4,>=1.1.0 (from sympy==1.13.1->torch>=1.8.0->spacy_transformers==1.3.5->-r requirements.txt (line 4))\n",
      "  Using cached mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\lenovo\\anaconda3\\envs\\resumeparser\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy==3.7.6->-r requirements.txt (line 1)) (0.4.6)\n",
      "Collecting huggingface-hub<1.0,>=0.19.3 (from transformers<4.37.0,>=3.4.0->spacy_transformers==1.3.5->-r requirements.txt (line 4))\n",
      "  Using cached huggingface_hub-0.26.2-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\lenovo\\anaconda3\\envs\\resumeparser\\lib\\site-packages (from transformers<4.37.0,>=3.4.0->spacy_transformers==1.3.5->-r requirements.txt (line 4)) (6.0.2)\n",
      "Collecting regex!=2019.12.17 (from transformers<4.37.0,>=3.4.0->spacy_transformers==1.3.5->-r requirements.txt (line 4))\n",
      "  Downloading regex-2024.11.6-cp310-cp310-win_amd64.whl.metadata (41 kB)\n",
      "Collecting tokenizers<0.19,>=0.14 (from transformers<4.37.0,>=3.4.0->spacy_transformers==1.3.5->-r requirements.txt (line 4))\n",
      "  Downloading tokenizers-0.15.2-cp310-none-win_amd64.whl.metadata (6.8 kB)\n",
      "Collecting safetensors>=0.3.1 (from transformers<4.37.0,>=3.4.0->spacy_transformers==1.3.5->-r requirements.txt (line 4))\n",
      "  Downloading safetensors-0.4.5-cp310-none-win_amd64.whl.metadata (3.9 kB)\n",
      "Requirement already satisfied: click>=8.0.0 in c:\\users\\lenovo\\anaconda3\\envs\\resumeparser\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy==3.7.6->-r requirements.txt (line 1)) (8.1.7)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in c:\\users\\lenovo\\anaconda3\\envs\\resumeparser\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy==3.7.6->-r requirements.txt (line 1)) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in c:\\users\\lenovo\\anaconda3\\envs\\resumeparser\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy==3.7.6->-r requirements.txt (line 1)) (13.9.4)\n",
      "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in c:\\users\\lenovo\\anaconda3\\envs\\resumeparser\\lib\\site-packages (from weasel<0.5.0,>=0.1.0->spacy==3.7.6->-r requirements.txt (line 1)) (0.20.0)\n",
      "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in c:\\users\\lenovo\\anaconda3\\envs\\resumeparser\\lib\\site-packages (from weasel<0.5.0,>=0.1.0->spacy==3.7.6->-r requirements.txt (line 1)) (7.0.5)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\lenovo\\anaconda3\\envs\\resumeparser\\lib\\site-packages (from jinja2->spacy==3.7.6->-r requirements.txt (line 1)) (2.1.3)\n",
      "Requirement already satisfied: pycparser in c:\\users\\lenovo\\anaconda3\\envs\\resumeparser\\lib\\site-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six==20240706->-r requirements.txt (line 5)) (2.21)\n",
      "Requirement already satisfied: marisa-trie>=1.1.0 in c:\\users\\lenovo\\anaconda3\\envs\\resumeparser\\lib\\site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy==3.7.6->-r requirements.txt (line 1)) (1.2.1)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\lenovo\\anaconda3\\envs\\resumeparser\\lib\\site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy==3.7.6->-r requirements.txt (line 1)) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\lenovo\\anaconda3\\envs\\resumeparser\\lib\\site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy==3.7.6->-r requirements.txt (line 1)) (2.15.1)\n",
      "Requirement already satisfied: wrapt in c:\\users\\lenovo\\anaconda3\\envs\\resumeparser\\lib\\site-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy==3.7.6->-r requirements.txt (line 1)) (1.16.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\lenovo\\anaconda3\\envs\\resumeparser\\lib\\site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy==3.7.6->-r requirements.txt (line 1)) (0.1.2)\n",
      "Downloading spacy_transformers-1.3.5-cp310-cp310-win_amd64.whl (343 kB)\n",
      "Using cached pdfminer.six-20240706-py3-none-any.whl (5.6 MB)\n",
      "Downloading pandas-2.2.2-cp310-cp310-win_amd64.whl (11.6 MB)\n",
      "   ---------------------------------------- 0.0/11.6 MB ? eta -:--:--\n",
      "   -------------- ------------------------- 4.2/11.6 MB 19.4 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 11.3/11.6 MB 27.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 11.6/11.6 MB 26.9 MB/s eta 0:00:00\n",
      "Using cached cryptography-43.0.3-cp39-abi3-win_amd64.whl (3.1 MB)\n",
      "Downloading spacy_alignments-0.9.1-cp310-cp310-win_amd64.whl (187 kB)\n",
      "Downloading torch-2.5.1-cp310-cp310-win_amd64.whl (203.1 MB)\n",
      "   ---------------------------------------- 0.0/203.1 MB ? eta -:--:--\n",
      "   - -------------------------------------- 7.6/203.1 MB 36.2 MB/s eta 0:00:06\n",
      "   -- ------------------------------------- 14.4/203.1 MB 34.9 MB/s eta 0:00:06\n",
      "   ---- ----------------------------------- 22.3/203.1 MB 36.2 MB/s eta 0:00:06\n",
      "   ----- ---------------------------------- 29.1/203.1 MB 35.5 MB/s eta 0:00:05\n",
      "   ------ --------------------------------- 34.1/203.1 MB 32.8 MB/s eta 0:00:06\n",
      "   -------- ------------------------------- 41.2/203.1 MB 32.7 MB/s eta 0:00:05\n",
      "   --------- ------------------------------ 47.4/203.1 MB 32.5 MB/s eta 0:00:05\n",
      "   ---------- ----------------------------- 54.0/203.1 MB 32.5 MB/s eta 0:00:05\n",
      "   ----------- ---------------------------- 60.6/203.1 MB 32.4 MB/s eta 0:00:05\n",
      "   ------------- -------------------------- 67.6/203.1 MB 32.7 MB/s eta 0:00:05\n",
      "   -------------- ------------------------- 73.9/203.1 MB 32.3 MB/s eta 0:00:04\n",
      "   --------------- ------------------------ 79.7/203.1 MB 32.0 MB/s eta 0:00:04\n",
      "   ---------------- ----------------------- 85.7/203.1 MB 31.8 MB/s eta 0:00:04\n",
      "   ------------------ --------------------- 93.6/203.1 MB 32.1 MB/s eta 0:00:04\n",
      "   ------------------- ------------------- 100.1/203.1 MB 32.1 MB/s eta 0:00:04\n",
      "   -------------------- ------------------ 104.9/203.1 MB 31.7 MB/s eta 0:00:04\n",
      "   --------------------- ----------------- 111.7/203.1 MB 31.7 MB/s eta 0:00:03\n",
      "   ---------------------- ---------------- 117.4/203.1 MB 31.5 MB/s eta 0:00:03\n",
      "   ------------------------ -------------- 126.1/203.1 MB 32.1 MB/s eta 0:00:03\n",
      "   ------------------------- ------------- 132.9/203.1 MB 32.2 MB/s eta 0:00:03\n",
      "   -------------------------- ------------ 140.0/203.1 MB 32.4 MB/s eta 0:00:02\n",
      "   ---------------------------- ---------- 146.8/203.1 MB 32.2 MB/s eta 0:00:02\n",
      "   ----------------------------- --------- 153.9/203.1 MB 32.3 MB/s eta 0:00:02\n",
      "   ------------------------------- ------- 162.5/203.1 MB 32.7 MB/s eta 0:00:02\n",
      "   -------------------------------- ------ 171.2/203.1 MB 33.0 MB/s eta 0:00:01\n",
      "   ---------------------------------- ---- 180.6/203.1 MB 33.5 MB/s eta 0:00:01\n",
      "   ------------------------------------ -- 189.3/203.1 MB 33.8 MB/s eta 0:00:01\n",
      "   ------------------------------------- - 197.7/203.1 MB 34.1 MB/s eta 0:00:01\n",
      "   --------------------------------------  202.9/203.1 MB 34.1 MB/s eta 0:00:01\n",
      "   --------------------------------------- 203.1/203.1 MB 32.8 MB/s eta 0:00:00\n",
      "Using cached sympy-1.13.1-py3-none-any.whl (6.2 MB)\n",
      "Using cached transformers-4.36.2-py3-none-any.whl (8.2 MB)\n",
      "Using cached tzdata-2024.2-py2.py3-none-any.whl (346 kB)\n",
      "Using cached huggingface_hub-0.26.2-py3-none-any.whl (447 kB)\n",
      "Using cached fsspec-2024.10.0-py3-none-any.whl (179 kB)\n",
      "Downloading regex-2024.11.6-cp310-cp310-win_amd64.whl (274 kB)\n",
      "Downloading safetensors-0.4.5-cp310-none-win_amd64.whl (285 kB)\n",
      "Downloading tokenizers-0.15.2-cp310-none-win_amd64.whl (2.2 MB)\n",
      "   ---------------------------------------- 0.0/2.2 MB ? eta -:--:--\n",
      "   ---------------------------------------- 2.2/2.2 MB 31.1 MB/s eta 0:00:00\n",
      "Using cached filelock-3.16.1-py3-none-any.whl (16 kB)\n",
      "Using cached networkx-3.4.2-py3-none-any.whl (1.7 MB)\n",
      "Using cached mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "Building wheels for collected packages: nltk, docx2txt\n",
      "  Building wheel for nltk (setup.py): started\n",
      "  Building wheel for nltk (setup.py): finished with status 'done'\n",
      "  Created wheel for nltk: filename=nltk-3.2.4-py3-none-any.whl size=1367720 sha256=16e4615f8d572d2c941aa76af416f9d1500669c5e6012a1b07544d9c86c2ae75\n",
      "  Stored in directory: c:\\users\\lenovo\\appdata\\local\\pip\\cache\\wheels\\0e\\8c\\42\\bcd0934b61ecf4cef964ccc9881888cca0841ec72266e99de1\n",
      "  Building wheel for docx2txt (setup.py): started\n",
      "  Building wheel for docx2txt (setup.py): finished with status 'done'\n",
      "  Created wheel for docx2txt: filename=docx2txt-0.8-py3-none-any.whl size=3973 sha256=5939034be9e567a9281709ccfcec05c4bbe94ddbccb9eaffac924a06847321ef\n",
      "  Stored in directory: c:\\users\\lenovo\\appdata\\local\\pip\\cache\\wheels\\22\\58\\cf\\093d0a6c3ecfdfc5f6ddd5524043b88e59a9a199cb02352966\n",
      "Successfully built nltk docx2txt\n",
      "Installing collected packages: mpmath, docx2txt, tzdata, sympy, spacy-alignments, safetensors, regex, nltk, networkx, fsspec, filelock, torch, pandas, huggingface-hub, cryptography, tokenizers, pdfminer.six, transformers, spacy_transformers\n",
      "Successfully installed cryptography-43.0.3 docx2txt-0.8 filelock-3.16.1 fsspec-2024.10.0 huggingface-hub-0.26.2 mpmath-1.3.0 networkx-3.4.2 nltk-3.2.4 pandas-2.2.2 pdfminer.six-20240706 regex-2024.11.6 safetensors-0.4.5 spacy-alignments-0.9.1 spacy_transformers-1.3.5 sympy-1.13.1 tokenizers-0.15.2 torch-2.5.1 transformers-4.36.2 tzdata-2024.2\n"
     ]
    }
   ],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     C:\\Users\\Lenovo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\Lenovo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Lenovo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Lenovo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Lenovo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Lenovo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "%run prerequisite.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-sm==3.7.1\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.7.1/en_core_web_sm-3.7.1-py3-none-any.whl (12.8 MB)\n",
      "     ---------------------------------------- 0.0/12.8 MB ? eta -:--:--\n",
      "     -- ------------------------------------- 0.8/12.8 MB 8.5 MB/s eta 0:00:02\n",
      "     --------- ------------------------------ 2.9/12.8 MB 9.9 MB/s eta 0:00:02\n",
      "     ---------------------------- ----------- 9.2/12.8 MB 18.4 MB/s eta 0:00:01\n",
      "     --------------------------------------- 12.8/12.8 MB 20.6 MB/s eta 0:00:00\n",
      "Requirement already satisfied: spacy<3.8.0,>=3.7.2 in c:\\users\\lenovo\\anaconda3\\envs\\resumeparser\\lib\\site-packages (from en-core-web-sm==3.7.1) (3.7.6)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\users\\lenovo\\anaconda3\\envs\\resumeparser\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\lenovo\\anaconda3\\envs\\resumeparser\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\lenovo\\anaconda3\\envs\\resumeparser\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.10)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\lenovo\\anaconda3\\envs\\resumeparser\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.8)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\lenovo\\anaconda3\\envs\\resumeparser\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.9)\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in c:\\users\\lenovo\\anaconda3\\envs\\resumeparser\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.2.5)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\users\\lenovo\\anaconda3\\envs\\resumeparser\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.1.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\lenovo\\anaconda3\\envs\\resumeparser\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.4.8)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\lenovo\\anaconda3\\envs\\resumeparser\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in c:\\users\\lenovo\\anaconda3\\envs\\resumeparser\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.4.1)\n",
      "Requirement already satisfied: typer<1.0.0,>=0.3.0 in c:\\users\\lenovo\\anaconda3\\envs\\resumeparser\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.13.1)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\lenovo\\anaconda3\\envs\\resumeparser\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.67.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\lenovo\\anaconda3\\envs\\resumeparser\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.32.3)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in c:\\users\\lenovo\\anaconda3\\envs\\resumeparser\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.9.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\lenovo\\anaconda3\\envs\\resumeparser\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.1.4)\n",
      "Requirement already satisfied: setuptools in c:\\users\\lenovo\\anaconda3\\envs\\resumeparser\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (75.1.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\lenovo\\anaconda3\\envs\\resumeparser\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (24.1)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\lenovo\\anaconda3\\envs\\resumeparser\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.5.0)\n",
      "Requirement already satisfied: numpy>=1.19.0 in c:\\users\\lenovo\\anaconda3\\envs\\resumeparser\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.26.4)\n",
      "Requirement already satisfied: language-data>=1.2 in c:\\users\\lenovo\\anaconda3\\envs\\resumeparser\\lib\\site-packages (from langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.3.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\lenovo\\anaconda3\\envs\\resumeparser\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in c:\\users\\lenovo\\anaconda3\\envs\\resumeparser\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.23.4)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in c:\\users\\lenovo\\anaconda3\\envs\\resumeparser\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.11.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\lenovo\\anaconda3\\envs\\resumeparser\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\lenovo\\anaconda3\\envs\\resumeparser\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\lenovo\\anaconda3\\envs\\resumeparser\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\lenovo\\anaconda3\\envs\\resumeparser\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2024.8.30)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in c:\\users\\lenovo\\anaconda3\\envs\\resumeparser\\lib\\site-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.11)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\lenovo\\anaconda3\\envs\\resumeparser\\lib\\site-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.5)\n",
      "Requirement already satisfied: colorama in c:\\users\\lenovo\\anaconda3\\envs\\resumeparser\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.4.6)\n",
      "Requirement already satisfied: click>=8.0.0 in c:\\users\\lenovo\\anaconda3\\envs\\resumeparser\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.1.7)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in c:\\users\\lenovo\\anaconda3\\envs\\resumeparser\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in c:\\users\\lenovo\\anaconda3\\envs\\resumeparser\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (13.9.4)\n",
      "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in c:\\users\\lenovo\\anaconda3\\envs\\resumeparser\\lib\\site-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.20.0)\n",
      "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in c:\\users\\lenovo\\anaconda3\\envs\\resumeparser\\lib\\site-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (7.0.5)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\lenovo\\anaconda3\\envs\\resumeparser\\lib\\site-packages (from jinja2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.1.3)\n",
      "Requirement already satisfied: marisa-trie>=1.1.0 in c:\\users\\lenovo\\anaconda3\\envs\\resumeparser\\lib\\site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.2.1)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\lenovo\\anaconda3\\envs\\resumeparser\\lib\\site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\lenovo\\anaconda3\\envs\\resumeparser\\lib\\site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.15.1)\n",
      "Requirement already satisfied: wrapt in c:\\users\\lenovo\\anaconda3\\envs\\resumeparser\\lib\\site-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.16.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\lenovo\\anaconda3\\envs\\resumeparser\\lib\\site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.2)\n",
      "Installing collected packages: en-core-web-sm\n",
      "Successfully installed en-core-web-sm-3.7.1\n"
     ]
    }
   ],
   "source": [
    "!pip install https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.7.1/en_core_web_sm-3.7.1-py3-none-any.whl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import constants as cs\n",
    "\n",
    "from spacy.matcher import Matcher\n",
    "import spacy\n",
    "\n",
    "import re\n",
    "from collections import defaultdict\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WrfB4co9dXen"
   },
   "source": [
    "## Extract name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 530,
     "status": "ok",
     "timestamp": 1731905651556,
     "user": {
      "displayName": "Giang Le Huynh",
      "userId": "12580968046014402630"
     },
     "user_tz": -420
    },
    "id": "t9IQHf-8nuB8",
    "outputId": "e7a7ffbd-f5b0-4d1d-9f42-c3ebd32a32af"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'POS': 'PROPN'}]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pattern = cs.NAME_PATTERN\n",
    "pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 340,
     "status": "ok",
     "timestamp": 1731905652231,
     "user": {
      "displayName": "Giang Le Huynh",
      "userId": "12580968046014402630"
     },
     "user_tz": -420
    },
    "id": "QwU_VDgH4P_t",
    "outputId": "b6224fc2-4990-4677-c5a2-ad9b50e20138"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['John Doe', 'Scatter William Smith']\n"
     ]
    }
   ],
   "source": [
    "def extract_name(doc, matcher):\n",
    "    '''\n",
    "    Helper function to extract name from spacy nlp text\n",
    "\n",
    "    :param nlp_text: object of `spacy.tokens.doc.Doc`\n",
    "    :param matcher: object of `spacy.matcher.Matcher`\n",
    "    :return: string of full name\n",
    "    '''\n",
    "    # Add a pattern to match proper nouns (PROPN)\n",
    "    pattern = cs.NAME_PATTERN\n",
    "    matcher.add('NAME', [pattern])\n",
    "\n",
    "    # Apply the matcher to the doc\n",
    "    matches = matcher(doc)\n",
    "\n",
    "    # List to store full names\n",
    "    full_names = []\n",
    "\n",
    "    # Temporary variable to track the end position of the last matched name\n",
    "    last_end_index = -1\n",
    "\n",
    "    # Iterate over the matches\n",
    "    for match_id, start, end in matches:\n",
    "        # If the current match is contiguous with the last match, append the name\n",
    "        if start == last_end_index:\n",
    "            full_names[-1] += \" \" + doc[start:end].text  # Merge the names\n",
    "        else:\n",
    "            full_names.append(doc[start:end].text)  # Add a new name\n",
    "\n",
    "        # Update the last_end_index to the end of the current match\n",
    "        last_end_index = end\n",
    "\n",
    "    return full_names\n",
    "\n",
    "\n",
    "text = \"John Doe is a software developer. And Scatter William Smith is an computer engineering.\"\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "matchers = Matcher(nlp.vocab)\n",
    "full_names = extract_name(nlp(text), matcher=matchers)\n",
    "print(full_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 347,
     "status": "ok",
     "timestamp": 1731903245924,
     "user": {
      "displayName": "Giang Le Huynh",
      "userId": "12580968046014402630"
     },
     "user_tz": -420
    },
    "id": "F8OUwQUAeG-5",
    "outputId": "5dcb52aa-1172-41ec-8940-75cc12b2d069"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(424143666773229379, 0, 1),\n",
       " (424143666773229379, 1, 2),\n",
       " (424143666773229379, 8, 9),\n",
       " (424143666773229379, 10, 11)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"John Doe is a software developer. And Scatter Will Smith is an computer engineering.\"\n",
    "matchers = Matcher(nlp.vocab)\n",
    "matchers.add('NAME', [[{'POS': 'PROPN'}]])\n",
    "matchers(nlp(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q2-Q2hoye3oJ"
   },
   "source": [
    "## Extract link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 342,
     "status": "ok",
     "timestamp": 1731906989351,
     "user": {
      "displayName": "Giang Le Huynh",
      "userId": "12580968046014402630"
     },
     "user_tz": -420
    },
    "id": "wp0up6rUm2Lw",
    "outputId": "edaffdd4-3895-4c0d-dc46-892b18e693e8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://example.com\n",
      "https://mywebsite.org/about\n",
      "https://subdomain.example.net/products?category=books\n",
      "https://another-site.co.uk/contact?ref=home\n",
      "ftp://fileserver.com/path/to/file\n"
     ]
    }
   ],
   "source": [
    "# Example text containing domain names with paths and some with protocols\n",
    "text = \"\"\"\n",
    "Here are some domain names with and without protocols:\n",
    "http://example.com\n",
    "https://mywebsite.org/about\n",
    "subdomain.example.net/products?category=books\n",
    "another-site.co.uk/contact?ref=home\n",
    "ftp://fileserver.com/path/to/file\n",
    "\"\"\"\n",
    "\n",
    "# Regular expression to match domain names with optional paths or query parameters\n",
    "domain_with_path_pattern = r'\\b(?:[A-Za-z][A-Za-z0-9+.-]*://)?[A-Za-z0-9.-]+\\.[a-zA-Z]{2,}(?:/[A-Za-z0-9&%_\\-.?=]*)*\\b'\n",
    "\n",
    "# Extracting all domain names with or without protocols\n",
    "domains_with_paths = re.findall(domain_with_path_pattern, text)\n",
    "\n",
    "# Prepare the final list with https:// if no protocol exists\n",
    "final_links = []\n",
    "for link in domains_with_paths:\n",
    "    # If the link doesn't have a protocol (i.e., it doesn't start with 'http://', 'https://', etc.)\n",
    "    if not re.match(r'^[a-zA-Z][a-zA-Z0-9+.-]*://', link):  # No protocol\n",
    "        link = 'https://' + link  # Add https:// as the default protocol\n",
    "    final_links.append(link)\n",
    "\n",
    "# Print the final list of links\n",
    "for link in final_links:\n",
    "    print(link)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 332
    },
    "executionInfo": {
     "elapsed": 381,
     "status": "error",
     "timestamp": 1731912246481,
     "user": {
      "displayName": "Giang Le Huynh",
      "userId": "12580968046014402630"
     },
     "user_tz": -420
    },
    "id": "IJTHB4jvsKMy",
    "outputId": "6b7ec85b-6bda-4012-ba9c-ba7563e3ab7b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "example.com:\n",
      "  http://example.com\n",
      "mywebsite.org:\n",
      "  https://mywebsite.org/about\n",
      "subdomain.example.net:\n",
      "  https://subdomain.example.net/products?category=books\n",
      "another-site.co.uk:\n",
      "  https://another-site.co.uk/contact?ref=home\n",
      "fileserver.com:\n",
      "  ftp://fileserver.com/path/to/file\n"
     ]
    }
   ],
   "source": [
    "def extract_links(text):\n",
    "    # Regular expression to match domain names with optional protocols and paths\n",
    "    domain_with_path_pattern = cs.LINK_PATTERN\n",
    "    # Extract all the links (with or without protocol)\n",
    "    links = re.findall(domain_with_path_pattern, text)\n",
    "\n",
    "    # Dictionary to hold domain as key and list of links as value\n",
    "    domain_dict = defaultdict(list)\n",
    "\n",
    "    # Iterate through each link\n",
    "    for link in links:\n",
    "        # If the link doesn't have a protocol, add https:// by default\n",
    "        if not re.match(r'^[a-zA-Z][a-zA-Z0-9+.-]*://', link):  # No protocol\n",
    "            link = 'https://' + link\n",
    "\n",
    "        # Parse the link to get the domain name (without protocol)\n",
    "        parsed_url = urlparse(link)\n",
    "        domain_name = parsed_url.netloc\n",
    "\n",
    "        # Append the link to the list of the corresponding domain\n",
    "        domain_dict[domain_name].append(link)\n",
    "\n",
    "    return domain_dict\n",
    "\n",
    "# Example text containing domain names with paths and some with protocols\n",
    "text = \"\"\"\n",
    "Here are some domain names with and without protocols:\n",
    "http://example.com\n",
    "https://mywebsite.org/about\n",
    "subdomain.example.net/products?category=books\n",
    "another-site.co.uk/contact?ref=home\n",
    "ftp://fileserver.com/path/to/file\n",
    "\"\"\"\n",
    "\n",
    "# Call the function and get the domain dictionary\n",
    "domain_links = extract_links(text)\n",
    "\n",
    "# Print the resulting dictionary\n",
    "for domain, links in domain_links.items():\n",
    "    print(f\"{domain}:\")\n",
    "    for link in links:\n",
    "        print(f\"  {link}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GMerCNRFx6Nf"
   },
   "source": [
    "## Extract phone number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 347,
     "status": "ok",
     "timestamp": 1731908333068,
     "user": {
      "displayName": "Giang Le Huynh",
      "userId": "12580968046014402630"
     },
     "user_tz": -420
    },
    "id": "KkhU2Q36r8lN",
    "outputId": "91ff7482-be19-4b37-cad8-a7f0f9a81333"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+1-800-5555555\n",
      "(800) 5555555\n",
      "555 555 5555\n",
      "+44 7911 123456\n",
      "123.456.7890\n",
      "800-123-4567\n"
     ]
    }
   ],
   "source": [
    "def extract_phone_numbers(text):\n",
    "    # Regular expression to match phone numbers (supports various formats)\n",
    "    phone_pattern = r'(\\+?\\d{1,3}[-.\\s]?)?(\\(?\\d{1,4}\\)?[-.\\s]?)?(\\d{1,4})[-.\\s]?(\\d{1,4})[-.\\s]?(\\d{1,4})'\n",
    "\n",
    "    # Find all matches in the text\n",
    "    phone_numbers = re.findall(phone_pattern, text)\n",
    "\n",
    "    # Clean up the result to return full phone numbers (combining the matched parts)\n",
    "    full_phone_numbers = [''.join(number) for number in phone_numbers]\n",
    "\n",
    "    return full_phone_numbers\n",
    "\n",
    "# Example text with phone numbers\n",
    "text = \"\"\"\n",
    "You can reach me at +1-800-555-5555 or (800) 555-5555.\n",
    "My office number is 555 555 5555. Call me at +44 7911 123456.\n",
    "Another contact: 123.456.7890. Or just use 800-123-4567.\n",
    "\"\"\"\n",
    "\n",
    "# Extract phone numbers from the text\n",
    "phones = extract_phone_numbers(text)\n",
    "\n",
    "# Print the extracted phone numbers\n",
    "for phone in phones:\n",
    "    print(phone)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v0nI6Df2x-p5"
   },
   "source": [
    "## Extract education"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('BACHELOR', '2015'), ('MASTERS', '2020')]\n"
     ]
    }
   ],
   "source": [
    "# Simulated constants\n",
    "class cs:\n",
    "    EDUCATION = {\"BACHELOR\", \"MASTERS\", \"PHD\", \"MBA\", \"BSC\", \"MSC\"}\n",
    "    STOPWORDS = {\"AND\", \"OF\", \"THE\"}\n",
    "    YEAR = r'\\b(19|20)\\d{2}\\b'  # Regex pattern to match years like 1999, 2020\n",
    "\n",
    "# Function to test\n",
    "def extract_education(nlp_text):\n",
    "    '''\n",
    "    Helper function to extract education from spacy nlp text\n",
    "\n",
    "    :param nlp_text: object of `spacy.tokens.doc.Doc`\n",
    "    :return: tuple of education degree and year if year is found\n",
    "             else only returns education degree\n",
    "    '''\n",
    "    edu = {}\n",
    "    # Extract education degree\n",
    "    try:\n",
    "        for index, token in enumerate(nlp_text):\n",
    "            text_cleaned = re.sub(r'[?|$|.|!|,]', r'', token.text)\n",
    "            if text_cleaned.upper() in cs.EDUCATION and text_cleaned.upper() not in cs.STOPWORDS:\n",
    "                # Collect surrounding context (current and next 5 tokens for simplicity)\n",
    "                context = \" \".join([t.text for t in nlp_text[index:index + 6]])\n",
    "                edu[text_cleaned.upper()] = context\n",
    "    except IndexError:\n",
    "        pass\n",
    "\n",
    "    # Extract year\n",
    "    education = []\n",
    "    for key in edu.keys():\n",
    "        year = re.search(re.compile(cs.YEAR), edu[key])\n",
    "        if year:\n",
    "            education.append((key, year.group(0)))  # Include degree and year\n",
    "        else:\n",
    "            education.append(key)  # Include degree only\n",
    "    return education\n",
    "\n",
    "# Test with spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "test_text = \"John completed his Bachelor of Science in 2015 and his Masters in 2020.\"\n",
    "nlp_text = nlp(test_text)\n",
    "\n",
    "# Execute the function\n",
    "result = extract_education(nlp_text)\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract skill"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted Skills: ['Python', 'Data analysis', 'Machine learning', 'Python']\n"
     ]
    }
   ],
   "source": [
    "# Mock skills data\n",
    "skills_data = {\"Skill\": [\"python\", \"data analysis\", \"machine learning\", \"nlp\", \"deep learning\"]}\n",
    "skills_file_path = \"skills.csv\"\n",
    "\n",
    "# Save the mock skills data to a CSV file\n",
    "pd.DataFrame(skills_data).to_csv(skills_file_path, index=False)\n",
    "\n",
    "# Function to extract skills\n",
    "def extract_skills(nlp_text, noun_chunks, skills_file=None):\n",
    "    '''\n",
    "    Helper function to extract skills from spacy nlp text\n",
    "\n",
    "    :param nlp_text: object of `spacy.tokens.doc.Doc`\n",
    "    :param noun_chunks: noun chunks extracted from nlp text\n",
    "    :param skills_file: path to skills file (optional)\n",
    "    :return: list of skills extracted\n",
    "    '''\n",
    "    tokens = [token.text for token in nlp_text if not token.is_stop]\n",
    "    if not skills_file:\n",
    "        raise ValueError(\"Skills file is required.\")\n",
    "    else:\n",
    "        data = pd.read_csv(skills_file)\n",
    "    skills = list(data['Skill'].str.lower())  # Convert skills to lowercase for matching\n",
    "    skillset = []\n",
    "    # Check for one-grams\n",
    "    for token in tokens:\n",
    "        if token.lower() in skills:\n",
    "            skillset.append(token)\n",
    "    # Check for bi-grams and tri-grams\n",
    "    for chunk in noun_chunks:\n",
    "        chunk_text = chunk.text.lower().strip()\n",
    "        if chunk_text in skills:\n",
    "            skillset.append(chunk_text)\n",
    "    return [i.capitalize() for i in set(skillset)]\n",
    "\n",
    "# Load SpaCy model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Test text\n",
    "test_text = \"I have experience in Python, data analysis, and machine learning.\"\n",
    "nlp_text = nlp(test_text)\n",
    "noun_chunks = list(nlp_text.noun_chunks)\n",
    "\n",
    "# Test the function\n",
    "result = extract_skills(nlp_text, noun_chunks, skills_file_path)\n",
    "print(\"Extracted Skills:\", result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract profession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EXPERIENCE :\n",
      "writing andoptimizing SQL code and Stored Procedures, creating functions, views,\n",
      "    -triggers and indexes.\n",
      "    -Cloud  platform:  Worked  on  Microsoft  Azure  cloud  services  like  Document  DB,  SQL\n",
      "    -Azure, StreamAnalytics, Event hub,  Power BI, Web Job, Web App, Power BI, Azure\n",
      "    -data lake analytics(U-SQL).\n",
      "    -Big Data: Worked Azure data lake store/analytics for big data processing and Azure\n",
      "    -data factoryto schedule U-SQL jobs. Designed and developed end to end  big data\n",
      "    -solution for data insights.\n",
      "    -Willing to relocate: Anywhere\n",
      "    -WORK EXPERIENCESoftware Engineer\n",
      "    -Microsoft - Manchester, UK.\n",
      "    -December 2015 to Present\n",
      "    -1. Microsoft Rewards Live dashboards:\n",
      "    -Description:  -  Microsoft  rewards  is  loyalty  program  that  rewards  Users  for\n",
      "    -browsing  and  shopping  online.  Microsoft  Rewards  members  can  earn  points  when\n",
      "    -searching  with  Bing,  browsing  with  Microsoft  Edge  and  making  purchases  at  the\n",
      "    -Xbox  Store,  the  Windows  Store  and  the  Microsoft  Store.  Plus,  user  can  pick  up\n",
      "    -bonus points for taking daily quizzes and tours on the Microsoft rewards website.\n",
      "    -Rewards live dashboards gives a live picture of usage world-wide and by markets\n",
      "    -like  US,  Canada,  Australia,  new  user  registration  count,  top/bottom  performing\n",
      "    -rewards offers, orders stats and weekly trends of user activities, orders and new\n",
      "    -user registrations. the PBI tiles gets refreshed in different frequencies starting\n",
      "    -from 5 seconds to 30 minutes.\n",
      "    -Technology/Tools used\n",
      "    -Event hub, stream analytics and Power BI.\n",
      "    -Responsibilities\n",
      "    -Created stream analytics jobs to process event hub data\n",
      "    -Created Power BI live dashboard to show live usage traffic, weekly trends, cards,\n",
      "    -charts to showtop/bottom 10 offers and usage metrics.\n",
      "    -2. Microsoft Rewards Data Insights:\n",
      "    -Description:  -  Microsoft  rewards  is  loyalty  program  that  rewards  Users  for\n",
      "    -browsing  and  shopping  online.  Microsoft  Rewards  members  can  earn  points  when\n",
      "    -searching  with  Bing,  browsing  with  Microsoft  Edge  and  making  purchases  at  the\n",
      "    -Xbox  Store,  the  Windows  Store  and  the  Microsoft  Store.  Plus,  user  can  pick  up\n",
      "    -bonus points for taking daily quizzes and tours on the Microsoft rewards website.\n",
      "    -Rewards  data  insights  is  data  analytics  and  reporting  platform,  processes  20\n",
      "    -million users daily activities and redemption across different markets like US,\n",
      "    -Canada, Australia.\n",
      "    -Technology/Tools used\n",
      "    -Cosmos (Microsoft big-data platform), c#, X-flow job monitoring, Power BI.\n",
      "    -Responsibilities\n",
      "    -Created big data scripts in cosmos\n",
      "    -C# data extractors, processors and reducers for data transformation\n",
      "    -Power BI dashboards\n",
      "    -3. End to end tracking Tool:\n",
      "    -Description:  -  This  is  real-time  Tracking  tool  to  track  different  business\n",
      "    -transactions  like  order,  order  response,  functional  acknowledgement,  invoice\n",
      "    -flowing inside ICOE. It gives flexibility to customers to track their transactions\n",
      "    -and appropriate error information in-case of any failure. Based on resource based\n",
      "    -access control the tool gives flexibility to end user to perform different actions\n",
      "    -like  view  transactions,  search  based  on  different  filter  criteria  and  view  and\n",
      "    -download  actual  message  payload.  End  to  end  tracking  tool  stitches  all  the\n",
      "    -business transaction like order to cash flow and connects different hops inside\n",
      "    -ICOE like gateway, routing server, Processing server. It also connects different\n",
      "    -systems like ICOE, partner end point and SAP.\n",
      "    -Technology/Tools used\n",
      "    -Azure Document db, Azure web job and Web APP, RBAC, Angular JS.\n",
      "    -Responsibilities\n",
      "    -Document dB stored procedures.\n",
      "    -Web job to process event hub data and populate Document db Web App API.\n",
      "    -Stream analytics job to transform data\n",
      "    -Power BI reports\n",
      "    -4. Biztrack Tracking Tool:\n",
      "    -Description:  -  This  is  real-time  Tracking  tool  to  track  different  business\n",
      "    -transactions  like  order,  order  response,  functional  acknowledgement,  invoice\n",
      "    -flowing inside ICOE. It gives flexibility to customers to track their transactions\n",
      "    -and appropriate error information in-case of any failure. Based on resource based\n",
      "    -access control the tool gives flexibility to end user to perform different actions\n",
      "    -like  view  transactions,  search  based  on  different  filter  criteria  and  view  and\n",
      "    -download actual message payload.\n",
      "    -Technology/Tools used\n",
      "    -SQL server 2014, SSIS, .net API, Angular JS.\n",
      "    -Responsibilities\n",
      "    -ETL solution to transform business transactions data stored in Biztalk tables.\n",
      "    -SQL azure tables, stored procedures, User defined functions.\n",
      "    -Performance tuning.\n",
      "    -Web API enhancements.\n",
      "EDUCATION :\n",
      "The University of Manchester - UK\n",
      "    -2007\n",
      "SKILLS :\n",
      "Excellent  analytical,  problem  solving,  communication,  knowledge  transfer  and\n",
      "    -interpersonalskills with ability to interact with individuals at all the levels\n",
      "    -Quick  learner  and  maintains  cordial  relationship  with  project  manager  and  team\n",
      "    -members andgood performer both in team and independent job environments\n",
      "    -Positive attitude towards superiors &amp; peers\n",
      "    -Supervised junior developers throughout project lifecycle and provided technical\n",
      "    -assistance.\n"
     ]
    }
   ],
   "source": [
    "# Example content for cs.RESUME_SECTIONS_PROFESSIONAL\n",
    "class cs:\n",
    "    RESUME_SECTIONS_PROFESSIONAL = {\"experience\", \"education\", \"skills\", \"certifications\", \"projects\"}\n",
    "\n",
    "# Function to extract entity sections from the resume\n",
    "def extract_entity_sections_professional(text):\n",
    "    '''\n",
    "    Helper function to extract all the raw text from sections of\n",
    "    resume specifically for professionals\n",
    "\n",
    "    :param text: Raw text of resume\n",
    "    :return: dictionary of entities\n",
    "    '''\n",
    "    text_split = [i.strip() for i in text.split('\\n')]\n",
    "    entities = {}\n",
    "    key = False\n",
    "    for phrase in text_split:\n",
    "        if len(phrase) == 1:\n",
    "            p_key = phrase\n",
    "        else:\n",
    "            p_key = set(phrase.lower().split()) & set(cs.RESUME_SECTIONS_PROFESSIONAL)\n",
    "        try:\n",
    "            p_key = list(p_key)[0]\n",
    "        except IndexError:\n",
    "            pass\n",
    "        if p_key in cs.RESUME_SECTIONS_PROFESSIONAL:\n",
    "            entities[p_key] = []\n",
    "            key = p_key\n",
    "        elif key and phrase.strip():\n",
    "            entities[key].append(phrase)\n",
    "    return entities\n",
    "\n",
    "# Testing the function\n",
    "entities = extract_entity_sections_professional(__text_raw)\n",
    "for k, v in entities.items():\n",
    "    print(k.upper(), ':')\n",
    "    print('\\n    -'.join(v))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract experience"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Lenovo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.data.path.append('C:/Users/Lenovo/nltk_data')  # Replace with your path\n",
    "nltk.download('wordnet')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Lenovo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "import constants as cs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource 'corpora/wordnet' not found.  Please use the NLTK\n  Downloader to obtain the resource:  >>> nltk.download()\n  Searched in:\n    - 'C:\\\\Users\\\\Lenovo/nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n    - 'C:\\\\Users\\\\Lenovo\\\\anaconda3\\\\envs\\\\resumeparser\\\\nltk_data'\n    - 'C:\\\\Users\\\\Lenovo\\\\anaconda3\\\\envs\\\\resumeparser\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\Lenovo\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:/Users/Lenovo/nltk_data'\n**********************************************************************",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda3\\envs\\resumeparser\\lib\\site-packages\\nltk\\corpus\\util.py:80\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     79\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m---> 80\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m: root \u001b[38;5;241m=\u001b[39m \u001b[43mnltk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;132;43;01m{}\u001b[39;49;00m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;132;43;01m{}\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msubdir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mzip_name\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     81\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m: \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\resumeparser\\lib\\site-packages\\nltk\\data.py:653\u001b[0m, in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    652\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m (sep, msg, sep)\n\u001b[1;32m--> 653\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource 'corpora/wordnet.zip/wordnet/.zip/' not found.  Please\n  use the NLTK Downloader to obtain the resource:  >>>\n  nltk.download()\n  Searched in:\n    - 'C:\\\\Users\\\\Lenovo/nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n    - 'C:\\\\Users\\\\Lenovo\\\\anaconda3\\\\envs\\\\resumeparser\\\\nltk_data'\n    - 'C:\\\\Users\\\\Lenovo\\\\anaconda3\\\\envs\\\\resumeparser\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\Lenovo\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:/Users/Lenovo/nltk_data'\n**********************************************************************",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[114], line 47\u001b[0m\n\u001b[0;32m     36\u001b[0m resume_text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[0;32m     37\u001b[0m \u001b[38;5;124mJohn Doe\u001b[39m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;124mSoftware Engineer at ABC Corp.\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[38;5;124m- Managed a team of 5 developers.\u001b[39m\n\u001b[0;32m     45\u001b[0m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;66;03m# Call the function to extract experience\u001b[39;00m\n\u001b[1;32m---> 47\u001b[0m experience \u001b[38;5;241m=\u001b[39m \u001b[43mextract_experience\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresume_text\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     49\u001b[0m \u001b[38;5;66;03m# Output the result\u001b[39;00m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m exp \u001b[38;5;129;01min\u001b[39;00m experience:\n",
      "Cell \u001b[1;32mIn[114], line 16\u001b[0m, in \u001b[0;36mextract_experience\u001b[1;34m(resume_text)\u001b[0m\n\u001b[0;32m     13\u001b[0m word_tokens \u001b[38;5;241m=\u001b[39m word_tokenize(resume_text)\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# Remove stop words and lemmatize\u001b[39;00m\n\u001b[1;32m---> 16\u001b[0m filtered_sentence \u001b[38;5;241m=\u001b[39m [w \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m word_tokens \u001b[38;5;28;01mif\u001b[39;00m w \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m stop_words \u001b[38;5;129;01mand\u001b[39;00m wordnet_lemmatizer\u001b[38;5;241m.\u001b[39mlemmatize(w) \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m stop_words]\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# POS tagging\u001b[39;00m\n\u001b[0;32m     19\u001b[0m sent \u001b[38;5;241m=\u001b[39m nltk\u001b[38;5;241m.\u001b[39mpos_tag(filtered_sentence)\n",
      "Cell \u001b[1;32mIn[114], line 16\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     13\u001b[0m word_tokens \u001b[38;5;241m=\u001b[39m word_tokenize(resume_text)\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# Remove stop words and lemmatize\u001b[39;00m\n\u001b[1;32m---> 16\u001b[0m filtered_sentence \u001b[38;5;241m=\u001b[39m [w \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m word_tokens \u001b[38;5;28;01mif\u001b[39;00m w \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m stop_words \u001b[38;5;129;01mand\u001b[39;00m \u001b[43mwordnet_lemmatizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlemmatize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mw\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m stop_words]\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# POS tagging\u001b[39;00m\n\u001b[0;32m     19\u001b[0m sent \u001b[38;5;241m=\u001b[39m nltk\u001b[38;5;241m.\u001b[39mpos_tag(filtered_sentence)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\resumeparser\\lib\\site-packages\\nltk\\stem\\wordnet.py:40\u001b[0m, in \u001b[0;36mWordNetLemmatizer.lemmatize\u001b[1;34m(self, word, pos)\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlemmatize\u001b[39m(\u001b[38;5;28mself\u001b[39m, word, pos\u001b[38;5;241m=\u001b[39mNOUN):\n\u001b[1;32m---> 40\u001b[0m     lemmas \u001b[38;5;241m=\u001b[39m \u001b[43mwordnet\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_morphy\u001b[49m(word, pos)\n\u001b[0;32m     41\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mmin\u001b[39m(lemmas, key\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m lemmas \u001b[38;5;28;01melse\u001b[39;00m word\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\resumeparser\\lib\\site-packages\\nltk\\corpus\\util.py:116\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__getattr__\u001b[1;34m(self, attr)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attr \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__bases__\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m    114\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLazyCorpusLoader object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__bases__\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 116\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__load\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    117\u001b[0m \u001b[38;5;66;03m# This looks circular, but its not, since __load() changes our\u001b[39;00m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;66;03m# __class__ to something new:\u001b[39;00m\n\u001b[0;32m    119\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, attr)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\resumeparser\\lib\\site-packages\\nltk\\corpus\\util.py:81\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     79\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     80\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m: root \u001b[38;5;241m=\u001b[39m nltk\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msubdir, zip_name))\n\u001b[1;32m---> 81\u001b[0m         \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m: \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m     83\u001b[0m \u001b[38;5;66;03m# Load the corpus.\u001b[39;00m\n\u001b[0;32m     84\u001b[0m corpus \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__reader_cls(root, \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__kwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\resumeparser\\lib\\site-packages\\nltk\\corpus\\util.py:78\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     76\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     77\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 78\u001b[0m         root \u001b[38;5;241m=\u001b[39m \u001b[43mnltk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;132;43;01m{}\u001b[39;49;00m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;132;43;01m{}\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msubdir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__name\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     79\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     80\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m: root \u001b[38;5;241m=\u001b[39m nltk\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msubdir, zip_name))\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\resumeparser\\lib\\site-packages\\nltk\\data.py:653\u001b[0m, in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    651\u001b[0m sep \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m70\u001b[39m\n\u001b[0;32m    652\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m (sep, msg, sep)\n\u001b[1;32m--> 653\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource 'corpora/wordnet' not found.  Please use the NLTK\n  Downloader to obtain the resource:  >>> nltk.download()\n  Searched in:\n    - 'C:\\\\Users\\\\Lenovo/nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n    - 'C:\\\\Users\\\\Lenovo\\\\anaconda3\\\\envs\\\\resumeparser\\\\nltk_data'\n    - 'C:\\\\Users\\\\Lenovo\\\\anaconda3\\\\envs\\\\resumeparser\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\Lenovo\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:/Users/Lenovo/nltk_data'\n**********************************************************************"
     ]
    }
   ],
   "source": [
    "def extract_experience(resume_text):\n",
    "    '''\n",
    "    Helper function to extract experience from resume text\n",
    "\n",
    "    :param resume_text: Plain resume text\n",
    "    :return: list of experience\n",
    "    '''\n",
    "    # Initialize lemmatizer and stopwords\n",
    "    wordnet_lemmatizer = WordNetLemmatizer()\n",
    "    stop_words = cs.STOPWORDS\n",
    "\n",
    "    # Word tokenization\n",
    "    word_tokens = word_tokenize(resume_text)\n",
    "\n",
    "    # Remove stop words and lemmatize\n",
    "    filtered_sentence = [w for w in word_tokens if w not in stop_words and wordnet_lemmatizer.lemmatize(w) not in stop_words]\n",
    "\n",
    "    # POS tagging\n",
    "    sent = nltk.pos_tag(filtered_sentence)\n",
    "\n",
    "    # Define regex pattern to capture proper nouns (NNP)\n",
    "    cp = nltk.RegexpParser('P: {<NNP>+}')\n",
    "    _cs = cp.parse(sent)\n",
    "\n",
    "    # Extract experience-related chunks (proper nouns)\n",
    "    test = []\n",
    "    for vp in list(_cs.subtrees(filter=lambda x: x.label() == 'P')):\n",
    "        test.append(\" \".join([i[0] for i in vp.leaves() if len(vp.leaves()) >= 2]))\n",
    "\n",
    "    # Search the word 'experience' in the chunk and return text after it\n",
    "    experience = [x[x.lower().index('experience') + 10:] for i, x in enumerate(test) if x and 'experience' in x.lower()]\n",
    "    \n",
    "    return experience\n",
    "\n",
    "# Example Resume Text for Testing\n",
    "resume_text = \"\"\"\n",
    "John Doe\n",
    "Software Engineer at ABC Corp.\n",
    "Responsible for designing and developing web applications.\n",
    "\n",
    "Experience:\n",
    "- Worked at XYZ Ltd. as a Senior Developer for 5 years.\n",
    "- Led the development of a major e-commerce platform.\n",
    "- Managed a team of 5 developers.\n",
    "\"\"\"\n",
    "# Call the function to extract experience\n",
    "experience = extract_experience(resume_text)\n",
    "\n",
    "# Output the result\n",
    "for exp in experience:\n",
    "    print(exp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['C:\\\\Users\\\\Lenovo/nltk_data', 'C:\\\\nltk_data', 'D:\\\\nltk_data', 'E:\\\\nltk_data', 'C:\\\\Users\\\\Lenovo\\\\anaconda3\\\\envs\\\\resumeparser\\\\nltk_data', 'C:\\\\Users\\\\Lenovo\\\\anaconda3\\\\envs\\\\resumeparser\\\\lib\\\\nltk_data', 'C:\\\\Users\\\\Lenovo\\\\AppData\\\\Roaming\\\\nltk_data', 'C:/Users/Lenovo/nltk_data']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "print(nltk.data.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_experience(resume_text):\n",
    "    '''\n",
    "    Helper function to extract experience from resume text\n",
    "\n",
    "    :param resume_text: Plain resume text\n",
    "    :return: list of experience\n",
    "    '''\n",
    "    wordnet_lemmatizer = WordNetLemmatizer()\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "\n",
    "    # word tokenization\n",
    "    word_tokens = nltk.word_tokenize(resume_text)\n",
    "\n",
    "    # remove stop words and lemmatize\n",
    "    filtered_sentence = [\n",
    "            w for w in word_tokens if w not\n",
    "            in stop_words and wordnet_lemmatizer.lemmatize(w)\n",
    "            not in stop_words\n",
    "        ]\n",
    "    sent = nltk.pos_tag(filtered_sentence)\n",
    "\n",
    "    # parse regex\n",
    "    cp = nltk.RegexpParser('P: {<NNP>+}')\n",
    "    cs = cp.parse(sent)\n",
    "\n",
    "    # for i in cs.subtrees(filter=lambda x: x.label() == 'P'):\n",
    "    #     print(i)\n",
    "\n",
    "    test = []\n",
    "\n",
    "    for vp in list(\n",
    "        cs.subtrees(filter=lambda x: x.label() == 'P')\n",
    "    ):\n",
    "        test.append(\" \".join([\n",
    "            i[0] for i in vp.leaves()\n",
    "            if len(vp.leaves()) >= 2])\n",
    "        )\n",
    "\n",
    "    # Search the word 'experience' in the chunk and\n",
    "    # then print out the text after it\n",
    "    x = [\n",
    "        x[x.lower().index('experience') + 10:]\n",
    "        for i, x in enumerate(test)\n",
    "        if x and 'experience' in x.lower()\n",
    "    ]\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use model pre-train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\Lenovo\\\\learning\\\\Resume Analyser\\\\4.version'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spacy_transformers==1.3.5 in c:\\users\\lenovo\\anaconda3\\envs\\resumeparser\\lib\\site-packages (1.3.5)\n",
      "Requirement already satisfied: spacy<4.1.0,>=3.5.0 in c:\\users\\lenovo\\anaconda3\\envs\\resumeparser\\lib\\site-packages (from spacy_transformers==1.3.5) (3.7.6)\n",
      "Requirement already satisfied: transformers<4.37.0,>=3.4.0 in c:\\users\\lenovo\\anaconda3\\envs\\resumeparser\\lib\\site-packages (from spacy_transformers==1.3.5) (4.36.2)\n",
      "Requirement already satisfied: torch>=1.8.0 in c:\\users\\lenovo\\anaconda3\\envs\\resumeparser\\lib\\site-packages (from spacy_transformers==1.3.5) (2.5.1)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.0 in c:\\users\\lenovo\\anaconda3\\envs\\resumeparser\\lib\\site-packages (from spacy_transformers==1.3.5) (2.4.8)\n",
      "Requirement already satisfied: spacy-alignments<1.0.0,>=0.7.2 in c:\\users\\lenovo\\anaconda3\\envs\\resumeparser\\lib\\site-packages (from spacy_transformers==1.3.5) (0.9.1)\n",
      "Requirement already satisfied: numpy>=1.19.0 in c:\\users\\lenovo\\anaconda3\\envs\\resumeparser\\lib\\site-packages (from spacy_transformers==1.3.5) (1.26.4)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\users\\lenovo\\anaconda3\\envs\\resumeparser\\lib\\site-packages (from spacy<4.1.0,>=3.5.0->spacy_transformers==1.3.5) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\lenovo\\anaconda3\\envs\\resumeparser\\lib\\site-packages (from spacy<4.1.0,>=3.5.0->spacy_transformers==1.3.5) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\lenovo\\anaconda3\\envs\\resumeparser\\lib\\site-packages (from spacy<4.1.0,>=3.5.0->spacy_transformers==1.3.5) (1.0.10)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\lenovo\\anaconda3\\envs\\resumeparser\\lib\\site-packages (from spacy<4.1.0,>=3.5.0->spacy_transformers==1.3.5) (2.0.8)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\lenovo\\anaconda3\\envs\\resumeparser\\lib\\site-packages (from spacy<4.1.0,>=3.5.0->spacy_transformers==1.3.5) (3.0.9)\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in c:\\users\\lenovo\\anaconda3\\envs\\resumeparser\\lib\\site-packages (from spacy<4.1.0,>=3.5.0->spacy_transformers==1.3.5) (8.2.5)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\users\\lenovo\\anaconda3\\envs\\resumeparser\\lib\\site-packages (from spacy<4.1.0,>=3.5.0->spacy_transformers==1.3.5) (1.1.3)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\lenovo\\anaconda3\\envs\\resumeparser\\lib\\site-packages (from spacy<4.1.0,>=3.5.0->spacy_transformers==1.3.5) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in c:\\users\\lenovo\\anaconda3\\envs\\resumeparser\\lib\\site-packages (from spacy<4.1.0,>=3.5.0->spacy_transformers==1.3.5) (0.4.1)\n",
      "Requirement already satisfied: typer<1.0.0,>=0.3.0 in c:\\users\\lenovo\\anaconda3\\envs\\resumeparser\\lib\\site-packages (from spacy<4.1.0,>=3.5.0->spacy_transformers==1.3.5) (0.13.1)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\lenovo\\anaconda3\\envs\\resumeparser\\lib\\site-packages (from spacy<4.1.0,>=3.5.0->spacy_transformers==1.3.5) (4.67.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\lenovo\\anaconda3\\envs\\resumeparser\\lib\\site-packages (from spacy<4.1.0,>=3.5.0->spacy_transformers==1.3.5) (2.32.3)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in c:\\users\\lenovo\\anaconda3\\envs\\resumeparser\\lib\\site-packages (from spacy<4.1.0,>=3.5.0->spacy_transformers==1.3.5) (2.9.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\lenovo\\anaconda3\\envs\\resumeparser\\lib\\site-packages (from spacy<4.1.0,>=3.5.0->spacy_transformers==1.3.5) (3.1.4)\n",
      "Requirement already satisfied: setuptools in c:\\users\\lenovo\\anaconda3\\envs\\resumeparser\\lib\\site-packages (from spacy<4.1.0,>=3.5.0->spacy_transformers==1.3.5) (75.1.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\lenovo\\anaconda3\\envs\\resumeparser\\lib\\site-packages (from spacy<4.1.0,>=3.5.0->spacy_transformers==1.3.5) (24.1)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\lenovo\\anaconda3\\envs\\resumeparser\\lib\\site-packages (from spacy<4.1.0,>=3.5.0->spacy_transformers==1.3.5) (3.5.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\lenovo\\anaconda3\\envs\\resumeparser\\lib\\site-packages (from torch>=1.8.0->spacy_transformers==1.3.5) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\lenovo\\anaconda3\\envs\\resumeparser\\lib\\site-packages (from torch>=1.8.0->spacy_transformers==1.3.5) (4.11.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\lenovo\\anaconda3\\envs\\resumeparser\\lib\\site-packages (from torch>=1.8.0->spacy_transformers==1.3.5) (3.4.2)\n",
      "Requirement already satisfied: fsspec in c:\\users\\lenovo\\anaconda3\\envs\\resumeparser\\lib\\site-packages (from torch>=1.8.0->spacy_transformers==1.3.5) (2024.10.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\lenovo\\anaconda3\\envs\\resumeparser\\lib\\site-packages (from torch>=1.8.0->spacy_transformers==1.3.5) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\lenovo\\anaconda3\\envs\\resumeparser\\lib\\site-packages (from sympy==1.13.1->torch>=1.8.0->spacy_transformers==1.3.5) (1.3.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in c:\\users\\lenovo\\anaconda3\\envs\\resumeparser\\lib\\site-packages (from transformers<4.37.0,>=3.4.0->spacy_transformers==1.3.5) (0.26.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\lenovo\\anaconda3\\envs\\resumeparser\\lib\\site-packages (from transformers<4.37.0,>=3.4.0->spacy_transformers==1.3.5) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\lenovo\\anaconda3\\envs\\resumeparser\\lib\\site-packages (from transformers<4.37.0,>=3.4.0->spacy_transformers==1.3.5) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in c:\\users\\lenovo\\anaconda3\\envs\\resumeparser\\lib\\site-packages (from transformers<4.37.0,>=3.4.0->spacy_transformers==1.3.5) (0.15.2)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in c:\\users\\lenovo\\anaconda3\\envs\\resumeparser\\lib\\site-packages (from transformers<4.37.0,>=3.4.0->spacy_transformers==1.3.5) (0.4.5)\n",
      "Requirement already satisfied: language-data>=1.2 in c:\\users\\lenovo\\anaconda3\\envs\\resumeparser\\lib\\site-packages (from langcodes<4.0.0,>=3.2.0->spacy<4.1.0,>=3.5.0->spacy_transformers==1.3.5) (1.3.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\lenovo\\anaconda3\\envs\\resumeparser\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<4.1.0,>=3.5.0->spacy_transformers==1.3.5) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in c:\\users\\lenovo\\anaconda3\\envs\\resumeparser\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<4.1.0,>=3.5.0->spacy_transformers==1.3.5) (2.23.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\lenovo\\anaconda3\\envs\\resumeparser\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<4.1.0,>=3.5.0->spacy_transformers==1.3.5) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\lenovo\\anaconda3\\envs\\resumeparser\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<4.1.0,>=3.5.0->spacy_transformers==1.3.5) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\lenovo\\anaconda3\\envs\\resumeparser\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<4.1.0,>=3.5.0->spacy_transformers==1.3.5) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\lenovo\\anaconda3\\envs\\resumeparser\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<4.1.0,>=3.5.0->spacy_transformers==1.3.5) (2024.8.30)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in c:\\users\\lenovo\\anaconda3\\envs\\resumeparser\\lib\\site-packages (from thinc<8.3.0,>=8.2.2->spacy<4.1.0,>=3.5.0->spacy_transformers==1.3.5) (0.7.11)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\lenovo\\anaconda3\\envs\\resumeparser\\lib\\site-packages (from thinc<8.3.0,>=8.2.2->spacy<4.1.0,>=3.5.0->spacy_transformers==1.3.5) (0.1.5)\n",
      "Requirement already satisfied: colorama in c:\\users\\lenovo\\anaconda3\\envs\\resumeparser\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy<4.1.0,>=3.5.0->spacy_transformers==1.3.5) (0.4.6)\n",
      "Requirement already satisfied: click>=8.0.0 in c:\\users\\lenovo\\anaconda3\\envs\\resumeparser\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy<4.1.0,>=3.5.0->spacy_transformers==1.3.5) (8.1.7)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in c:\\users\\lenovo\\anaconda3\\envs\\resumeparser\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy<4.1.0,>=3.5.0->spacy_transformers==1.3.5) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in c:\\users\\lenovo\\anaconda3\\envs\\resumeparser\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy<4.1.0,>=3.5.0->spacy_transformers==1.3.5) (13.9.4)\n",
      "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in c:\\users\\lenovo\\anaconda3\\envs\\resumeparser\\lib\\site-packages (from weasel<0.5.0,>=0.1.0->spacy<4.1.0,>=3.5.0->spacy_transformers==1.3.5) (0.20.0)\n",
      "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in c:\\users\\lenovo\\anaconda3\\envs\\resumeparser\\lib\\site-packages (from weasel<0.5.0,>=0.1.0->spacy<4.1.0,>=3.5.0->spacy_transformers==1.3.5) (7.0.5)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\lenovo\\anaconda3\\envs\\resumeparser\\lib\\site-packages (from jinja2->spacy<4.1.0,>=3.5.0->spacy_transformers==1.3.5) (2.1.3)\n",
      "Requirement already satisfied: marisa-trie>=1.1.0 in c:\\users\\lenovo\\anaconda3\\envs\\resumeparser\\lib\\site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<4.1.0,>=3.5.0->spacy_transformers==1.3.5) (1.2.1)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\lenovo\\anaconda3\\envs\\resumeparser\\lib\\site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<4.1.0,>=3.5.0->spacy_transformers==1.3.5) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\lenovo\\anaconda3\\envs\\resumeparser\\lib\\site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<4.1.0,>=3.5.0->spacy_transformers==1.3.5) (2.15.1)\n",
      "Requirement already satisfied: wrapt in c:\\users\\lenovo\\anaconda3\\envs\\resumeparser\\lib\\site-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy<4.1.0,>=3.5.0->spacy_transformers==1.3.5) (1.16.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\lenovo\\anaconda3\\envs\\resumeparser\\lib\\site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<4.1.0,>=3.5.0->spacy_transformers==1.3.5) (0.1.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install spacy_transformers==1.3.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers==4.36.2 in c:\\users\\lenovo\\anaconda3\\envs\\resumeparser\\lib\\site-packages (4.36.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\lenovo\\anaconda3\\envs\\resumeparser\\lib\\site-packages (from transformers==4.36.2) (3.16.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in c:\\users\\lenovo\\anaconda3\\envs\\resumeparser\\lib\\site-packages (from transformers==4.36.2) (0.26.2)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\lenovo\\anaconda3\\envs\\resumeparser\\lib\\site-packages (from transformers==4.36.2) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\lenovo\\anaconda3\\envs\\resumeparser\\lib\\site-packages (from transformers==4.36.2) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\lenovo\\anaconda3\\envs\\resumeparser\\lib\\site-packages (from transformers==4.36.2) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\lenovo\\anaconda3\\envs\\resumeparser\\lib\\site-packages (from transformers==4.36.2) (2024.11.6)\n",
      "Requirement already satisfied: requests in c:\\users\\lenovo\\anaconda3\\envs\\resumeparser\\lib\\site-packages (from transformers==4.36.2) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in c:\\users\\lenovo\\anaconda3\\envs\\resumeparser\\lib\\site-packages (from transformers==4.36.2) (0.15.2)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in c:\\users\\lenovo\\anaconda3\\envs\\resumeparser\\lib\\site-packages (from transformers==4.36.2) (0.4.5)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\lenovo\\anaconda3\\envs\\resumeparser\\lib\\site-packages (from transformers==4.36.2) (4.67.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\lenovo\\anaconda3\\envs\\resumeparser\\lib\\site-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.36.2) (2024.10.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\lenovo\\anaconda3\\envs\\resumeparser\\lib\\site-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.36.2) (4.11.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\lenovo\\anaconda3\\envs\\resumeparser\\lib\\site-packages (from tqdm>=4.27->transformers==4.36.2) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\lenovo\\anaconda3\\envs\\resumeparser\\lib\\site-packages (from requests->transformers==4.36.2) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\lenovo\\anaconda3\\envs\\resumeparser\\lib\\site-packages (from requests->transformers==4.36.2) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\lenovo\\anaconda3\\envs\\resumeparser\\lib\\site-packages (from requests->transformers==4.36.2) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\lenovo\\anaconda3\\envs\\resumeparser\\lib\\site-packages (from requests->transformers==4.36.2) (2024.8.30)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers==4.36.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: torch\n",
      "Version: 2.5.1\n",
      "Summary: Tensors and Dynamic neural networks in Python with strong GPU acceleration\n",
      "Home-page: https://pytorch.org/\n",
      "Author: PyTorch Team\n",
      "Author-email: packages@pytorch.org\n",
      "License: BSD-3-Clause\n",
      "Location: c:\\users\\lenovo\\anaconda3\\envs\\resumeparser\\lib\\site-packages\n",
      "Requires: filelock, fsspec, jinja2, networkx, sympy, typing-extensions\n",
      "Required-by: spacy-transformers\n"
     ]
    }
   ],
   "source": [
    "!pip show torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spacy-transformers in c:\\users\\lenovo\\anaconda3\\envs\\resumeparser\\lib\\site-packages (1.3.5)\n",
      "Requirement already satisfied: spacy<4.1.0,>=3.5.0 in c:\\users\\lenovo\\anaconda3\\envs\\resumeparser\\lib\\site-packages (from spacy-transformers) (3.8.2)\n",
      "Requirement already satisfied: transformers<4.37.0,>=3.4.0 in c:\\users\\lenovo\\anaconda3\\envs\\resumeparser\\lib\\site-packages (from spacy-transformers) (4.36.2)\n",
      "Requirement already satisfied: torch>=1.8.0 in c:\\users\\lenovo\\anaconda3\\envs\\resumeparser\\lib\\site-packages (from spacy-transformers) (2.5.1)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.0 in c:\\users\\lenovo\\anaconda3\\envs\\resumeparser\\lib\\site-packages (from spacy-transformers) (2.4.8)\n",
      "Requirement already satisfied: spacy-alignments<1.0.0,>=0.7.2 in c:\\users\\lenovo\\anaconda3\\envs\\resumeparser\\lib\\site-packages (from spacy-transformers) (0.9.1)\n",
      "Requirement already satisfied: numpy>=1.19.0 in c:\\users\\lenovo\\anaconda3\\envs\\resumeparser\\lib\\site-packages (from spacy-transformers) (2.0.2)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\users\\lenovo\\anaconda3\\envs\\resumeparser\\lib\\site-packages (from spacy<4.1.0,>=3.5.0->spacy-transformers) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\lenovo\\anaconda3\\envs\\resumeparser\\lib\\site-packages (from spacy<4.1.0,>=3.5.0->spacy-transformers) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\lenovo\\anaconda3\\envs\\resumeparser\\lib\\site-packages (from spacy<4.1.0,>=3.5.0->spacy-transformers) (1.0.10)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\lenovo\\anaconda3\\envs\\resumeparser\\lib\\site-packages (from spacy<4.1.0,>=3.5.0->spacy-transformers) (2.0.8)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\lenovo\\anaconda3\\envs\\resumeparser\\lib\\site-packages (from spacy<4.1.0,>=3.5.0->spacy-transformers) (3.0.9)\n",
      "Requirement already satisfied: thinc<8.4.0,>=8.3.0 in c:\\users\\lenovo\\anaconda3\\envs\\resumeparser\\lib\\site-packages (from spacy<4.1.0,>=3.5.0->spacy-transformers) (8.3.2)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\users\\lenovo\\anaconda3\\envs\\resumeparser\\lib\\site-packages (from spacy<4.1.0,>=3.5.0->spacy-transformers) (1.1.3)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\lenovo\\anaconda3\\envs\\resumeparser\\lib\\site-packages (from spacy<4.1.0,>=3.5.0->spacy-transformers) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in c:\\users\\lenovo\\anaconda3\\envs\\resumeparser\\lib\\site-packages (from spacy<4.1.0,>=3.5.0->spacy-transformers) (0.4.1)\n",
      "Requirement already satisfied: typer<1.0.0,>=0.3.0 in c:\\users\\lenovo\\anaconda3\\envs\\resumeparser\\lib\\site-packages (from spacy<4.1.0,>=3.5.0->spacy-transformers) (0.13.1)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\lenovo\\anaconda3\\envs\\resumeparser\\lib\\site-packages (from spacy<4.1.0,>=3.5.0->spacy-transformers) (4.67.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\lenovo\\anaconda3\\envs\\resumeparser\\lib\\site-packages (from spacy<4.1.0,>=3.5.0->spacy-transformers) (2.32.3)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in c:\\users\\lenovo\\anaconda3\\envs\\resumeparser\\lib\\site-packages (from spacy<4.1.0,>=3.5.0->spacy-transformers) (2.9.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\lenovo\\anaconda3\\envs\\resumeparser\\lib\\site-packages (from spacy<4.1.0,>=3.5.0->spacy-transformers) (3.1.4)\n",
      "Requirement already satisfied: setuptools in c:\\users\\lenovo\\anaconda3\\envs\\resumeparser\\lib\\site-packages (from spacy<4.1.0,>=3.5.0->spacy-transformers) (75.1.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\lenovo\\anaconda3\\envs\\resumeparser\\lib\\site-packages (from spacy<4.1.0,>=3.5.0->spacy-transformers) (24.1)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\lenovo\\anaconda3\\envs\\resumeparser\\lib\\site-packages (from spacy<4.1.0,>=3.5.0->spacy-transformers) (3.5.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\lenovo\\anaconda3\\envs\\resumeparser\\lib\\site-packages (from torch>=1.8.0->spacy-transformers) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\lenovo\\anaconda3\\envs\\resumeparser\\lib\\site-packages (from torch>=1.8.0->spacy-transformers) (4.11.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\lenovo\\anaconda3\\envs\\resumeparser\\lib\\site-packages (from torch>=1.8.0->spacy-transformers) (3.4.2)\n",
      "Requirement already satisfied: fsspec in c:\\users\\lenovo\\anaconda3\\envs\\resumeparser\\lib\\site-packages (from torch>=1.8.0->spacy-transformers) (2024.10.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\lenovo\\anaconda3\\envs\\resumeparser\\lib\\site-packages (from torch>=1.8.0->spacy-transformers) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\lenovo\\anaconda3\\envs\\resumeparser\\lib\\site-packages (from sympy==1.13.1->torch>=1.8.0->spacy-transformers) (1.3.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in c:\\users\\lenovo\\anaconda3\\envs\\resumeparser\\lib\\site-packages (from transformers<4.37.0,>=3.4.0->spacy-transformers) (0.26.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\lenovo\\anaconda3\\envs\\resumeparser\\lib\\site-packages (from transformers<4.37.0,>=3.4.0->spacy-transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\lenovo\\anaconda3\\envs\\resumeparser\\lib\\site-packages (from transformers<4.37.0,>=3.4.0->spacy-transformers) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in c:\\users\\lenovo\\anaconda3\\envs\\resumeparser\\lib\\site-packages (from transformers<4.37.0,>=3.4.0->spacy-transformers) (0.15.2)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in c:\\users\\lenovo\\anaconda3\\envs\\resumeparser\\lib\\site-packages (from transformers<4.37.0,>=3.4.0->spacy-transformers) (0.4.5)\n",
      "Requirement already satisfied: language-data>=1.2 in c:\\users\\lenovo\\anaconda3\\envs\\resumeparser\\lib\\site-packages (from langcodes<4.0.0,>=3.2.0->spacy<4.1.0,>=3.5.0->spacy-transformers) (1.3.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\lenovo\\anaconda3\\envs\\resumeparser\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<4.1.0,>=3.5.0->spacy-transformers) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in c:\\users\\lenovo\\anaconda3\\envs\\resumeparser\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<4.1.0,>=3.5.0->spacy-transformers) (2.23.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\lenovo\\anaconda3\\envs\\resumeparser\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<4.1.0,>=3.5.0->spacy-transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\lenovo\\anaconda3\\envs\\resumeparser\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<4.1.0,>=3.5.0->spacy-transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\lenovo\\anaconda3\\envs\\resumeparser\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<4.1.0,>=3.5.0->spacy-transformers) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\lenovo\\anaconda3\\envs\\resumeparser\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<4.1.0,>=3.5.0->spacy-transformers) (2024.8.30)\n",
      "Requirement already satisfied: blis<1.1.0,>=1.0.0 in c:\\users\\lenovo\\anaconda3\\envs\\resumeparser\\lib\\site-packages (from thinc<8.4.0,>=8.3.0->spacy<4.1.0,>=3.5.0->spacy-transformers) (1.0.1)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\lenovo\\anaconda3\\envs\\resumeparser\\lib\\site-packages (from thinc<8.4.0,>=8.3.0->spacy<4.1.0,>=3.5.0->spacy-transformers) (0.1.5)\n",
      "Requirement already satisfied: colorama in c:\\users\\lenovo\\anaconda3\\envs\\resumeparser\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy<4.1.0,>=3.5.0->spacy-transformers) (0.4.6)\n",
      "Requirement already satisfied: click>=8.0.0 in c:\\users\\lenovo\\anaconda3\\envs\\resumeparser\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy<4.1.0,>=3.5.0->spacy-transformers) (8.1.7)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in c:\\users\\lenovo\\anaconda3\\envs\\resumeparser\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy<4.1.0,>=3.5.0->spacy-transformers) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in c:\\users\\lenovo\\anaconda3\\envs\\resumeparser\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy<4.1.0,>=3.5.0->spacy-transformers) (13.9.4)\n",
      "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in c:\\users\\lenovo\\anaconda3\\envs\\resumeparser\\lib\\site-packages (from weasel<0.5.0,>=0.1.0->spacy<4.1.0,>=3.5.0->spacy-transformers) (0.20.0)\n",
      "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in c:\\users\\lenovo\\anaconda3\\envs\\resumeparser\\lib\\site-packages (from weasel<0.5.0,>=0.1.0->spacy<4.1.0,>=3.5.0->spacy-transformers) (7.0.5)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\lenovo\\anaconda3\\envs\\resumeparser\\lib\\site-packages (from jinja2->spacy<4.1.0,>=3.5.0->spacy-transformers) (2.1.3)\n",
      "Requirement already satisfied: marisa-trie>=1.1.0 in c:\\users\\lenovo\\anaconda3\\envs\\resumeparser\\lib\\site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<4.1.0,>=3.5.0->spacy-transformers) (1.2.1)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\lenovo\\anaconda3\\envs\\resumeparser\\lib\\site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<4.1.0,>=3.5.0->spacy-transformers) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\lenovo\\anaconda3\\envs\\resumeparser\\lib\\site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<4.1.0,>=3.5.0->spacy-transformers) (2.15.1)\n",
      "Requirement already satisfied: wrapt in c:\\users\\lenovo\\anaconda3\\envs\\resumeparser\\lib\\site-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy<4.1.0,>=3.5.0->spacy-transformers) (1.16.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\lenovo\\anaconda3\\envs\\resumeparser\\lib\\site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<4.1.0,>=3.5.0->spacy-transformers) (0.1.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install -U spacy-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'full'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[59], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrainning/_model/hyper_model/model-best\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m----> 2\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mspacy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\resumeparser\\lib\\site-packages\\spacy\\__init__.py:51\u001b[0m, in \u001b[0;36mload\u001b[1;34m(name, vocab, disable, enable, exclude, config)\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload\u001b[39m(\n\u001b[0;32m     28\u001b[0m     name: Union[\u001b[38;5;28mstr\u001b[39m, Path],\n\u001b[0;32m     29\u001b[0m     \u001b[38;5;241m*\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     34\u001b[0m     config: Union[Dict[\u001b[38;5;28mstr\u001b[39m, Any], Config] \u001b[38;5;241m=\u001b[39m util\u001b[38;5;241m.\u001b[39mSimpleFrozenDict(),\n\u001b[0;32m     35\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Language:\n\u001b[0;32m     36\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Load a spaCy model from an installed package or a local path.\u001b[39;00m\n\u001b[0;32m     37\u001b[0m \n\u001b[0;32m     38\u001b[0m \u001b[38;5;124;03m    name (str): Package name or model path.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[38;5;124;03m    RETURNS (Language): The loaded nlp object.\u001b[39;00m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 51\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mutil\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     52\u001b[0m \u001b[43m        \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     53\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvocab\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvocab\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     54\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdisable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdisable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     55\u001b[0m \u001b[43m        \u001b[49m\u001b[43menable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     56\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexclude\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexclude\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     57\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     58\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\resumeparser\\lib\\site-packages\\spacy\\util.py:467\u001b[0m, in \u001b[0;36mload_model\u001b[1;34m(name, vocab, disable, enable, exclude, config)\u001b[0m\n\u001b[0;32m    465\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m load_model_from_package(name, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[0;32m    466\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m Path(name)\u001b[38;5;241m.\u001b[39mexists():  \u001b[38;5;66;03m# path to model data directory\u001b[39;00m\n\u001b[1;32m--> 467\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m load_model_from_path(Path(name), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[0;32m    468\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(name, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexists\u001b[39m\u001b[38;5;124m\"\u001b[39m):  \u001b[38;5;66;03m# Path or Path-like to model data\u001b[39;00m\n\u001b[0;32m    469\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m load_model_from_path(name, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\resumeparser\\lib\\site-packages\\spacy\\util.py:539\u001b[0m, in \u001b[0;36mload_model_from_path\u001b[1;34m(model_path, meta, vocab, disable, enable, exclude, config)\u001b[0m\n\u001b[0;32m    537\u001b[0m overrides \u001b[38;5;241m=\u001b[39m dict_to_dot(config, for_overrides\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    538\u001b[0m config \u001b[38;5;241m=\u001b[39m load_config(config_path, overrides\u001b[38;5;241m=\u001b[39moverrides)\n\u001b[1;32m--> 539\u001b[0m nlp \u001b[38;5;241m=\u001b[39m \u001b[43mload_model_from_config\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    540\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    541\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvocab\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvocab\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    542\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdisable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdisable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    543\u001b[0m \u001b[43m    \u001b[49m\u001b[43menable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    544\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexclude\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexclude\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    545\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmeta\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmeta\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    546\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    547\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m nlp\u001b[38;5;241m.\u001b[39mfrom_disk(model_path, exclude\u001b[38;5;241m=\u001b[39mexclude, overrides\u001b[38;5;241m=\u001b[39moverrides)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\resumeparser\\lib\\site-packages\\spacy\\util.py:587\u001b[0m, in \u001b[0;36mload_model_from_config\u001b[1;34m(config, meta, vocab, disable, enable, exclude, auto_fill, validate)\u001b[0m\n\u001b[0;32m    584\u001b[0m \u001b[38;5;66;03m# This will automatically handle all codes registered via the languages\u001b[39;00m\n\u001b[0;32m    585\u001b[0m \u001b[38;5;66;03m# registry, including custom subclasses provided via entry points\u001b[39;00m\n\u001b[0;32m    586\u001b[0m lang_cls \u001b[38;5;241m=\u001b[39m get_lang_class(nlp_config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlang\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m--> 587\u001b[0m nlp \u001b[38;5;241m=\u001b[39m \u001b[43mlang_cls\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_config\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    588\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    589\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvocab\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvocab\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    590\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdisable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdisable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    591\u001b[0m \u001b[43m    \u001b[49m\u001b[43menable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    592\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexclude\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexclude\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    593\u001b[0m \u001b[43m    \u001b[49m\u001b[43mauto_fill\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mauto_fill\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    594\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    595\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmeta\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmeta\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    596\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    597\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m nlp\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\resumeparser\\lib\\site-packages\\spacy\\language.py:1889\u001b[0m, in \u001b[0;36mLanguage.from_config\u001b[1;34m(cls, config, vocab, disable, enable, exclude, meta, auto_fill, validate)\u001b[0m\n\u001b[0;32m   1886\u001b[0m     factory \u001b[38;5;241m=\u001b[39m pipe_cfg\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfactory\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1887\u001b[0m     \u001b[38;5;66;03m# The pipe name (key in the config) here is the unique name\u001b[39;00m\n\u001b[0;32m   1888\u001b[0m     \u001b[38;5;66;03m# of the component, not necessarily the factory\u001b[39;00m\n\u001b[1;32m-> 1889\u001b[0m     \u001b[43mnlp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_pipe\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1890\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfactory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1891\u001b[0m \u001b[43m        \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpipe_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1892\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpipe_cfg\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1893\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvalidate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1894\u001b[0m \u001b[43m        \u001b[49m\u001b[43mraw_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mraw_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1895\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1896\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1897\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msource\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m pipe_cfg\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\resumeparser\\lib\\site-packages\\spacy\\language.py:821\u001b[0m, in \u001b[0;36mLanguage.add_pipe\u001b[1;34m(self, factory_name, name, before, after, first, last, source, config, raw_config, validate)\u001b[0m\n\u001b[0;32m    817\u001b[0m     pipe_component, factory_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreate_pipe_from_source(\n\u001b[0;32m    818\u001b[0m         factory_name, source, name\u001b[38;5;241m=\u001b[39mname\n\u001b[0;32m    819\u001b[0m     )\n\u001b[0;32m    820\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 821\u001b[0m     pipe_component \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_pipe\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    822\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfactory_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    823\u001b[0m \u001b[43m        \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    824\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    825\u001b[0m \u001b[43m        \u001b[49m\u001b[43mraw_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mraw_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    826\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvalidate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    827\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    828\u001b[0m pipe_index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_pipe_index(before, after, first, last)\n\u001b[0;32m    829\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pipe_meta[name] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_factory_meta(factory_name)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\resumeparser\\lib\\site-packages\\spacy\\language.py:709\u001b[0m, in \u001b[0;36mLanguage.create_pipe\u001b[1;34m(self, factory_name, name, config, raw_config, validate)\u001b[0m\n\u001b[0;32m    706\u001b[0m cfg \u001b[38;5;241m=\u001b[39m {factory_name: config}\n\u001b[0;32m    707\u001b[0m \u001b[38;5;66;03m# We're calling the internal _fill here to avoid constructing the\u001b[39;00m\n\u001b[0;32m    708\u001b[0m \u001b[38;5;66;03m# registered functions twice\u001b[39;00m\n\u001b[1;32m--> 709\u001b[0m resolved \u001b[38;5;241m=\u001b[39m \u001b[43mregistry\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresolve\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    710\u001b[0m filled \u001b[38;5;241m=\u001b[39m registry\u001b[38;5;241m.\u001b[39mfill({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcfg\u001b[39m\u001b[38;5;124m\"\u001b[39m: cfg[factory_name]}, validate\u001b[38;5;241m=\u001b[39mvalidate)[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcfg\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m    711\u001b[0m filled \u001b[38;5;241m=\u001b[39m Config(filled)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\resumeparser\\lib\\site-packages\\confection\\__init__.py:760\u001b[0m, in \u001b[0;36mregistry.resolve\u001b[1;34m(cls, config, schema, overrides, validate)\u001b[0m\n\u001b[0;32m    751\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[0;32m    752\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mresolve\u001b[39m(\n\u001b[0;32m    753\u001b[0m     \u001b[38;5;28mcls\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    758\u001b[0m     validate: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    759\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Dict[\u001b[38;5;28mstr\u001b[39m, Any]:\n\u001b[1;32m--> 760\u001b[0m     resolved, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    761\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mschema\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mschema\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moverrides\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moverrides\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresolve\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[0;32m    762\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    763\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m resolved\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\resumeparser\\lib\\site-packages\\confection\\__init__.py:809\u001b[0m, in \u001b[0;36mregistry._make\u001b[1;34m(cls, config, schema, overrides, resolve, validate)\u001b[0m\n\u001b[0;32m    807\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_interpolated:\n\u001b[0;32m    808\u001b[0m     config \u001b[38;5;241m=\u001b[39m Config(orig_config)\u001b[38;5;241m.\u001b[39minterpolate()\n\u001b[1;32m--> 809\u001b[0m filled, _, resolved \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fill\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    810\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mschema\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moverrides\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moverrides\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresolve\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresolve\u001b[49m\n\u001b[0;32m    811\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    812\u001b[0m filled \u001b[38;5;241m=\u001b[39m Config(filled, section_order\u001b[38;5;241m=\u001b[39msection_order)\n\u001b[0;32m    813\u001b[0m \u001b[38;5;66;03m# Check that overrides didn't include invalid properties not in config\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\resumeparser\\lib\\site-packages\\confection\\__init__.py:864\u001b[0m, in \u001b[0;36mregistry._fill\u001b[1;34m(cls, config, schema, validate, resolve, parent, overrides)\u001b[0m\n\u001b[0;32m    862\u001b[0m     schema\u001b[38;5;241m.\u001b[39m__fields__[key] \u001b[38;5;241m=\u001b[39m copy_model_field(field, Any)\n\u001b[0;32m    863\u001b[0m promise_schema \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mmake_promise_schema(value, resolve\u001b[38;5;241m=\u001b[39mresolve)\n\u001b[1;32m--> 864\u001b[0m filled[key], validation[v_key], final[key] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fill\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    865\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    866\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpromise_schema\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    867\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    868\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresolve\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresolve\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    869\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkey_parent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    870\u001b[0m \u001b[43m    \u001b[49m\u001b[43moverrides\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moverrides\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    871\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    872\u001b[0m reg_name, func_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mget_constructor(final[key])\n\u001b[0;32m    873\u001b[0m args, kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mparse_args(final[key])\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\resumeparser\\lib\\site-packages\\confection\\__init__.py:881\u001b[0m, in \u001b[0;36mregistry._fill\u001b[1;34m(cls, config, schema, validate, resolve, parent, overrides)\u001b[0m\n\u001b[0;32m    878\u001b[0m     getter \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mget(reg_name, func_name)\n\u001b[0;32m    879\u001b[0m     \u001b[38;5;66;03m# We don't want to try/except this and raise our own error\u001b[39;00m\n\u001b[0;32m    880\u001b[0m     \u001b[38;5;66;03m# here, because we want the traceback if the function fails.\u001b[39;00m\n\u001b[1;32m--> 881\u001b[0m     getter_result \u001b[38;5;241m=\u001b[39m getter(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    882\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    883\u001b[0m     \u001b[38;5;66;03m# We're not resolving and calling the function, so replace\u001b[39;00m\n\u001b[0;32m    884\u001b[0m     \u001b[38;5;66;03m# the getter_result with a Promise class\u001b[39;00m\n\u001b[0;32m    885\u001b[0m     getter_result \u001b[38;5;241m=\u001b[39m Promise(\n\u001b[0;32m    886\u001b[0m         registry\u001b[38;5;241m=\u001b[39mreg_name, name\u001b[38;5;241m=\u001b[39mfunc_name, args\u001b[38;5;241m=\u001b[39margs, kwargs\u001b[38;5;241m=\u001b[39mkwargs\n\u001b[0;32m    887\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\resumeparser\\lib\\site-packages\\spacy_transformers\\architectures.py:235\u001b[0m, in \u001b[0;36mcreate_TransformerModel_v3\u001b[1;34m(name, get_spans, tokenizer_config, transformer_config, mixed_precision, grad_scaler_config)\u001b[0m\n\u001b[0;32m    200\u001b[0m \u001b[38;5;129m@registry\u001b[39m\u001b[38;5;241m.\u001b[39marchitectures\u001b[38;5;241m.\u001b[39mregister(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspacy-transformers.TransformerModel.v3\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    201\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate_TransformerModel_v3\u001b[39m(\n\u001b[0;32m    202\u001b[0m     name: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    207\u001b[0m     grad_scaler_config: \u001b[38;5;28mdict\u001b[39m \u001b[38;5;241m=\u001b[39m {},\n\u001b[0;32m    208\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Model[List[Doc], FullTransformerBatch]:\n\u001b[0;32m    209\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Pretrained transformer model that can be finetuned for downstream tasks.\u001b[39;00m\n\u001b[0;32m    210\u001b[0m \n\u001b[0;32m    211\u001b[0m \u001b[38;5;124;03m    name (str): Name of the pretrained Huggingface model to use.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    233\u001b[0m \u001b[38;5;124;03m        `growth_interval` steps.\u001b[39;00m\n\u001b[0;32m    234\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 235\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mTransformerModel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    236\u001b[0m \u001b[43m        \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    237\u001b[0m \u001b[43m        \u001b[49m\u001b[43mget_spans\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    238\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtokenizer_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    239\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtransformer_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    240\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmixed_precision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    241\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrad_scaler_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    242\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    243\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\resumeparser\\lib\\site-packages\\spacy_transformers\\layers\\transformer_model.py:45\u001b[0m, in \u001b[0;36mTransformerModel.__init__\u001b[1;34m(self, name, get_spans, tokenizer_config, transformer_config, mixed_precision, grad_scaler_config)\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;124;03mget_spans (Callable[[List[Doc]], List[Span]]):\u001b[39;00m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;124;03m    A function to extract spans from the batch of Doc objects.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[38;5;124;03mtransformer_config (dict): Settings to pass to the transformers forward pass.\u001b[39;00m\n\u001b[0;32m     43\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     44\u001b[0m hf_model \u001b[38;5;241m=\u001b[39m HFObjects(\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m, tokenizer_config, transformer_config)\n\u001b[1;32m---> 45\u001b[0m wrapper \u001b[38;5;241m=\u001b[39m \u001b[43mHFWrapper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     46\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhf_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     47\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconvert_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_convert_transformer_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     48\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconvert_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_convert_transformer_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     49\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmixed_precision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmixed_precision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     50\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_scaler_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrad_scaler_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     51\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\n\u001b[0;32m     53\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtransformer\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     54\u001b[0m     forward,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     66\u001b[0m     },\n\u001b[0;32m     67\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\resumeparser\\lib\\site-packages\\spacy_transformers\\layers\\hf_wrapper.py:55\u001b[0m, in \u001b[0;36mHFWrapper\u001b[1;34m(hf_model, convert_inputs, convert_outputs, mixed_precision, grad_scaler_config, config_cls, model_cls, tokenizer_cls)\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m convert_outputs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     48\u001b[0m     convert_outputs \u001b[38;5;241m=\u001b[39m convert_pytorch_default_outputs\n\u001b[0;32m     50\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m Model(\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhf-pytorch\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     52\u001b[0m     pt_forward,\n\u001b[0;32m     53\u001b[0m     attrs\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconvert_inputs\u001b[39m\u001b[38;5;124m\"\u001b[39m: convert_inputs, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconvert_outputs\u001b[39m\u001b[38;5;124m\"\u001b[39m: convert_outputs},\n\u001b[0;32m     54\u001b[0m     shims\u001b[38;5;241m=\u001b[39m[\n\u001b[1;32m---> 55\u001b[0m         \u001b[43mHFShim\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     56\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhf_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     57\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmixed_precision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmixed_precision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     58\u001b[0m \u001b[43m            \u001b[49m\u001b[43mgrad_scaler_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrad_scaler_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     59\u001b[0m \u001b[43m            \u001b[49m\u001b[43mconfig_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     60\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmodel_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     61\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtokenizer_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     62\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     63\u001b[0m     ],\n\u001b[0;32m     64\u001b[0m     dims\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnI\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnO\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mNone\u001b[39;00m},\n\u001b[0;32m     65\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\resumeparser\\lib\\site-packages\\spacy_transformers\\layers\\hf_shim.py:47\u001b[0m, in \u001b[0;36mHFShim.__init__\u001b[1;34m(self, model, config, optimizer, mixed_precision, grad_scaler_config, config_cls, model_cls, tokenizer_cls)\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menabled\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m grad_scaler_config:\n\u001b[0;32m     40\u001b[0m     grad_scaler_config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menabled\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m mixed_precision\n\u001b[0;32m     42\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\n\u001b[0;32m     43\u001b[0m     model\u001b[38;5;241m.\u001b[39mtransformer,\n\u001b[0;32m     44\u001b[0m     config,\n\u001b[0;32m     45\u001b[0m     optimizer,\n\u001b[0;32m     46\u001b[0m     mixed_precision,\n\u001b[1;32m---> 47\u001b[0m     grad_scaler\u001b[38;5;241m=\u001b[39mPyTorchGradScaler(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mgrad_scaler_config),\n\u001b[0;32m     48\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\resumeparser\\lib\\site-packages\\thinc\\shims\\pytorch_grad_scaler.py:54\u001b[0m, in \u001b[0;36mPyTorchGradScaler.__init__\u001b[1;34m(self, enabled, init_scale, backoff_factor, growth_factor, growth_interval)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backoff_factor \u001b[38;5;241m=\u001b[39m backoff_factor\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_growth_interval \u001b[38;5;241m=\u001b[39m growth_interval\n\u001b[1;32m---> 54\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_growth_tracker \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfull\u001b[49m((\u001b[38;5;241m1\u001b[39m,), \u001b[38;5;241m0\u001b[39m, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mint)\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_scale \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfull((\u001b[38;5;241m1\u001b[39m,), init_scale)\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_found_inf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'full'"
     ]
    }
   ],
   "source": [
    "# (which is NVIDIA-specific)\n",
    "path = 'trainning/_model/hyper_model/model-best'\n",
    "model = spacy.load(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test example\n",
    "main_str = '''Manufacturing Production Manager Resume\n",
    "Desired Industry Manufacturing\n",
    "SpiderID 78692\n",
    "Desired Job Location Windsor Colorado\n",
    "Date Posted 1 23 2017\n",
    "Type of Position Full Time Permanent\n",
    "Availability Date Immediately\n",
    "Desired Wage 95000\n",
    "U S Work Authorization Yes\n",
    "Job Level Management Manager Director\n",
    "Willing to Travel\n",
    "Highest Degree Attained Other\n",
    "Willing to Relocate Yes\n",
    "Objective Dynamic and growth driven professional offering hands on management experience and comprehensive background in manufacturing and engineering operations within highly competitive setting Adept at reengineering unproductive work processes as well as in planning and implementing various sustainable and cost effective work programs to drive continuous improvement of operations Armed with exceptional organizational and critical problem solving aptitudes to formulate effective solutions on complex production and quality issues Equipped with tactical leadership capabilities in supervising and guiding teams toward the successful and timely completion of projects Proficient with Microsoft Office applications Project Management and Microsoft Visio\n",
    "Experience Relevant ExperienceVestas Blades America Windsor COProduction Engineer Jan 2014 PresentContribute efforts in achieving production plan budget tooling equipment and bill of material along with quality control and safety Conceptualized new methods to optimize production levels while maintaining production costs yields quality and safety Identified and resolved process problems with effective solutions which decreased downtime and minimized costs Initiated plant trials to measure performance capabilities while ensuring updated documentation of process procedures Leveraged industry expertise in streamlining the manufacturing of turbine blades Production Supervisor Mar 2009 Jan 2014Rendered oversight to more than 70 employees to guarantee accordance of operation with production plan and goals Observed strict compliance with safety and quality guidelines and handled inventory control created schedules delegated work and facilitated training of staff Generated production reports for production and operations managers regarding production areas performance Served as a Shells Lighthouse project member while drafting and modifying all standard operating procedures Functioned as department lead for production quality training and process improvements Closely monitored operations productivity to determine areas for improvement in overall production process Pioneered the development of all shells production process job cards that decreased process times and improved efficiency and quality through changed production flow layout Anheuser Busch Fort Collis COBrew House Control Panel Operator Mar 2007 Mar 2009Efficiently administered beer brewing process from raw material selection and recipe formulation through the use of Siemens software Strictly enforced standard operating procedures and safe working practices Determined and evaluated all critical control points to achieve consistent product quality within allotted schedules Other ExperienceUnited States Airways Denver CoCustomer Service Representative Kroger Co King Soopers Smiths Food Drug Fort Collins CoGrocery Manager Head Clerk Night Crew Manager Front End ManagerInventory Control Manager Warehouse Manager\n",
    "Education EducationAssociate of Science with emphasis in chemistry and biologyFront Range Community College Fort Collins COPharmacy Pre Requisites for Doctor of Pharmacy Pharm D University of Wyoming Laramie WYPharmacy Pre Requisites for Doctor of Pharmacy Pharm D Western Wyoming Community College Rock Springs WY Deans Honor RollProfessional DevelopmentCertificationsSupervisor Certification Six Sigma Yellow Belt Project ManagementQuality Management Crucial Conversations Fort Lift License Crane License\n",
    "Affiliations\n",
    "Skills Manufacturing Production Manager Project Planning Cost Reduction and Budget Optimization Resource Allocation Six Sigma Quality ControlLean Manufacturing Plant Management Manufacturing Inspection Administration Cross functional Team Building\n",
    "Additional Information TrainingProduction Instructor Coordinator Planner Coordinator Wrote Training Document\n",
    "Reference Available upon request\n",
    "Candidate Contact Information\n",
    "JobSpider com has chosen not to make contact information available on this page Click Contact Candidate to send this candidate a response\n",
    "Manufacturing Production Manager Resume\n",
    "Desired Industry Manufacturing\n",
    "SpiderID 78692\n",
    "Desired Job Location Windsor Colorado\n",
    "Date Posted 1 23 2017\n",
    "Type of Position Full Time Permanent\n",
    "Availability Date Immediately\n",
    "Desired Wage 95000\n",
    "U S Work Authorization Yes\n",
    "Job Level Management Manager Director\n",
    "Willing to Travel\n",
    "Highest Degree Attained Other\n",
    "Willing to Relocate Yes\n",
    "Objective Dynamic and growth driven professional offering hands on management experience and comprehensive background in manufacturing and engineering operations within highly competitive setting Adept at reengineering unproductive work processes as well as in planning and implementing various sustainable and cost effective work programs to drive continuous improvement of operations Armed with exceptional organizational and critical problem solving aptitudes to formulate effective solutions on complex production and quality issues Equipped with tactical leadership capabilities in supervising and guiding teams toward the successful and timely completion of projects Proficient with Microsoft Office applications Project Management and Microsoft Visio\n",
    "Experience Relevant ExperienceVestas Blades America Windsor COProduction Engineer Jan 2014 PresentContribute efforts in achieving production plan budget tooling equipment and bill of material along with quality control and safety Conceptualized new methods to optimize production levels while maintaining production costs yields quality and safety Identified and resolved process problems with effective solutions which decreased downtime and minimized costs Initiated plant trials to measure performance capabilities while ensuring updated documentation of process procedures Leveraged industry expertise in streamlining the manufacturing of turbine blades Production Supervisor Mar 2009 Jan 2014Rendered oversight to more than 70 employees to guarantee accordance of operation with production plan and goals Observed strict compliance with safety and quality guidelines and handled inventory control created schedules delegated work and facilitated training of staff Generated production reports for production and operations managers regarding production areas performance Served as a Shells Lighthouse project member while drafting and modifying all standard operating procedures Functioned as department lead for production quality training and process improvements Closely monitored operations productivity to determine areas for improvement in overall production process Pioneered the development of all shells production process job cards that decreased process times and improved efficiency and quality through changed production flow layout Anheuser Busch Fort Collis COBrew House Control Panel Operator Mar 2007 Mar 2009Efficiently administered beer brewing process from raw material selection and recipe formulation through the use of Siemens software Strictly enforced standard operating procedures and safe working practices Determined and evaluated all critical control points to achieve consistent product quality within allotted schedules Other ExperienceUnited States Airways Denver CoCustomer Service Representative Kroger Co King Soopers Smiths Food Drug Fort Collins CoGrocery Manager Head Clerk Night Crew Manager Front End ManagerInventory Control Manager Warehouse Manager\n",
    "Education EducationAssociate of Science with emphasis in chemistry and biologyFront Range Community College Fort Collins COPharmacy Pre Requisites for Doctor of Pharmacy Pharm D University of Wyoming Laramie WYPharmacy Pre Requisites for Doctor of Pharmacy Pharm D Western Wyoming Community College Rock Springs WY Deans Honor RollProfessional DevelopmentCertificationsSupervisor Certification Six Sigma Yellow Belt Project ManagementQuality Management Crucial Conversations Fort Lift License Crane License\n",
    "Affiliations\n",
    "Skills Manufacturing Production Manager Project Planning Cost Reduction and Budget Optimization Resource Allocation Six Sigma Quality ControlLean Manufacturing Plant Management Manufacturing Inspection Administration Cross functional Team Building\n",
    "Additional Information TrainingProduction Instructor Coordinator Planner Coordinator Wrote Training Document\n",
    "Reference Available upon request\n",
    "Candidate Contact Information\n",
    "JobSpider com has chosen not to make contact information available on this page Click Contact Candidate to send this candidate a response'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('7', 'CARDINAL')]\n",
      "[('8', 'CARDINAL')]\n",
      "[('6', 'CARDINAL')]\n",
      "[('9', 'CARDINAL')]\n",
      "[('2', 'CARDINAL')]\n",
      "[('1', 'CARDINAL')]\n",
      "[('2', 'CARDINAL')]\n",
      "[('3', 'CARDINAL')]\n",
      "[('2', 'CARDINAL')]\n",
      "[('0', 'CARDINAL')]\n",
      "[('1', 'CARDINAL')]\n",
      "[('7', 'CARDINAL')]\n",
      "[('F', 'ORG')]\n",
      "[('9', 'CARDINAL')]\n",
      "[('5', 'CARDINAL')]\n",
      "[('0', 'CARDINAL')]\n",
      "[('0', 'CARDINAL')]\n",
      "[('0', 'CARDINAL')]\n",
      "[('2', 'CARDINAL')]\n",
      "[('0', 'CARDINAL')]\n",
      "[('1', 'CARDINAL')]\n",
      "[('4', 'CARDINAL')]\n",
      "[('2', 'CARDINAL')]\n",
      "[('0', 'CARDINAL')]\n",
      "[('0', 'CARDINAL')]\n",
      "[('9', 'CARDINAL')]\n",
      "[('2', 'CARDINAL')]\n",
      "[('0', 'CARDINAL')]\n",
      "[('1', 'CARDINAL')]\n",
      "[('4', 'CARDINAL')]\n",
      "[('7', 'CARDINAL')]\n",
      "[('0', 'CARDINAL')]\n",
      "[('F', 'ORG')]\n",
      "[('F', 'ORG')]\n",
      "[('2', 'CARDINAL')]\n",
      "[('0', 'CARDINAL')]\n",
      "[('0', 'CARDINAL')]\n",
      "[('7', 'CARDINAL')]\n",
      "[('2', 'CARDINAL')]\n",
      "[('0', 'CARDINAL')]\n",
      "[('0', 'CARDINAL')]\n",
      "[('9', 'CARDINAL')]\n",
      "[('F', 'ORG')]\n",
      "[('F', 'ORG')]\n",
      "[('F', 'ORG')]\n",
      "[('F', 'ORG')]\n",
      "[('F', 'ORG')]\n",
      "[('F', 'ORG')]\n",
      "[('7', 'CARDINAL')]\n",
      "[('8', 'CARDINAL')]\n",
      "[('6', 'CARDINAL')]\n",
      "[('9', 'CARDINAL')]\n",
      "[('2', 'CARDINAL')]\n",
      "[('1', 'CARDINAL')]\n",
      "[('2', 'CARDINAL')]\n",
      "[('3', 'CARDINAL')]\n",
      "[('2', 'CARDINAL')]\n",
      "[('0', 'CARDINAL')]\n",
      "[('1', 'CARDINAL')]\n",
      "[('7', 'CARDINAL')]\n",
      "[('F', 'ORG')]\n",
      "[('9', 'CARDINAL')]\n",
      "[('5', 'CARDINAL')]\n",
      "[('0', 'CARDINAL')]\n",
      "[('0', 'CARDINAL')]\n",
      "[('0', 'CARDINAL')]\n",
      "[('2', 'CARDINAL')]\n",
      "[('0', 'CARDINAL')]\n",
      "[('1', 'CARDINAL')]\n",
      "[('4', 'CARDINAL')]\n",
      "[('2', 'CARDINAL')]\n",
      "[('0', 'CARDINAL')]\n",
      "[('0', 'CARDINAL')]\n",
      "[('9', 'CARDINAL')]\n",
      "[('2', 'CARDINAL')]\n",
      "[('0', 'CARDINAL')]\n",
      "[('1', 'CARDINAL')]\n",
      "[('4', 'CARDINAL')]\n",
      "[('7', 'CARDINAL')]\n",
      "[('0', 'CARDINAL')]\n",
      "[('F', 'ORG')]\n",
      "[('F', 'ORG')]\n",
      "[('2', 'CARDINAL')]\n",
      "[('0', 'CARDINAL')]\n",
      "[('0', 'CARDINAL')]\n",
      "[('7', 'CARDINAL')]\n",
      "[('2', 'CARDINAL')]\n",
      "[('0', 'CARDINAL')]\n",
      "[('0', 'CARDINAL')]\n",
      "[('9', 'CARDINAL')]\n",
      "[('F', 'ORG')]\n",
      "[('F', 'ORG')]\n",
      "[('F', 'ORG')]\n",
      "[('F', 'ORG')]\n",
      "[('F', 'ORG')]\n",
      "[('F', 'ORG')]\n"
     ]
    }
   ],
   "source": [
    "for doc in model.pipe(main_str, disable=[\"tagger\", \"parser\"]):\n",
    "  for ent in doc.ents:\n",
    "    print([(ent.text, ent.label_) for ent in doc.ents])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Manufacturing Production Manager Resume\\nDesired Industry Manufacturing\\nSpiderID 78692\\nDesired Job Location Windsor Colorado\\nDate Posted 1 23 2017\\nType of Position Full Time Permanent\\nAvailability Date Immediately\\nDesired Wage 95000\\nU S Work Authorization Yes\\nJob Level Management Manager Director\\nWilling to Travel\\nHighest Degree Attained Other\\nWilling to Relocate Yes\\nObjective Dynamic and growth driven professional offering hands on management experience and comprehensive background in manufacturing and engineering operations within highly competitive setting Adept at reengineering unproductive work processes as well as in planning and implementing various sustainable and cost effective work programs to drive continuous improvement of operations Armed with exceptional organizational and critical problem solving aptitudes to formulate effective solutions on complex production and quality issues Equipped with tactical leadership capabilities in supervising and guiding teams toward the successful and timely completion of projects Proficient with Microsoft Office applications Project Management and Microsoft Visio\\nExperience Relevant ExperienceVestas Blades America Windsor COProduction Engineer Jan 2014 PresentContribute efforts in achieving production plan budget tooling equipment and bill of material along with quality control and safety Conceptualized new methods to optimize production levels while maintaining production costs yields quality and safety Identified and resolved process problems with effective solutions which decreased downtime and minimized costs Initiated plant trials to measure performance capabilities while ensuring updated documentation of process procedures Leveraged industry expertise in streamlining the manufacturing of turbine blades Production Supervisor Mar 2009 Jan 2014Rendered oversight to more than 70 employees to guarantee accordance of operation with production plan and goals Observed strict compliance with safety and quality guidelines and handled inventory control created schedules delegated work and facilitated training of staff Generated production reports for production and operations managers regarding production areas performance Served as a Shells Lighthouse project member while drafting and modifying all standard operating procedures Functioned as department lead for production quality training and process improvements Closely monitored operations productivity to determine areas for improvement in overall production process Pioneered the development of all shells production process job cards that decreased process times and improved efficiency and quality through changed production flow layout Anheuser Busch Fort Collis COBrew House Control Panel Operator Mar 2007 Mar 2009Efficiently administered beer brewing process from raw material selection and recipe formulation through the use of Siemens software Strictly enforced standard operating procedures and safe working practices Determined and evaluated all critical control points to achieve consistent product quality within allotted schedules Other ExperienceUnited States Airways Denver CoCustomer Service Representative Kroger Co King Soopers Smiths Food Drug Fort Collins CoGrocery Manager Head Clerk Night Crew Manager Front End ManagerInventory Control Manager Warehouse Manager\\nEducation EducationAssociate of Science with emphasis in chemistry and biologyFront Range Community College Fort Collins COPharmacy Pre Requisites for Doctor of Pharmacy Pharm D University of Wyoming Laramie WYPharmacy Pre Requisites for Doctor of Pharmacy Pharm D Western Wyoming Community College Rock Springs WY Deans Honor RollProfessional DevelopmentCertificationsSupervisor Certification Six Sigma Yellow Belt Project ManagementQuality Management Crucial Conversations Fort Lift License Crane License\\nAffiliations\\nSkills Manufacturing Production Manager Project Planning Cost Reduction and Budget Optimization Resource Allocation Six Sigma Quality ControlLean Manufacturing Plant Management Manufacturing Inspection Administration Cross functional Team Building\\nAdditional Information TrainingProduction Instructor Coordinator Planner Coordinator Wrote Training Document\\nReference Available upon request\\nCandidate Contact Information\\nJobSpider com has chosen not to make contact information available on this page Click Contact Candidate to send this candidate a response\\nManufacturing Production Manager Resume\\nDesired Industry Manufacturing\\nSpiderID 78692\\nDesired Job Location Windsor Colorado\\nDate Posted 1 23 2017\\nType of Position Full Time Permanent\\nAvailability Date Immediately\\nDesired Wage 95000\\nU S Work Authorization Yes\\nJob Level Management Manager Director\\nWilling to Travel\\nHighest Degree Attained Other\\nWilling to Relocate Yes\\nObjective Dynamic and growth driven professional offering hands on management experience and comprehensive background in manufacturing and engineering operations within highly competitive setting Adept at reengineering unproductive work processes as well as in planning and implementing various sustainable and cost effective work programs to drive continuous improvement of operations Armed with exceptional organizational and critical problem solving aptitudes to formulate effective solutions on complex production and quality issues Equipped with tactical leadership capabilities in supervising and guiding teams toward the successful and timely completion of projects Proficient with Microsoft Office applications Project Management and Microsoft Visio\\nExperience Relevant ExperienceVestas Blades America Windsor COProduction Engineer Jan 2014 PresentContribute efforts in achieving production plan budget tooling equipment and bill of material along with quality control and safety Conceptualized new methods to optimize production levels while maintaining production costs yields quality and safety Identified and resolved process problems with effective solutions which decreased downtime and minimized costs Initiated plant trials to measure performance capabilities while ensuring updated documentation of process procedures Leveraged industry expertise in streamlining the manufacturing of turbine blades Production Supervisor Mar 2009 Jan 2014Rendered oversight to more than 70 employees to guarantee accordance of operation with production plan and goals Observed strict compliance with safety and quality guidelines and handled inventory control created schedules delegated work and facilitated training of staff Generated production reports for production and operations managers regarding production areas performance Served as a Shells Lighthouse project member while drafting and modifying all standard operating procedures Functioned as department lead for production quality training and process improvements Closely monitored operations productivity to determine areas for improvement in overall production process Pioneered the development of all shells production process job cards that decreased process times and improved efficiency and quality through changed production flow layout Anheuser Busch Fort Collis COBrew House Control Panel Operator Mar 2007 Mar 2009Efficiently administered beer brewing process from raw material selection and recipe formulation through the use of Siemens software Strictly enforced standard operating procedures and safe working practices Determined and evaluated all critical control points to achieve consistent product quality within allotted schedules Other ExperienceUnited States Airways Denver CoCustomer Service Representative Kroger Co King Soopers Smiths Food Drug Fort Collins CoGrocery Manager Head Clerk Night Crew Manager Front End ManagerInventory Control Manager Warehouse Manager\\nEducation EducationAssociate of Science with emphasis in chemistry and biologyFront Range Community College Fort Collins COPharmacy Pre Requisites for Doctor of Pharmacy Pharm D University of Wyoming Laramie WYPharmacy Pre Requisites for Doctor of Pharmacy Pharm D Western Wyoming Community College Rock Springs WY Deans Honor RollProfessional DevelopmentCertificationsSupervisor Certification Six Sigma Yellow Belt Project ManagementQuality Management Crucial Conversations Fort Lift License Crane License\\nAffiliations\\nSkills Manufacturing Production Manager Project Planning Cost Reduction and Budget Optimization Resource Allocation Six Sigma Quality ControlLean Manufacturing Plant Management Manufacturing Inspection Administration Cross functional Team Building\\nAdditional Information TrainingProduction Instructor Coordinator Planner Coordinator Wrote Training Document\\nReference Available upon request\\nCandidate Contact Information\\nJobSpider com has chosen not to make contact information available on this page Click Contact Candidate to send this candidate a response'"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "main_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'PERSON': ['Resume\\nDesired Industry Manufacturing', 'Served', 'Functioned', 'Fort Collis', 'Mar', 'Clerk Night Crew', 'Resume\\nDesired Industry Manufacturing', 'Served', 'Functioned', 'Fort Collis', 'Mar', 'Clerk Night Crew'], 'DATE': ['78692', '2017', 'Jan 2014', '2009', '2007', '78692', '2017', 'Jan 2014', '2009', '2007'], 'GPE': ['Colorado', 'Proficient', 'Colorado', 'Proficient'], 'ORG': ['U S Work Authorization', 'Relocate', 'Adept', 'Microsoft Office', 'Project Management', 'Microsoft Visio', 'Initiated', 'Shells Lighthouse', 'Anheuser Busch', 'COBrew House', 'Siemens', 'Strictly', 'ExperienceUnited States Airways', 'Denver CoCustomer Service', 'Kroger Co King Soopers Smiths Food Drug Fort Collins CoGrocery', 'Range Community College', 'Pharmacy Pharm D University of Wyoming Laramie WYPharmacy Pre Requisites', 'Rock Springs', 'Skills Manufacturing Production', 'Project Planning Cost Reduction and Budget Optimization Resource Allocation Six Sigma Quality ControlLean Manufacturing Plant Management Manufacturing Inspection Administration Cross', 'Team Building\\nAdditional Information TrainingProduction Instructor Coordinator Planner', 'Candidate Contact Information', 'JobSpider', 'Click Contact Candidate', 'U S Work Authorization', 'Relocate', 'Adept', 'Microsoft Office', 'Project Management', 'Microsoft Visio', 'Initiated', 'Shells Lighthouse', 'Anheuser Busch', 'COBrew House', 'Siemens', 'Strictly', 'ExperienceUnited States Airways', 'Denver CoCustomer Service', 'Kroger Co King Soopers Smiths Food Drug Fort Collins CoGrocery', 'Range Community College', 'Pharmacy Pharm D University of Wyoming Laramie WYPharmacy Pre Requisites', 'Rock Springs', 'Skills Manufacturing Production', 'Project Planning Cost Reduction and Budget Optimization Resource Allocation Six Sigma Quality ControlLean Manufacturing Plant Management Manufacturing Inspection Administration Cross', 'Team Building\\nAdditional Information TrainingProduction Instructor Coordinator Planner', 'Candidate Contact Information', 'JobSpider', 'Click Contact Candidate'], 'PRODUCT': ['Conceptualized', 'Determined', 'Conceptualized', 'Determined'], 'CARDINAL': ['more than 70', 'more than 70'], 'NORP': ['Deans', 'Deans']}\n"
     ]
    }
   ],
   "source": [
    "def extract_entity_sections_grad_(text):\n",
    "    # Load the spaCy model\n",
    "    nlp = spacy.load('en_core_web_sm')\n",
    "    \n",
    "    # Process the input text\n",
    "    doc = nlp(text)\n",
    "\n",
    "    # Initialize an empty dictionary to store entities\n",
    "    entities = {}\n",
    "\n",
    "    # Iterate over the entities found in the doc\n",
    "    for ent in doc.ents:\n",
    "        # If the entity label is not already in the dictionary, add it with the entity text as a list\n",
    "        if ent.label_ not in entities:\n",
    "            entities[ent.label_] = [ent.text]\n",
    "        else:\n",
    "            # Otherwise, append the entity text to the existing list\n",
    "            entities[ent.label_].append(ent.text)\n",
    "    \n",
    "    return entities\n",
    "\n",
    "print(extract_entity_sections_grad_(main_str))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\Lenovo\\\\learning\\\\Resume Analyser\\\\4.version'"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Michael Smith \\nBI / Big Data/ Azure \\nManchester, UK- Email me on Indeed: indeed.com/r/falicent/140749dace5dc26f \\n\\n10+  years  of  Experience  in  Designing,  Development,  Administration,  Analysis, \\nManagement  inthe  Business  Intelligence  Data  warehousing,  Client  Server \\nTechnologies, Web-based Applications, cloud solutions and Databases. \\nData warehouse: Data analysis, star/ snow flake schema data modeling and design \\nspecific todata warehousing and business intelligence environment. \\nDatabase:  Experience  in  database  designing,  scalability,  back-up  and  recovery, \\nwriting andoptimizing SQL code and Stored Procedures, creating functions, views, \\ntriggers and indexes.  \\nCloud  platform:  Worked  on  Microsoft  Azure  cloud  services  like  Document  DB,  SQL \\nAzure, StreamAnalytics, Event hub,  Power BI, Web Job, Web App, Power BI, Azure \\ndata lake analytics(U-SQL). \\nBig Data: Worked Azure data lake store/analytics for big data processing and Azure \\ndata factoryto schedule U-SQL jobs. Designed and developed end to end  big data \\nsolution for data insights.  \\n\\nWilling to relocate: Anywhere \\nWORK EXPERIENCESoftware Engineer \\nMicrosoft - Manchester, UK. \\nDecember 2015 to Present \\n1. Microsoft Rewards Live dashboards: \\nDescription:  -  Microsoft  rewards  is  loyalty  program  that  rewards  Users  for \\nbrowsing  and  shopping  online.  Microsoft  Rewards  members  can  earn  points  when \\nsearching  with  Bing,  browsing  with  Microsoft  Edge  and  making  purchases  at  the \\nXbox  Store,  the  Windows  Store  and  the  Microsoft  Store.  Plus,  user  can  pick  up \\nbonus points for taking daily quizzes and tours on the Microsoft rewards website. \\nRewards live dashboards gives a live picture of usage world-wide and by markets \\nlike  US,  Canada,  Australia,  new  user  registration  count,  top/bottom  performing \\nrewards offers, orders stats and weekly trends of user activities, orders and new \\nuser registrations. the PBI tiles gets refreshed in different frequencies starting \\nfrom 5 seconds to 30 minutes. \\nTechnology/Tools used \\nEvent hub, stream analytics and Power BI. \\nResponsibilities \\nCreated stream analytics jobs to process event hub data \\nCreated Power BI live dashboard to show live usage traffic, weekly trends, cards, \\ncharts to showtop/bottom 10 offers and usage metrics. \\n2. Microsoft Rewards Data Insights: \\nDescription:  -  Microsoft  rewards  is  loyalty  program  that  rewards  Users  for \\nbrowsing  and  shopping  online.  Microsoft  Rewards  members  can  earn  points  when \\nsearching  with  Bing,  browsing  with  Microsoft  Edge  and  making  purchases  at  the \\nXbox  Store,  the  Windows  Store  and  the  Microsoft  Store.  Plus,  user  can  pick  up \\nbonus points for taking daily quizzes and tours on the Microsoft rewards website. \\nRewards  data  insights  is  data  analytics  and  reporting  platform,  processes  20 \\nmillion users daily activities and redemption across different markets like US, \\nCanada, Australia. \\nTechnology/Tools used \\nCosmos (Microsoft big-data platform), c#, X-flow job monitoring, Power BI. \\nResponsibilities \\n\\n \\n \\n\\x0c Created big data scripts in cosmos \\nC# data extractors, processors and reducers for data transformation \\nPower BI dashboards \\n3. End to end tracking Tool: \\nDescription:  -  This  is  real-time  Tracking  tool  to  track  different  business \\ntransactions  like  order,  order  response,  functional  acknowledgement,  invoice \\nflowing inside ICOE. It gives flexibility to customers to track their transactions \\nand appropriate error information in-case of any failure. Based on resource based \\naccess control the tool gives flexibility to end user to perform different actions \\nlike  view  transactions,  search  based  on  different  filter  criteria  and  view  and \\ndownload  actual  message  payload.  End  to  end  tracking  tool  stitches  all  the \\nbusiness transaction like order to cash flow and connects different hops inside \\nICOE like gateway, routing server, Processing server. It also connects different \\nsystems like ICOE, partner end point and SAP. \\nTechnology/Tools used \\nAzure Document db, Azure web job and Web APP, RBAC, Angular JS. \\nResponsibilities \\nDocument dB stored procedures. \\nWeb job to process event hub data and populate Document db Web App API. \\nStream analytics job to transform data \\nPower BI reports \\n4. Biztrack Tracking Tool: \\nDescription:  -  This  is  real-time  Tracking  tool  to  track  different  business \\ntransactions  like  order,  order  response,  functional  acknowledgement,  invoice \\nflowing inside ICOE. It gives flexibility to customers to track their transactions \\nand appropriate error information in-case of any failure. Based on resource based \\naccess control the tool gives flexibility to end user to perform different actions \\nlike  view  transactions,  search  based  on  different  filter  criteria  and  view  and \\ndownload actual message payload. \\nTechnology/Tools used \\nSQL server 2014, SSIS, .net API, Angular JS. \\nResponsibilities \\nETL solution to transform business transactions data stored in Biztalk tables. \\nSQL azure tables, stored procedures, User defined functions. \\nPerformance tuning. \\nWeb API enhancements. \\n\\nEDUCATION \\nThe University of Manchester - UK \\n2007 \\n\\nSKILLS \\nproblem solving (Less than 1 year), project lifecycle (Less than 1 year), project \\nmanager (Less than 1 year), technical assistance. (Less than 1 year) \\nADDITIONAL INFORMATION \\nProfessional Skills \\nExcellent  analytical,  problem  solving,  communication,  knowledge  transfer  and \\ninterpersonalskills with ability to interact with individuals at all the levels \\nQuick  learner  and  maintains  cordial  relationship  with  project  manager  and  team \\nmembers andgood performer both in team and independent job environments \\nPositive attitude towards superiors &amp; peers \\n\\n \\n \\n\\x0c Supervised junior developers throughout project lifecycle and provided technical \\nassistance. \\n\\n\\x0c'"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "__text_raw = utils.extract_text('../data/data_summarise_info/test/Smith Resume.pdf')\n",
    "__text_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Michael Smith BI / Big Data/ Azure Manchester, UK- Email me on Indeed: indeed.com/r/falicent/140749dace5dc26f 10+ years of Experience in Designing, Development, Administration, Analysis, Management inthe Business Intelligence Data warehousing, Client Server Technologies, Web-based Applications, cloud solutions and Databases. Data warehouse: Data analysis, star/ snow flake schema data modeling and design specific todata warehousing and business intelligence environment. Database: Experience in database designing, scalability, back-up and recovery, writing andoptimizing SQL code and Stored Procedures, creating functions, views, triggers and indexes. Cloud platform: Worked on Microsoft Azure cloud services like Document DB, SQL Azure, StreamAnalytics, Event hub, Power BI, Web Job, Web App, Power BI, Azure data lake analytics(U-SQL). Big Data: Worked Azure data lake store/analytics for big data processing and Azure data factoryto schedule U-SQL jobs. Designed and developed end to end big data solution for data insights. Willing to relocate: Anywhere WORK EXPERIENCESoftware Engineer Microsoft - Manchester, UK. December 2015 to Present 1. Microsoft Rewards Live dashboards: Description: - Microsoft rewards is loyalty program that rewards Users for browsing and shopping online. Microsoft Rewards members can earn points when searching with Bing, browsing with Microsoft Edge and making purchases at the Xbox Store, the Windows Store and the Microsoft Store. Plus, user can pick up bonus points for taking daily quizzes and tours on the Microsoft rewards website. Rewards live dashboards gives a live picture of usage world-wide and by markets like US, Canada, Australia, new user registration count, top/bottom performing rewards offers, orders stats and weekly trends of user activities, orders and new user registrations. the PBI tiles gets refreshed in different frequencies starting from 5 seconds to 30 minutes. Technology/Tools used Event hub, stream analytics and Power BI. Responsibilities Created stream analytics jobs to process event hub data Created Power BI live dashboard to show live usage traffic, weekly trends, cards, charts to showtop/bottom 10 offers and usage metrics. 2. Microsoft Rewards Data Insights: Description: - Microsoft rewards is loyalty program that rewards Users for browsing and shopping online. Microsoft Rewards members can earn points when searching with Bing, browsing with Microsoft Edge and making purchases at the Xbox Store, the Windows Store and the Microsoft Store. Plus, user can pick up bonus points for taking daily quizzes and tours on the Microsoft rewards website. Rewards data insights is data analytics and reporting platform, processes 20 million users daily activities and redemption across different markets like US, Canada, Australia. Technology/Tools used Cosmos (Microsoft big-data platform), c#, X-flow job monitoring, Power BI. Responsibilities Created big data scripts in cosmos C# data extractors, processors and reducers for data transformation Power BI dashboards 3. End to end tracking Tool: Description: - This is real-time Tracking tool to track different business transactions like order, order response, functional acknowledgement, invoice flowing inside ICOE. It gives flexibility to customers to track their transactions and appropriate error information in-case of any failure. Based on resource based access control the tool gives flexibility to end user to perform different actions like view transactions, search based on different filter criteria and view and download actual message payload. End to end tracking tool stitches all the business transaction like order to cash flow and connects different hops inside ICOE like gateway, routing server, Processing server. It also connects different systems like ICOE, partner end point and SAP. Technology/Tools used Azure Document db, Azure web job and Web APP, RBAC, Angular JS. Responsibilities Document dB stored procedures. Web job to process event hub data and populate Document db Web App API. Stream analytics job to transform data Power BI reports 4. Biztrack Tracking Tool: Description: - This is real-time Tracking tool to track different business transactions like order, order response, functional acknowledgement, invoice flowing inside ICOE. It gives flexibility to customers to track their transactions and appropriate error information in-case of any failure. Based on resource based access control the tool gives flexibility to end user to perform different actions like view transactions, search based on different filter criteria and view and download actual message payload. Technology/Tools used SQL server 2014, SSIS, .net API, Angular JS. Responsibilities ETL solution to transform business transactions data stored in Biztalk tables. SQL azure tables, stored procedures, User defined functions. Performance tuning. Web API enhancements. EDUCATION The University of Manchester - UK 2007 SKILLS problem solving (Less than 1 year), project lifecycle (Less than 1 year), project manager (Less than 1 year), technical assistance. (Less than 1 year) ADDITIONAL INFORMATION Professional Skills Excellent analytical, problem solving, communication, knowledge transfer and interpersonalskills with ability to interact with individuals at all the levels Quick learner and maintains cordial relationship with project manager and team members andgood performer both in team and independent job environments Positive attitude towards superiors &amp; peers Supervised junior developers throughout project lifecycle and provided technical assistance.'"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "__text = ' '.join(__text_raw.split())\n",
    "__text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "utils.get_number_of_pages('../data/data_summarise_info/test/Smith Resume.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EXPERIENCE :\n",
      "writing andoptimizing SQL code and Stored Procedures, creating functions, views,\n",
      "    -triggers and indexes.\n",
      "    -Cloud  platform:  Worked  on  Microsoft  Azure  cloud  services  like  Document  DB,  SQL\n",
      "    -Azure, StreamAnalytics, Event hub,  Power BI, Web Job, Web App, Power BI, Azure\n",
      "    -data lake analytics(U-SQL).\n",
      "    -Big Data: Worked Azure data lake store/analytics for big data processing and Azure\n",
      "    -data factoryto schedule U-SQL jobs. Designed and developed end to end  big data\n",
      "    -solution for data insights.\n",
      "    -Willing to relocate: Anywhere\n",
      "    -WORK EXPERIENCESoftware Engineer\n",
      "    -Microsoft - Manchester, UK.\n",
      "    -December 2015 to Present\n",
      "    -1. Microsoft Rewards Live dashboards:\n",
      "    -Description:  -  Microsoft  rewards  is  loyalty  program  that  rewards  Users  for\n",
      "    -browsing  and  shopping  online.  Microsoft  Rewards  members  can  earn  points  when\n",
      "    -searching  with  Bing,  browsing  with  Microsoft  Edge  and  making  purchases  at  the\n",
      "    -Xbox  Store,  the  Windows  Store  and  the  Microsoft  Store.  Plus,  user  can  pick  up\n",
      "    -bonus points for taking daily quizzes and tours on the Microsoft rewards website.\n",
      "    -Rewards live dashboards gives a live picture of usage world-wide and by markets\n",
      "    -like  US,  Canada,  Australia,  new  user  registration  count,  top/bottom  performing\n",
      "    -rewards offers, orders stats and weekly trends of user activities, orders and new\n",
      "    -user registrations. the PBI tiles gets refreshed in different frequencies starting\n",
      "    -from 5 seconds to 30 minutes.\n",
      "    -Technology/Tools used\n",
      "    -Event hub, stream analytics and Power BI.\n",
      "    -Responsibilities\n",
      "    -Created stream analytics jobs to process event hub data\n",
      "    -Created Power BI live dashboard to show live usage traffic, weekly trends, cards,\n",
      "    -charts to showtop/bottom 10 offers and usage metrics.\n",
      "    -2. Microsoft Rewards Data Insights:\n",
      "    -Description:  -  Microsoft  rewards  is  loyalty  program  that  rewards  Users  for\n",
      "    -browsing  and  shopping  online.  Microsoft  Rewards  members  can  earn  points  when\n",
      "    -searching  with  Bing,  browsing  with  Microsoft  Edge  and  making  purchases  at  the\n",
      "    -Xbox  Store,  the  Windows  Store  and  the  Microsoft  Store.  Plus,  user  can  pick  up\n",
      "    -bonus points for taking daily quizzes and tours on the Microsoft rewards website.\n",
      "    -Rewards  data  insights  is  data  analytics  and  reporting  platform,  processes  20\n",
      "    -million users daily activities and redemption across different markets like US,\n",
      "    -Canada, Australia.\n",
      "    -Technology/Tools used\n",
      "    -Cosmos (Microsoft big-data platform), c#, X-flow job monitoring, Power BI.\n",
      "    -Responsibilities\n",
      "    -Created big data scripts in cosmos\n",
      "    -C# data extractors, processors and reducers for data transformation\n",
      "    -Power BI dashboards\n",
      "    -3. End to end tracking Tool:\n",
      "    -Description:  -  This  is  real-time  Tracking  tool  to  track  different  business\n",
      "    -transactions  like  order,  order  response,  functional  acknowledgement,  invoice\n",
      "    -flowing inside ICOE. It gives flexibility to customers to track their transactions\n",
      "    -and appropriate error information in-case of any failure. Based on resource based\n",
      "    -access control the tool gives flexibility to end user to perform different actions\n",
      "    -like  view  transactions,  search  based  on  different  filter  criteria  and  view  and\n",
      "    -download  actual  message  payload.  End  to  end  tracking  tool  stitches  all  the\n",
      "    -business transaction like order to cash flow and connects different hops inside\n",
      "    -ICOE like gateway, routing server, Processing server. It also connects different\n",
      "    -systems like ICOE, partner end point and SAP.\n",
      "    -Technology/Tools used\n",
      "    -Azure Document db, Azure web job and Web APP, RBAC, Angular JS.\n",
      "    -Responsibilities\n",
      "    -Document dB stored procedures.\n",
      "    -Web job to process event hub data and populate Document db Web App API.\n",
      "    -Stream analytics job to transform data\n",
      "    -Power BI reports\n",
      "    -4. Biztrack Tracking Tool:\n",
      "    -Description:  -  This  is  real-time  Tracking  tool  to  track  different  business\n",
      "    -transactions  like  order,  order  response,  functional  acknowledgement,  invoice\n",
      "    -flowing inside ICOE. It gives flexibility to customers to track their transactions\n",
      "    -and appropriate error information in-case of any failure. Based on resource based\n",
      "    -access control the tool gives flexibility to end user to perform different actions\n",
      "    -like  view  transactions,  search  based  on  different  filter  criteria  and  view  and\n",
      "    -download actual message payload.\n",
      "    -Technology/Tools used\n",
      "    -SQL server 2014, SSIS, .net API, Angular JS.\n",
      "    -Responsibilities\n",
      "    -ETL solution to transform business transactions data stored in Biztalk tables.\n",
      "    -SQL azure tables, stored procedures, User defined functions.\n",
      "    -Performance tuning.\n",
      "    -Web API enhancements.\n",
      "EDUCATION :\n",
      "The University of Manchester - UK\n",
      "    -2007\n",
      "SKILLS :\n",
      "Excellent  analytical,  problem  solving,  communication,  knowledge  transfer  and\n",
      "    -interpersonalskills with ability to interact with individuals at all the levels\n",
      "    -Quick  learner  and  maintains  cordial  relationship  with  project  manager  and  team\n",
      "    -members andgood performer both in team and independent job environments\n",
      "    -Positive attitude towards superiors &amp; peers\n",
      "    -Supervised junior developers throughout project lifecycle and provided technical\n",
      "    -assistance.\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPkAU8dUR8yw87X90Q5ysWj",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
