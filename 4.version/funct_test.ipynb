{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\learning\\Resume Analyser\\4.version\n"
     ]
    }
   ],
   "source": [
    "cd learning/Resume Analyser/4.version/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Volume in drive C has no label.\n",
      " Volume Serial Number is 7010-C44B\n",
      "\n",
      " Directory of C:\\Users\\Lenovo\\learning\\Resume Analyser\\4.version\n",
      "\n",
      "11/19/2024  01:45 PM    <DIR>          .\n",
      "11/04/2024  08:36 PM    <DIR>          ..\n",
      "11/19/2024  01:37 PM    <DIR>          .ipynb_checkpoints\n",
      "11/19/2024  01:43 PM    <DIR>          __pycache__\n",
      "11/18/2024  10:46 PM             1,903 constants.py\n",
      "11/19/2024  01:45 PM            37,219 funct_test.ipynb\n",
      "11/11/2024  07:34 PM            23,719 HR_analytics.ipynb\n",
      "11/11/2024  07:34 PM         5,213,294 job_final.csv\n",
      "11/04/2024  08:38 PM    <DIR>          marker\n",
      "11/19/2024  01:46 PM               411 prerequisite.py\n",
      "11/18/2024  12:43 PM             3,721 pyresparser.py\n",
      "11/19/2024  01:39 PM                61 requirements.txt\n",
      "11/13/2024  08:30 PM            13,439 skills.csv\n",
      "11/14/2024  10:13 PM    <DIR>          trainning\n",
      "11/18/2024  10:44 PM            15,034 utils.py\n",
      "               9 File(s)      5,308,801 bytes\n",
      "               6 Dir(s)  198,219,657,216 bytes free\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 33968,
     "status": "ok",
     "timestamp": 1731902950171,
     "user": {
      "displayName": "Giang Le Huynh",
      "userId": "12580968046014402630"
     },
     "user_tz": -420
    },
    "id": "iR8NbgiB9yuA",
    "outputId": "976c1300-b9d9-4618-ee8b-6f3f482e96be"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spacy==3.7.5 in c:\\users\\lenovo\\anaconda3\\envs\\resume_parser_project\\lib\\site-packages (from -r requirements.txt (line 1)) (3.7.5)\n",
      "Requirement already satisfied: nltk==3.9.1 in c:\\users\\lenovo\\anaconda3\\envs\\resume_parser_project\\lib\\site-packages (from -r requirements.txt (line 2)) (3.9.1)\n",
      "Requirement already satisfied: docx2txt in c:\\users\\lenovo\\anaconda3\\envs\\resume_parser_project\\lib\\site-packages (from -r requirements.txt (line 3)) (0.8)\n",
      "Requirement already satisfied: pdfminer.six==20240706 in c:\\users\\lenovo\\anaconda3\\envs\\resume_parser_project\\lib\\site-packages (from -r requirements.txt (line 4)) (20240706)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\users\\lenovo\\anaconda3\\envs\\resume_parser_project\\lib\\site-packages (from spacy==3.7.5->-r requirements.txt (line 1)) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\lenovo\\anaconda3\\envs\\resume_parser_project\\lib\\site-packages (from spacy==3.7.5->-r requirements.txt (line 1)) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\lenovo\\anaconda3\\envs\\resume_parser_project\\lib\\site-packages (from spacy==3.7.5->-r requirements.txt (line 1)) (1.0.10)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\lenovo\\anaconda3\\envs\\resume_parser_project\\lib\\site-packages (from spacy==3.7.5->-r requirements.txt (line 1)) (2.0.8)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\lenovo\\anaconda3\\envs\\resume_parser_project\\lib\\site-packages (from spacy==3.7.5->-r requirements.txt (line 1)) (3.0.9)\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in c:\\users\\lenovo\\anaconda3\\envs\\resume_parser_project\\lib\\site-packages (from spacy==3.7.5->-r requirements.txt (line 1)) (8.2.5)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\users\\lenovo\\anaconda3\\envs\\resume_parser_project\\lib\\site-packages (from spacy==3.7.5->-r requirements.txt (line 1)) (1.1.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\lenovo\\anaconda3\\envs\\resume_parser_project\\lib\\site-packages (from spacy==3.7.5->-r requirements.txt (line 1)) (2.4.8)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\lenovo\\anaconda3\\envs\\resume_parser_project\\lib\\site-packages (from spacy==3.7.5->-r requirements.txt (line 1)) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in c:\\users\\lenovo\\anaconda3\\envs\\resume_parser_project\\lib\\site-packages (from spacy==3.7.5->-r requirements.txt (line 1)) (0.4.1)\n",
      "Requirement already satisfied: typer<1.0.0,>=0.3.0 in c:\\users\\lenovo\\anaconda3\\envs\\resume_parser_project\\lib\\site-packages (from spacy==3.7.5->-r requirements.txt (line 1)) (0.13.1)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\lenovo\\anaconda3\\envs\\resume_parser_project\\lib\\site-packages (from spacy==3.7.5->-r requirements.txt (line 1)) (4.67.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\lenovo\\anaconda3\\envs\\resume_parser_project\\lib\\site-packages (from spacy==3.7.5->-r requirements.txt (line 1)) (2.32.3)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in c:\\users\\lenovo\\anaconda3\\envs\\resume_parser_project\\lib\\site-packages (from spacy==3.7.5->-r requirements.txt (line 1)) (2.9.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\lenovo\\anaconda3\\envs\\resume_parser_project\\lib\\site-packages (from spacy==3.7.5->-r requirements.txt (line 1)) (3.1.4)\n",
      "Requirement already satisfied: setuptools in c:\\users\\lenovo\\anaconda3\\envs\\resume_parser_project\\lib\\site-packages (from spacy==3.7.5->-r requirements.txt (line 1)) (75.1.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\lenovo\\anaconda3\\envs\\resume_parser_project\\lib\\site-packages (from spacy==3.7.5->-r requirements.txt (line 1)) (24.1)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\lenovo\\anaconda3\\envs\\resume_parser_project\\lib\\site-packages (from spacy==3.7.5->-r requirements.txt (line 1)) (3.4.1)\n",
      "Requirement already satisfied: numpy>=1.19.0 in c:\\users\\lenovo\\anaconda3\\envs\\resume_parser_project\\lib\\site-packages (from spacy==3.7.5->-r requirements.txt (line 1)) (1.26.4)\n",
      "Requirement already satisfied: click in c:\\users\\lenovo\\anaconda3\\envs\\resume_parser_project\\lib\\site-packages (from nltk==3.9.1->-r requirements.txt (line 2)) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\lenovo\\anaconda3\\envs\\resume_parser_project\\lib\\site-packages (from nltk==3.9.1->-r requirements.txt (line 2)) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\lenovo\\anaconda3\\envs\\resume_parser_project\\lib\\site-packages (from nltk==3.9.1->-r requirements.txt (line 2)) (2024.11.6)\n",
      "Requirement already satisfied: charset-normalizer>=2.0.0 in c:\\users\\lenovo\\anaconda3\\envs\\resume_parser_project\\lib\\site-packages (from pdfminer.six==20240706->-r requirements.txt (line 4)) (3.3.2)\n",
      "Requirement already satisfied: cryptography>=36.0.0 in c:\\users\\lenovo\\anaconda3\\envs\\resume_parser_project\\lib\\site-packages (from pdfminer.six==20240706->-r requirements.txt (line 4)) (43.0.3)\n",
      "Requirement already satisfied: cffi>=1.12 in c:\\users\\lenovo\\anaconda3\\envs\\resume_parser_project\\lib\\site-packages (from cryptography>=36.0.0->pdfminer.six==20240706->-r requirements.txt (line 4)) (1.17.1)\n",
      "Requirement already satisfied: language-data>=1.2 in c:\\users\\lenovo\\anaconda3\\envs\\resume_parser_project\\lib\\site-packages (from langcodes<4.0.0,>=3.2.0->spacy==3.7.5->-r requirements.txt (line 1)) (1.2.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\lenovo\\anaconda3\\envs\\resume_parser_project\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy==3.7.5->-r requirements.txt (line 1)) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in c:\\users\\lenovo\\anaconda3\\envs\\resume_parser_project\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy==3.7.5->-r requirements.txt (line 1)) (2.23.4)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in c:\\users\\lenovo\\anaconda3\\envs\\resume_parser_project\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy==3.7.5->-r requirements.txt (line 1)) (4.11.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\lenovo\\anaconda3\\envs\\resume_parser_project\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy==3.7.5->-r requirements.txt (line 1)) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\lenovo\\anaconda3\\envs\\resume_parser_project\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy==3.7.5->-r requirements.txt (line 1)) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\lenovo\\anaconda3\\envs\\resume_parser_project\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy==3.7.5->-r requirements.txt (line 1)) (2024.8.30)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in c:\\users\\lenovo\\anaconda3\\envs\\resume_parser_project\\lib\\site-packages (from thinc<8.3.0,>=8.2.2->spacy==3.7.5->-r requirements.txt (line 1)) (0.7.11)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\lenovo\\anaconda3\\envs\\resume_parser_project\\lib\\site-packages (from thinc<8.3.0,>=8.2.2->spacy==3.7.5->-r requirements.txt (line 1)) (0.1.5)\n",
      "Requirement already satisfied: colorama in c:\\users\\lenovo\\anaconda3\\envs\\resume_parser_project\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy==3.7.5->-r requirements.txt (line 1)) (0.4.6)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in c:\\users\\lenovo\\anaconda3\\envs\\resume_parser_project\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy==3.7.5->-r requirements.txt (line 1)) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in c:\\users\\lenovo\\anaconda3\\envs\\resume_parser_project\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy==3.7.5->-r requirements.txt (line 1)) (13.9.4)\n",
      "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in c:\\users\\lenovo\\anaconda3\\envs\\resume_parser_project\\lib\\site-packages (from weasel<0.5.0,>=0.1.0->spacy==3.7.5->-r requirements.txt (line 1)) (0.20.0)\n",
      "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in c:\\users\\lenovo\\anaconda3\\envs\\resume_parser_project\\lib\\site-packages (from weasel<0.5.0,>=0.1.0->spacy==3.7.5->-r requirements.txt (line 1)) (7.0.5)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\lenovo\\anaconda3\\envs\\resume_parser_project\\lib\\site-packages (from jinja2->spacy==3.7.5->-r requirements.txt (line 1)) (2.1.3)\n",
      "Requirement already satisfied: pycparser in c:\\users\\lenovo\\anaconda3\\envs\\resume_parser_project\\lib\\site-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six==20240706->-r requirements.txt (line 4)) (2.21)\n",
      "Requirement already satisfied: marisa-trie>=0.7.7 in c:\\users\\lenovo\\anaconda3\\envs\\resume_parser_project\\lib\\site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy==3.7.5->-r requirements.txt (line 1)) (1.2.1)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\lenovo\\anaconda3\\envs\\resume_parser_project\\lib\\site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy==3.7.5->-r requirements.txt (line 1)) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\lenovo\\anaconda3\\envs\\resume_parser_project\\lib\\site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy==3.7.5->-r requirements.txt (line 1)) (2.15.1)\n",
      "Requirement already satisfied: wrapt in c:\\users\\lenovo\\anaconda3\\envs\\resume_parser_project\\lib\\site-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy==3.7.5->-r requirements.txt (line 1)) (1.16.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\lenovo\\anaconda3\\envs\\resume_parser_project\\lib\\site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy==3.7.5->-r requirements.txt (line 1)) (0.1.2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     C:\\Users\\Lenovo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\Lenovo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Lenovo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Lenovo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Lenovo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Lenovo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "!pip install -r requirements.txt\n",
    "%run prerequisite.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import constants as cs\n",
    "\n",
    "from spacy.matcher import Matcher\n",
    "import spacy\n",
    "\n",
    "import re\n",
    "from collections import defaultdict\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WrfB4co9dXen"
   },
   "source": [
    "## Extract name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 530,
     "status": "ok",
     "timestamp": 1731905651556,
     "user": {
      "displayName": "Giang Le Huynh",
      "userId": "12580968046014402630"
     },
     "user_tz": -420
    },
    "id": "t9IQHf-8nuB8",
    "outputId": "e7a7ffbd-f5b0-4d1d-9f42-c3ebd32a32af"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'POS': 'PROPN'}]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pattern = cs.NAME_PATTERN\n",
    "pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 340,
     "status": "ok",
     "timestamp": 1731905652231,
     "user": {
      "displayName": "Giang Le Huynh",
      "userId": "12580968046014402630"
     },
     "user_tz": -420
    },
    "id": "QwU_VDgH4P_t",
    "outputId": "b6224fc2-4990-4677-c5a2-ad9b50e20138"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['John Doe', 'Scatter William Smith']\n"
     ]
    }
   ],
   "source": [
    "def extract_name(doc, matcher):\n",
    "    '''\n",
    "    Helper function to extract name from spacy nlp text\n",
    "\n",
    "    :param nlp_text: object of `spacy.tokens.doc.Doc`\n",
    "    :param matcher: object of `spacy.matcher.Matcher`\n",
    "    :return: string of full name\n",
    "    '''\n",
    "    # Add a pattern to match proper nouns (PROPN)\n",
    "    pattern = cs.NAME_PATTERN\n",
    "    matcher.add('NAME', [pattern])\n",
    "\n",
    "    # Apply the matcher to the doc\n",
    "    matches = matcher(doc)\n",
    "\n",
    "    # List to store full names\n",
    "    full_names = []\n",
    "\n",
    "    # Temporary variable to track the end position of the last matched name\n",
    "    last_end_index = -1\n",
    "\n",
    "    # Iterate over the matches\n",
    "    for match_id, start, end in matches:\n",
    "        # If the current match is contiguous with the last match, append the name\n",
    "        if start == last_end_index:\n",
    "            full_names[-1] += \" \" + doc[start:end].text  # Merge the names\n",
    "        else:\n",
    "            full_names.append(doc[start:end].text)  # Add a new name\n",
    "\n",
    "        # Update the last_end_index to the end of the current match\n",
    "        last_end_index = end\n",
    "\n",
    "    return full_names\n",
    "\n",
    "\n",
    "text = \"John Doe is a software developer. And Scatter William Smith is an computer engineering.\"\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "matchers = Matcher(nlp.vocab)\n",
    "full_names = extract_name(nlp(text), matcher=matchers)\n",
    "print(full_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 347,
     "status": "ok",
     "timestamp": 1731903245924,
     "user": {
      "displayName": "Giang Le Huynh",
      "userId": "12580968046014402630"
     },
     "user_tz": -420
    },
    "id": "F8OUwQUAeG-5",
    "outputId": "5dcb52aa-1172-41ec-8940-75cc12b2d069"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(424143666773229379, 0, 1),\n",
       " (424143666773229379, 1, 2),\n",
       " (424143666773229379, 8, 9),\n",
       " (424143666773229379, 10, 11)]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"John Doe is a software developer. And Scatter Will Smith is an computer engineering.\"\n",
    "matchers = Matcher(nlp.vocab)\n",
    "matchers.add('NAME', [[{'POS': 'PROPN'}]])\n",
    "matchers(nlp(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q2-Q2hoye3oJ"
   },
   "source": [
    "## Extract link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 342,
     "status": "ok",
     "timestamp": 1731906989351,
     "user": {
      "displayName": "Giang Le Huynh",
      "userId": "12580968046014402630"
     },
     "user_tz": -420
    },
    "id": "wp0up6rUm2Lw",
    "outputId": "edaffdd4-3895-4c0d-dc46-892b18e693e8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://example.com\n",
      "https://mywebsite.org/about\n",
      "https://subdomain.example.net/products?category=books\n",
      "https://another-site.co.uk/contact?ref=home\n",
      "ftp://fileserver.com/path/to/file\n"
     ]
    }
   ],
   "source": [
    "# Example text containing domain names with paths and some with protocols\n",
    "text = \"\"\"\n",
    "Here are some domain names with and without protocols:\n",
    "http://example.com\n",
    "https://mywebsite.org/about\n",
    "subdomain.example.net/products?category=books\n",
    "another-site.co.uk/contact?ref=home\n",
    "ftp://fileserver.com/path/to/file\n",
    "\"\"\"\n",
    "\n",
    "# Regular expression to match domain names with optional paths or query parameters\n",
    "domain_with_path_pattern = r'\\b(?:[A-Za-z][A-Za-z0-9+.-]*://)?[A-Za-z0-9.-]+\\.[a-zA-Z]{2,}(?:/[A-Za-z0-9&%_\\-.?=]*)*\\b'\n",
    "\n",
    "# Extracting all domain names with or without protocols\n",
    "domains_with_paths = re.findall(domain_with_path_pattern, text)\n",
    "\n",
    "# Prepare the final list with https:// if no protocol exists\n",
    "final_links = []\n",
    "for link in domains_with_paths:\n",
    "    # If the link doesn't have a protocol (i.e., it doesn't start with 'http://', 'https://', etc.)\n",
    "    if not re.match(r'^[a-zA-Z][a-zA-Z0-9+.-]*://', link):  # No protocol\n",
    "        link = 'https://' + link  # Add https:// as the default protocol\n",
    "    final_links.append(link)\n",
    "\n",
    "# Print the final list of links\n",
    "for link in final_links:\n",
    "    print(link)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 332
    },
    "executionInfo": {
     "elapsed": 381,
     "status": "error",
     "timestamp": 1731912246481,
     "user": {
      "displayName": "Giang Le Huynh",
      "userId": "12580968046014402630"
     },
     "user_tz": -420
    },
    "id": "IJTHB4jvsKMy",
    "outputId": "6b7ec85b-6bda-4012-ba9c-ba7563e3ab7b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "example.com:\n",
      "  http://example.com\n",
      "mywebsite.org:\n",
      "  https://mywebsite.org/about\n",
      "subdomain.example.net:\n",
      "  https://subdomain.example.net/products?category=books\n",
      "another-site.co.uk:\n",
      "  https://another-site.co.uk/contact?ref=home\n",
      "fileserver.com:\n",
      "  ftp://fileserver.com/path/to/file\n"
     ]
    }
   ],
   "source": [
    "def extract_links(text):\n",
    "    # Regular expression to match domain names with optional protocols and paths\n",
    "    domain_with_path_pattern = cs.LINK_PATTERN\n",
    "    # Extract all the links (with or without protocol)\n",
    "    links = re.findall(domain_with_path_pattern, text)\n",
    "\n",
    "    # Dictionary to hold domain as key and list of links as value\n",
    "    domain_dict = defaultdict(list)\n",
    "\n",
    "    # Iterate through each link\n",
    "    for link in links:\n",
    "        # If the link doesn't have a protocol, add https:// by default\n",
    "        if not re.match(r'^[a-zA-Z][a-zA-Z0-9+.-]*://', link):  # No protocol\n",
    "            link = 'https://' + link\n",
    "\n",
    "        # Parse the link to get the domain name (without protocol)\n",
    "        parsed_url = urlparse(link)\n",
    "        domain_name = parsed_url.netloc\n",
    "\n",
    "        # Append the link to the list of the corresponding domain\n",
    "        domain_dict[domain_name].append(link)\n",
    "\n",
    "    return domain_dict\n",
    "\n",
    "# Example text containing domain names with paths and some with protocols\n",
    "text = \"\"\"\n",
    "Here are some domain names with and without protocols:\n",
    "http://example.com\n",
    "https://mywebsite.org/about\n",
    "subdomain.example.net/products?category=books\n",
    "another-site.co.uk/contact?ref=home\n",
    "ftp://fileserver.com/path/to/file\n",
    "\"\"\"\n",
    "\n",
    "# Call the function and get the domain dictionary\n",
    "domain_links = extract_links(text)\n",
    "\n",
    "# Print the resulting dictionary\n",
    "for domain, links in domain_links.items():\n",
    "    print(f\"{domain}:\")\n",
    "    for link in links:\n",
    "        print(f\"  {link}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GMerCNRFx6Nf"
   },
   "source": [
    "## Extract phone number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 347,
     "status": "ok",
     "timestamp": 1731908333068,
     "user": {
      "displayName": "Giang Le Huynh",
      "userId": "12580968046014402630"
     },
     "user_tz": -420
    },
    "id": "KkhU2Q36r8lN",
    "outputId": "91ff7482-be19-4b37-cad8-a7f0f9a81333"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+1-800-5555555\n",
      "(800) 5555555\n",
      "555 555 5555\n",
      "+44 7911 123456\n",
      "123.456.7890\n",
      "800-123-4567\n"
     ]
    }
   ],
   "source": [
    "def extract_phone_numbers(text):\n",
    "    # Regular expression to match phone numbers (supports various formats)\n",
    "    phone_pattern = r'(\\+?\\d{1,3}[-.\\s]?)?(\\(?\\d{1,4}\\)?[-.\\s]?)?(\\d{1,4})[-.\\s]?(\\d{1,4})[-.\\s]?(\\d{1,4})'\n",
    "\n",
    "    # Find all matches in the text\n",
    "    phone_numbers = re.findall(phone_pattern, text)\n",
    "\n",
    "    # Clean up the result to return full phone numbers (combining the matched parts)\n",
    "    full_phone_numbers = [''.join(number) for number in phone_numbers]\n",
    "\n",
    "    return full_phone_numbers\n",
    "\n",
    "# Example text with phone numbers\n",
    "text = \"\"\"\n",
    "You can reach me at +1-800-555-5555 or (800) 555-5555.\n",
    "My office number is 555 555 5555. Call me at +44 7911 123456.\n",
    "Another contact: 123.456.7890. Or just use 800-123-4567.\n",
    "\"\"\"\n",
    "\n",
    "# Extract phone numbers from the text\n",
    "phones = extract_phone_numbers(text)\n",
    "\n",
    "# Print the extracted phone numbers\n",
    "for phone in phones:\n",
    "    print(phone)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v0nI6Df2x-p5"
   },
   "source": [
    "## Extract education"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('BACHELOR', '2015'), ('MASTERS', '2020')]\n"
     ]
    }
   ],
   "source": [
    "# Simulated constants\n",
    "class cs:\n",
    "    EDUCATION = {\"BACHELOR\", \"MASTERS\", \"PHD\", \"MBA\", \"BSC\", \"MSC\"}\n",
    "    STOPWORDS = {\"AND\", \"OF\", \"THE\"}\n",
    "    YEAR = r'\\b(19|20)\\d{2}\\b'  # Regex pattern to match years like 1999, 2020\n",
    "\n",
    "# Function to test\n",
    "def extract_education(nlp_text):\n",
    "    '''\n",
    "    Helper function to extract education from spacy nlp text\n",
    "\n",
    "    :param nlp_text: object of `spacy.tokens.doc.Doc`\n",
    "    :return: tuple of education degree and year if year is found\n",
    "             else only returns education degree\n",
    "    '''\n",
    "    edu = {}\n",
    "    # Extract education degree\n",
    "    try:\n",
    "        for index, token in enumerate(nlp_text):\n",
    "            text_cleaned = re.sub(r'[?|$|.|!|,]', r'', token.text)\n",
    "            if text_cleaned.upper() in cs.EDUCATION and text_cleaned.upper() not in cs.STOPWORDS:\n",
    "                # Collect surrounding context (current and next 5 tokens for simplicity)\n",
    "                context = \" \".join([t.text for t in nlp_text[index:index + 6]])\n",
    "                edu[text_cleaned.upper()] = context\n",
    "    except IndexError:\n",
    "        pass\n",
    "\n",
    "    # Extract year\n",
    "    education = []\n",
    "    for key in edu.keys():\n",
    "        year = re.search(re.compile(cs.YEAR), edu[key])\n",
    "        if year:\n",
    "            education.append((key, year.group(0)))  # Include degree and year\n",
    "        else:\n",
    "            education.append(key)  # Include degree only\n",
    "    return education\n",
    "\n",
    "# Test with spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "test_text = \"John completed his Bachelor of Science in 2015 and his Masters in 2020.\"\n",
    "nlp_text = nlp(test_text)\n",
    "\n",
    "# Execute the function\n",
    "result = extract_education(nlp_text)\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract skill"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\envs\\resume_parser_project\\Lib\\site-packages\\spacy\\util.py:910: UserWarning: [W095] Model 'en_core_web_sm' (3.6.0) was trained with spaCy v3.6.0 and may not be 100% compatible with the current version (3.7.5). If you see errors or degraded performance, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted Skills: ['Machine learning', 'Python', 'Python', 'Data analysis']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Mock skills data\n",
    "skills_data = {\"Skill\": [\"python\", \"data analysis\", \"machine learning\", \"nlp\", \"deep learning\"]}\n",
    "skills_file_path = \"skills.csv\"\n",
    "\n",
    "# Save the mock skills data to a CSV file\n",
    "pd.DataFrame(skills_data).to_csv(skills_file_path, index=False)\n",
    "\n",
    "# Function to extract skills\n",
    "def extract_skills(nlp_text, noun_chunks, skills_file=None):\n",
    "    '''\n",
    "    Helper function to extract skills from spacy nlp text\n",
    "\n",
    "    :param nlp_text: object of `spacy.tokens.doc.Doc`\n",
    "    :param noun_chunks: noun chunks extracted from nlp text\n",
    "    :param skills_file: path to skills file (optional)\n",
    "    :return: list of skills extracted\n",
    "    '''\n",
    "    tokens = [token.text for token in nlp_text if not token.is_stop]\n",
    "    if not skills_file:\n",
    "        raise ValueError(\"Skills file is required.\")\n",
    "    else:\n",
    "        data = pd.read_csv(skills_file)\n",
    "    skills = list(data['Skill'].str.lower())  # Convert skills to lowercase for matching\n",
    "    skillset = []\n",
    "    # Check for one-grams\n",
    "    for token in tokens:\n",
    "        if token.lower() in skills:\n",
    "            skillset.append(token)\n",
    "    # Check for bi-grams and tri-grams\n",
    "    for chunk in noun_chunks:\n",
    "        chunk_text = chunk.text.lower().strip()\n",
    "        if chunk_text in skills:\n",
    "            skillset.append(chunk_text)\n",
    "    return [i.capitalize() for i in set(skillset)]\n",
    "\n",
    "# Load SpaCy model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Test text\n",
    "test_text = \"I have experience in Python, data analysis, and machine learning.\"\n",
    "nlp_text = nlp(test_text)\n",
    "noun_chunks = list(nlp_text.noun_chunks)\n",
    "\n",
    "# Test the function\n",
    "result = extract_skills(nlp_text, noun_chunks, skills_file_path)\n",
    "print(\"Extracted Skills:\", result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use model pre-train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\n"
     ]
    }
   ],
   "source": [
    "cd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "[E002] Can't find factory for 'transformer' for language English (en). This usually happens when spaCy calls `nlp.create_pipe` with a custom component name that's not registered on the current language class. If you're using a custom component, make sure you've added the decorator `@Language.component` (for function components) or `@Language.factory` (for class components).\n\nAvailable factories: attribute_ruler, tok2vec, merge_noun_chunks, merge_entities, merge_subtokens, token_splitter, doc_cleaner, parser, beam_parser, lemmatizer, trainable_lemmatizer, entity_linker, entity_ruler, tagger, morphologizer, ner, beam_ner, senter, sentencizer, spancat, spancat_singlelabel, span_finder, future_entity_ruler, span_ruler, textcat, textcat_multilabel, en.lemmatizer",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[47], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mspacy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlearning/Resume Analyser/4.version/trainning/_model/output/model-best\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\resume_parser_project\\Lib\\site-packages\\spacy\\__init__.py:51\u001b[0m, in \u001b[0;36mload\u001b[1;34m(name, vocab, disable, enable, exclude, config)\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload\u001b[39m(\n\u001b[0;32m     28\u001b[0m     name: Union[\u001b[38;5;28mstr\u001b[39m, Path],\n\u001b[0;32m     29\u001b[0m     \u001b[38;5;241m*\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     34\u001b[0m     config: Union[Dict[\u001b[38;5;28mstr\u001b[39m, Any], Config] \u001b[38;5;241m=\u001b[39m util\u001b[38;5;241m.\u001b[39mSimpleFrozenDict(),\n\u001b[0;32m     35\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Language:\n\u001b[0;32m     36\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Load a spaCy model from an installed package or a local path.\u001b[39;00m\n\u001b[0;32m     37\u001b[0m \n\u001b[0;32m     38\u001b[0m \u001b[38;5;124;03m    name (str): Package name or model path.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[38;5;124;03m    RETURNS (Language): The loaded nlp object.\u001b[39;00m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 51\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mutil\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     52\u001b[0m \u001b[43m        \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     53\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvocab\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvocab\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     54\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdisable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdisable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     55\u001b[0m \u001b[43m        \u001b[49m\u001b[43menable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     56\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexclude\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexclude\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     57\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     58\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\resume_parser_project\\Lib\\site-packages\\spacy\\util.py:467\u001b[0m, in \u001b[0;36mload_model\u001b[1;34m(name, vocab, disable, enable, exclude, config)\u001b[0m\n\u001b[0;32m    465\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m load_model_from_package(name, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[0;32m    466\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m Path(name)\u001b[38;5;241m.\u001b[39mexists():  \u001b[38;5;66;03m# path to model data directory\u001b[39;00m\n\u001b[1;32m--> 467\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mload_model_from_path\u001b[49m\u001b[43m(\u001b[49m\u001b[43mPath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[0;32m    468\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(name, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexists\u001b[39m\u001b[38;5;124m\"\u001b[39m):  \u001b[38;5;66;03m# Path or Path-like to model data\u001b[39;00m\n\u001b[0;32m    469\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m load_model_from_path(name, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\resume_parser_project\\Lib\\site-packages\\spacy\\util.py:539\u001b[0m, in \u001b[0;36mload_model_from_path\u001b[1;34m(model_path, meta, vocab, disable, enable, exclude, config)\u001b[0m\n\u001b[0;32m    537\u001b[0m overrides \u001b[38;5;241m=\u001b[39m dict_to_dot(config, for_overrides\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    538\u001b[0m config \u001b[38;5;241m=\u001b[39m load_config(config_path, overrides\u001b[38;5;241m=\u001b[39moverrides)\n\u001b[1;32m--> 539\u001b[0m nlp \u001b[38;5;241m=\u001b[39m \u001b[43mload_model_from_config\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    540\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    541\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvocab\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvocab\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    542\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdisable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdisable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    543\u001b[0m \u001b[43m    \u001b[49m\u001b[43menable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    544\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexclude\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexclude\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    545\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmeta\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmeta\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    546\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    547\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m nlp\u001b[38;5;241m.\u001b[39mfrom_disk(model_path, exclude\u001b[38;5;241m=\u001b[39mexclude, overrides\u001b[38;5;241m=\u001b[39moverrides)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\resume_parser_project\\Lib\\site-packages\\spacy\\util.py:587\u001b[0m, in \u001b[0;36mload_model_from_config\u001b[1;34m(config, meta, vocab, disable, enable, exclude, auto_fill, validate)\u001b[0m\n\u001b[0;32m    584\u001b[0m \u001b[38;5;66;03m# This will automatically handle all codes registered via the languages\u001b[39;00m\n\u001b[0;32m    585\u001b[0m \u001b[38;5;66;03m# registry, including custom subclasses provided via entry points\u001b[39;00m\n\u001b[0;32m    586\u001b[0m lang_cls \u001b[38;5;241m=\u001b[39m get_lang_class(nlp_config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlang\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m--> 587\u001b[0m nlp \u001b[38;5;241m=\u001b[39m \u001b[43mlang_cls\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_config\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    588\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    589\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvocab\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvocab\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    590\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdisable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdisable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    591\u001b[0m \u001b[43m    \u001b[49m\u001b[43menable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    592\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexclude\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexclude\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    593\u001b[0m \u001b[43m    \u001b[49m\u001b[43mauto_fill\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mauto_fill\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    594\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    595\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmeta\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmeta\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    596\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    597\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m nlp\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\resume_parser_project\\Lib\\site-packages\\spacy\\language.py:1889\u001b[0m, in \u001b[0;36mfrom_config\u001b[1;34m(cls, config, vocab, disable, enable, exclude, meta, auto_fill, validate)\u001b[0m\n\u001b[0;32m   1885\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m warnings\u001b[38;5;241m.\u001b[39mcatch_warnings():\n\u001b[0;32m   1886\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mfilterwarnings(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m, message\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124m[W113\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1887\u001b[0m     nlp\u001b[38;5;241m.\u001b[39madd_pipe(\n\u001b[0;32m   1888\u001b[0m         source_name, source\u001b[38;5;241m=\u001b[39msource_nlps[model], name\u001b[38;5;241m=\u001b[39mpipe_name\n\u001b[1;32m-> 1889\u001b[0m     )\n\u001b[0;32m   1890\u001b[0m     \u001b[38;5;66;03m# At this point after nlp.add_pipe, the listener map\u001b[39;00m\n\u001b[0;32m   1891\u001b[0m     \u001b[38;5;66;03m# corresponds to the new pipeline.\u001b[39;00m\n\u001b[0;32m   1892\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m model \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m source_nlp_vectors_hashes:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\resume_parser_project\\Lib\\site-packages\\spacy\\language.py:821\u001b[0m, in \u001b[0;36madd_pipe\u001b[1;34m(self, factory_name, name, before, after, first, last, source, config, raw_config, validate)\u001b[0m\n\u001b[0;32m    813\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    814\u001b[0m     pipe_component \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreate_pipe(\n\u001b[0;32m    815\u001b[0m         factory_name,\n\u001b[0;32m    816\u001b[0m         name\u001b[38;5;241m=\u001b[39mname,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    819\u001b[0m         validate\u001b[38;5;241m=\u001b[39mvalidate,\n\u001b[0;32m    820\u001b[0m     )\n\u001b[1;32m--> 821\u001b[0m pipe_index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_pipe_index(before, after, first, last)\n\u001b[0;32m    822\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pipe_meta[name] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_factory_meta(factory_name)\n\u001b[0;32m    823\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_components\u001b[38;5;241m.\u001b[39minsert(pipe_index, (name, pipe_component))\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\resume_parser_project\\Lib\\site-packages\\spacy\\language.py:690\u001b[0m, in \u001b[0;36mcreate_pipe\u001b[1;34m(self, factory_name, name, config, raw_config, validate)\u001b[0m\n\u001b[0;32m    688\u001b[0m     config = Config(pipe_meta.default_config).merge(config)\n\u001b[0;32m    689\u001b[0m internal_name = self.get_factory_name(factory_name)\n\u001b[1;32m--> 690\u001b[0m # If the language-specific factory doesn't exist, try again with the\n\u001b[0;32m    691\u001b[0m # not-specific name\n\u001b[0;32m    692\u001b[0m if internal_name not in registry.factories:\n\u001b[0;32m    693\u001b[0m     internal_name = factory_name\n",
      "\u001b[1;31mValueError\u001b[0m: [E002] Can't find factory for 'transformer' for language English (en). This usually happens when spaCy calls `nlp.create_pipe` with a custom component name that's not registered on the current language class. If you're using a custom component, make sure you've added the decorator `@Language.component` (for function components) or `@Language.factory` (for class components).\n\nAvailable factories: attribute_ruler, tok2vec, merge_noun_chunks, merge_entities, merge_subtokens, token_splitter, doc_cleaner, parser, beam_parser, lemmatizer, trainable_lemmatizer, entity_linker, entity_ruler, tagger, morphologizer, ner, beam_ner, senter, sentencizer, spancat, spancat_singlelabel, span_finder, future_entity_ruler, span_ruler, textcat, textcat_multilabel, en.lemmatizer"
     ]
    }
   ],
   "source": [
    "model = spacy.load('learning/Resume Analyser/4.version/trainning/_model/output/model-best')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test example\n",
    "main_str = ['''Manufacturing Production Manager Resume\n",
    "Desired Industry Manufacturing\n",
    "SpiderID 78692\n",
    "Desired Job Location Windsor Colorado\n",
    "Date Posted 1 23 2017\n",
    "Type of Position Full Time Permanent\n",
    "Availability Date Immediately\n",
    "Desired Wage 95000\n",
    "U S Work Authorization Yes\n",
    "Job Level Management Manager Director\n",
    "Willing to Travel\n",
    "Highest Degree Attained Other\n",
    "Willing to Relocate Yes\n",
    "Objective Dynamic and growth driven professional offering hands on management experience and comprehensive background in manufacturing and engineering operations within highly competitive setting Adept at reengineering unproductive work processes as well as in planning and implementing various sustainable and cost effective work programs to drive continuous improvement of operations Armed with exceptional organizational and critical problem solving aptitudes to formulate effective solutions on complex production and quality issues Equipped with tactical leadership capabilities in supervising and guiding teams toward the successful and timely completion of projects Proficient with Microsoft Office applications Project Management and Microsoft Visio\n",
    "Experience Relevant ExperienceVestas Blades America Windsor COProduction Engineer Jan 2014 PresentContribute efforts in achieving production plan budget tooling equipment and bill of material along with quality control and safety Conceptualized new methods to optimize production levels while maintaining production costs yields quality and safety Identified and resolved process problems with effective solutions which decreased downtime and minimized costs Initiated plant trials to measure performance capabilities while ensuring updated documentation of process procedures Leveraged industry expertise in streamlining the manufacturing of turbine blades Production Supervisor Mar 2009 Jan 2014Rendered oversight to more than 70 employees to guarantee accordance of operation with production plan and goals Observed strict compliance with safety and quality guidelines and handled inventory control created schedules delegated work and facilitated training of staff Generated production reports for production and operations managers regarding production areas performance Served as a Shells Lighthouse project member while drafting and modifying all standard operating procedures Functioned as department lead for production quality training and process improvements Closely monitored operations productivity to determine areas for improvement in overall production process Pioneered the development of all shells production process job cards that decreased process times and improved efficiency and quality through changed production flow layout Anheuser Busch Fort Collis COBrew House Control Panel Operator Mar 2007 Mar 2009Efficiently administered beer brewing process from raw material selection and recipe formulation through the use of Siemens software Strictly enforced standard operating procedures and safe working practices Determined and evaluated all critical control points to achieve consistent product quality within allotted schedules Other ExperienceUnited States Airways Denver CoCustomer Service Representative Kroger Co King Soopers Smiths Food Drug Fort Collins CoGrocery Manager Head Clerk Night Crew Manager Front End ManagerInventory Control Manager Warehouse Manager\n",
    "Education EducationAssociate of Science with emphasis in chemistry and biologyFront Range Community College Fort Collins COPharmacy Pre Requisites for Doctor of Pharmacy Pharm D University of Wyoming Laramie WYPharmacy Pre Requisites for Doctor of Pharmacy Pharm D Western Wyoming Community College Rock Springs WY Deans Honor RollProfessional DevelopmentCertificationsSupervisor Certification Six Sigma Yellow Belt Project ManagementQuality Management Crucial Conversations Fort Lift License Crane License\n",
    "Affiliations\n",
    "Skills Manufacturing Production Manager Project Planning Cost Reduction and Budget Optimization Resource Allocation Six Sigma Quality ControlLean Manufacturing Plant Management Manufacturing Inspection Administration Cross functional Team Building\n",
    "Additional Information TrainingProduction Instructor Coordinator Planner Coordinator Wrote Training Document\n",
    "Reference Available upon request\n",
    "Candidate Contact Information\n",
    "JobSpider com has chosen not to make contact information available on this page Click Contact Candidate to send this candidate a response\n",
    "Manufacturing Production Manager Resume\n",
    "Desired Industry Manufacturing\n",
    "SpiderID 78692\n",
    "Desired Job Location Windsor Colorado\n",
    "Date Posted 1 23 2017\n",
    "Type of Position Full Time Permanent\n",
    "Availability Date Immediately\n",
    "Desired Wage 95000\n",
    "U S Work Authorization Yes\n",
    "Job Level Management Manager Director\n",
    "Willing to Travel\n",
    "Highest Degree Attained Other\n",
    "Willing to Relocate Yes\n",
    "Objective Dynamic and growth driven professional offering hands on management experience and comprehensive background in manufacturing and engineering operations within highly competitive setting Adept at reengineering unproductive work processes as well as in planning and implementing various sustainable and cost effective work programs to drive continuous improvement of operations Armed with exceptional organizational and critical problem solving aptitudes to formulate effective solutions on complex production and quality issues Equipped with tactical leadership capabilities in supervising and guiding teams toward the successful and timely completion of projects Proficient with Microsoft Office applications Project Management and Microsoft Visio\n",
    "Experience Relevant ExperienceVestas Blades America Windsor COProduction Engineer Jan 2014 PresentContribute efforts in achieving production plan budget tooling equipment and bill of material along with quality control and safety Conceptualized new methods to optimize production levels while maintaining production costs yields quality and safety Identified and resolved process problems with effective solutions which decreased downtime and minimized costs Initiated plant trials to measure performance capabilities while ensuring updated documentation of process procedures Leveraged industry expertise in streamlining the manufacturing of turbine blades Production Supervisor Mar 2009 Jan 2014Rendered oversight to more than 70 employees to guarantee accordance of operation with production plan and goals Observed strict compliance with safety and quality guidelines and handled inventory control created schedules delegated work and facilitated training of staff Generated production reports for production and operations managers regarding production areas performance Served as a Shells Lighthouse project member while drafting and modifying all standard operating procedures Functioned as department lead for production quality training and process improvements Closely monitored operations productivity to determine areas for improvement in overall production process Pioneered the development of all shells production process job cards that decreased process times and improved efficiency and quality through changed production flow layout Anheuser Busch Fort Collis COBrew House Control Panel Operator Mar 2007 Mar 2009Efficiently administered beer brewing process from raw material selection and recipe formulation through the use of Siemens software Strictly enforced standard operating procedures and safe working practices Determined and evaluated all critical control points to achieve consistent product quality within allotted schedules Other ExperienceUnited States Airways Denver CoCustomer Service Representative Kroger Co King Soopers Smiths Food Drug Fort Collins CoGrocery Manager Head Clerk Night Crew Manager Front End ManagerInventory Control Manager Warehouse Manager\n",
    "Education EducationAssociate of Science with emphasis in chemistry and biologyFront Range Community College Fort Collins COPharmacy Pre Requisites for Doctor of Pharmacy Pharm D University of Wyoming Laramie WYPharmacy Pre Requisites for Doctor of Pharmacy Pharm D Western Wyoming Community College Rock Springs WY Deans Honor RollProfessional DevelopmentCertificationsSupervisor Certification Six Sigma Yellow Belt Project ManagementQuality Management Crucial Conversations Fort Lift License Crane License\n",
    "Affiliations\n",
    "Skills Manufacturing Production Manager Project Planning Cost Reduction and Budget Optimization Resource Allocation Six Sigma Quality ControlLean Manufacturing Plant Management Manufacturing Inspection Administration Cross functional Team Building\n",
    "Additional Information TrainingProduction Instructor Coordinator Planner Coordinator Wrote Training Document\n",
    "Reference Available upon request\n",
    "Candidate Contact Information\n",
    "JobSpider com has chosen not to make contact information available on this page Click Contact Candidate to send this candidate a response''' ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for doc in model.pipe(main_str, disable=[\"tagger\", \"parser\"]):\n",
    "  for ent in doc.ents:\n",
    "    print((ent.text,ent.label_))\n",
    "    # print([(ent.text, ent.label_) for ent in doc.ents])\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPkAU8dUR8yw87X90Q5ysWj",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
