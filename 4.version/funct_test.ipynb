{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\learning\\Resume Analyser\\4.version\n"
     ]
    }
   ],
   "source": [
    "cd learning/Resume Analyser/4.version/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Volume in drive C has no label.\n",
      " Volume Serial Number is 7010-C44B\n",
      "\n",
      " Directory of C:\\Users\\Lenovo\\learning\\Resume Analyser\\4.version\n",
      "\n",
      "11/20/2024  06:35 AM    <DIR>          .\n",
      "11/04/2024  08:36 PM    <DIR>          ..\n",
      "11/20/2024  06:34 AM    <DIR>          .ipynb_checkpoints\n",
      "11/20/2024  06:33 AM    <DIR>          __pycache__\n",
      "11/19/2024  02:12 PM             2,031 constants.py\n",
      "11/20/2024  06:35 AM            54,962 funct_test.ipynb\n",
      "11/11/2024  07:34 PM            23,719 HR_analytics.ipynb\n",
      "11/11/2024  07:34 PM         5,213,294 job_final.csv\n",
      "11/04/2024  08:38 PM    <DIR>          marker\n",
      "11/19/2024  01:46 PM               411 prerequisite.py\n",
      "11/19/2024  02:32 PM             3,673 pyresparser.py\n",
      "11/19/2024  02:46 PM                98 requirements.txt\n",
      "11/20/2024  06:34 AM                68 skills.csv\n",
      "11/14/2024  10:13 PM    <DIR>          trainning\n",
      "11/19/2024  02:29 PM            16,227 utils.py\n",
      "               9 File(s)      5,314,483 bytes\n",
      "               6 Dir(s)  202,382,499,840 bytes free\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\Lenovo\\\\learning\\\\Resume Analyser\\\\4.version'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 33968,
     "status": "ok",
     "timestamp": 1731902950171,
     "user": {
      "displayName": "Giang Le Huynh",
      "userId": "12580968046014402630"
     },
     "user_tz": -420
    },
    "id": "iR8NbgiB9yuA",
    "outputId": "976c1300-b9d9-4618-ee8b-6f3f482e96be"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting spacy==3.7.5 (from -r requirements.txt (line 1))\n",
      "  Using cached spacy-3.7.5-cp311-cp311-win_amd64.whl.metadata (27 kB)\n",
      "Collecting transformers==4.46.2 (from -r requirements.txt (line 2))\n",
      "  Downloading transformers-4.46.2-py3-none-any.whl.metadata (44 kB)\n",
      "Requirement already satisfied: nltk==3.9.1 in c:\\users\\lenovo\\anaconda3\\envs\\resume_parser_project\\lib\\site-packages (from -r requirements.txt (line 3)) (3.9.1)\n",
      "Requirement already satisfied: docx2txt in c:\\users\\lenovo\\anaconda3\\envs\\resume_parser_project\\lib\\site-packages (from -r requirements.txt (line 4)) (0.8)\n",
      "Requirement already satisfied: pdfminer.six==20240706 in c:\\users\\lenovo\\anaconda3\\envs\\resume_parser_project\\lib\\site-packages (from -r requirements.txt (line 5)) (20240706)\n",
      "Collecting pandas==2.2.2 (from -r requirements.txt (line 6))\n",
      "  Downloading pandas-2.2.2-cp311-cp311-win_amd64.whl.metadata (19 kB)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\users\\lenovo\\anaconda3\\envs\\resume_parser_project\\lib\\site-packages (from spacy==3.7.5->-r requirements.txt (line 1)) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\lenovo\\anaconda3\\envs\\resume_parser_project\\lib\\site-packages (from spacy==3.7.5->-r requirements.txt (line 1)) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\lenovo\\anaconda3\\envs\\resume_parser_project\\lib\\site-packages (from spacy==3.7.5->-r requirements.txt (line 1)) (1.0.10)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\lenovo\\anaconda3\\envs\\resume_parser_project\\lib\\site-packages (from spacy==3.7.5->-r requirements.txt (line 1)) (2.0.8)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\lenovo\\anaconda3\\envs\\resume_parser_project\\lib\\site-packages (from spacy==3.7.5->-r requirements.txt (line 1)) (3.0.9)\n",
      "Collecting thinc<8.3.0,>=8.2.2 (from spacy==3.7.5->-r requirements.txt (line 1))\n",
      "  Using cached thinc-8.2.5-cp311-cp311-win_amd64.whl.metadata (15 kB)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\users\\lenovo\\anaconda3\\envs\\resume_parser_project\\lib\\site-packages (from spacy==3.7.5->-r requirements.txt (line 1)) (1.1.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\lenovo\\anaconda3\\envs\\resume_parser_project\\lib\\site-packages (from spacy==3.7.5->-r requirements.txt (line 1)) (2.4.8)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\lenovo\\anaconda3\\envs\\resume_parser_project\\lib\\site-packages (from spacy==3.7.5->-r requirements.txt (line 1)) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in c:\\users\\lenovo\\anaconda3\\envs\\resume_parser_project\\lib\\site-packages (from spacy==3.7.5->-r requirements.txt (line 1)) (0.4.1)\n",
      "Requirement already satisfied: typer<1.0.0,>=0.3.0 in c:\\users\\lenovo\\anaconda3\\envs\\resume_parser_project\\lib\\site-packages (from spacy==3.7.5->-r requirements.txt (line 1)) (0.9.4)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\lenovo\\anaconda3\\envs\\resume_parser_project\\lib\\site-packages (from spacy==3.7.5->-r requirements.txt (line 1)) (4.67.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\lenovo\\anaconda3\\envs\\resume_parser_project\\lib\\site-packages (from spacy==3.7.5->-r requirements.txt (line 1)) (2.32.3)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in c:\\users\\lenovo\\anaconda3\\envs\\resume_parser_project\\lib\\site-packages (from spacy==3.7.5->-r requirements.txt (line 1)) (2.9.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\lenovo\\anaconda3\\envs\\resume_parser_project\\lib\\site-packages (from spacy==3.7.5->-r requirements.txt (line 1)) (3.1.4)\n",
      "Requirement already satisfied: setuptools in c:\\users\\lenovo\\anaconda3\\envs\\resume_parser_project\\lib\\site-packages (from spacy==3.7.5->-r requirements.txt (line 1)) (75.1.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\lenovo\\anaconda3\\envs\\resume_parser_project\\lib\\site-packages (from spacy==3.7.5->-r requirements.txt (line 1)) (24.1)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\lenovo\\anaconda3\\envs\\resume_parser_project\\lib\\site-packages (from spacy==3.7.5->-r requirements.txt (line 1)) (3.4.1)\n",
      "Requirement already satisfied: numpy>=1.19.0 in c:\\users\\lenovo\\anaconda3\\envs\\resume_parser_project\\lib\\site-packages (from spacy==3.7.5->-r requirements.txt (line 1)) (1.26.4)\n",
      "Collecting filelock (from transformers==4.46.2->-r requirements.txt (line 2))\n",
      "  Downloading filelock-3.16.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting huggingface-hub<1.0,>=0.23.2 (from transformers==4.46.2->-r requirements.txt (line 2))\n",
      "  Downloading huggingface_hub-0.26.2-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\lenovo\\anaconda3\\envs\\resume_parser_project\\lib\\site-packages (from transformers==4.46.2->-r requirements.txt (line 2)) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\lenovo\\anaconda3\\envs\\resume_parser_project\\lib\\site-packages (from transformers==4.46.2->-r requirements.txt (line 2)) (2024.11.6)\n",
      "Collecting safetensors>=0.4.1 (from transformers==4.46.2->-r requirements.txt (line 2))\n",
      "  Downloading safetensors-0.4.5-cp311-none-win_amd64.whl.metadata (3.9 kB)\n",
      "Collecting tokenizers<0.21,>=0.20 (from transformers==4.46.2->-r requirements.txt (line 2))\n",
      "  Downloading tokenizers-0.20.3-cp311-none-win_amd64.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: click in c:\\users\\lenovo\\anaconda3\\envs\\resume_parser_project\\lib\\site-packages (from nltk==3.9.1->-r requirements.txt (line 3)) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\lenovo\\anaconda3\\envs\\resume_parser_project\\lib\\site-packages (from nltk==3.9.1->-r requirements.txt (line 3)) (1.4.2)\n",
      "Requirement already satisfied: charset-normalizer>=2.0.0 in c:\\users\\lenovo\\anaconda3\\envs\\resume_parser_project\\lib\\site-packages (from pdfminer.six==20240706->-r requirements.txt (line 5)) (3.3.2)\n",
      "Requirement already satisfied: cryptography>=36.0.0 in c:\\users\\lenovo\\anaconda3\\envs\\resume_parser_project\\lib\\site-packages (from pdfminer.six==20240706->-r requirements.txt (line 5)) (43.0.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\lenovo\\anaconda3\\envs\\resume_parser_project\\lib\\site-packages (from pandas==2.2.2->-r requirements.txt (line 6)) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\lenovo\\anaconda3\\envs\\resume_parser_project\\lib\\site-packages (from pandas==2.2.2->-r requirements.txt (line 6)) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\lenovo\\anaconda3\\envs\\resume_parser_project\\lib\\site-packages (from pandas==2.2.2->-r requirements.txt (line 6)) (2024.2)\n",
      "Requirement already satisfied: cffi>=1.12 in c:\\users\\lenovo\\anaconda3\\envs\\resume_parser_project\\lib\\site-packages (from cryptography>=36.0.0->pdfminer.six==20240706->-r requirements.txt (line 5)) (1.17.1)\n",
      "Collecting fsspec>=2023.5.0 (from huggingface-hub<1.0,>=0.23.2->transformers==4.46.2->-r requirements.txt (line 2))\n",
      "  Downloading fsspec-2024.10.0-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\lenovo\\anaconda3\\envs\\resume_parser_project\\lib\\site-packages (from huggingface-hub<1.0,>=0.23.2->transformers==4.46.2->-r requirements.txt (line 2)) (4.11.0)\n",
      "Requirement already satisfied: language-data>=1.2 in c:\\users\\lenovo\\anaconda3\\envs\\resume_parser_project\\lib\\site-packages (from langcodes<4.0.0,>=3.2.0->spacy==3.7.5->-r requirements.txt (line 1)) (1.2.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\lenovo\\anaconda3\\envs\\resume_parser_project\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy==3.7.5->-r requirements.txt (line 1)) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in c:\\users\\lenovo\\anaconda3\\envs\\resume_parser_project\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy==3.7.5->-r requirements.txt (line 1)) (2.23.4)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\lenovo\\anaconda3\\envs\\resume_parser_project\\lib\\site-packages (from python-dateutil>=2.8.2->pandas==2.2.2->-r requirements.txt (line 6)) (1.16.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\lenovo\\anaconda3\\envs\\resume_parser_project\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy==3.7.5->-r requirements.txt (line 1)) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\lenovo\\anaconda3\\envs\\resume_parser_project\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy==3.7.5->-r requirements.txt (line 1)) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\lenovo\\anaconda3\\envs\\resume_parser_project\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy==3.7.5->-r requirements.txt (line 1)) (2024.8.30)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in c:\\users\\lenovo\\anaconda3\\envs\\resume_parser_project\\lib\\site-packages (from thinc<8.3.0,>=8.2.2->spacy==3.7.5->-r requirements.txt (line 1)) (0.7.11)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\lenovo\\anaconda3\\envs\\resume_parser_project\\lib\\site-packages (from thinc<8.3.0,>=8.2.2->spacy==3.7.5->-r requirements.txt (line 1)) (0.1.5)\n",
      "Requirement already satisfied: colorama in c:\\users\\lenovo\\anaconda3\\envs\\resume_parser_project\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy==3.7.5->-r requirements.txt (line 1)) (0.4.6)\n",
      "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in c:\\users\\lenovo\\anaconda3\\envs\\resume_parser_project\\lib\\site-packages (from weasel<0.5.0,>=0.1.0->spacy==3.7.5->-r requirements.txt (line 1)) (0.20.0)\n",
      "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in c:\\users\\lenovo\\anaconda3\\envs\\resume_parser_project\\lib\\site-packages (from weasel<0.5.0,>=0.1.0->spacy==3.7.5->-r requirements.txt (line 1)) (6.4.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\lenovo\\anaconda3\\envs\\resume_parser_project\\lib\\site-packages (from jinja2->spacy==3.7.5->-r requirements.txt (line 1)) (2.1.3)\n",
      "Requirement already satisfied: pycparser in c:\\users\\lenovo\\anaconda3\\envs\\resume_parser_project\\lib\\site-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six==20240706->-r requirements.txt (line 5)) (2.21)\n",
      "Requirement already satisfied: marisa-trie>=0.7.7 in c:\\users\\lenovo\\anaconda3\\envs\\resume_parser_project\\lib\\site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy==3.7.5->-r requirements.txt (line 1)) (1.2.1)\n",
      "Using cached spacy-3.7.5-cp311-cp311-win_amd64.whl (12.1 MB)\n",
      "Downloading transformers-4.46.2-py3-none-any.whl (10.0 MB)\n",
      "   ---------------------------------------- 0.0/10.0 MB ? eta -:--:--\n",
      "   -------- ------------------------------- 2.1/10.0 MB 16.7 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 9.7/10.0 MB 30.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 10.0/10.0 MB 26.1 MB/s eta 0:00:00\n",
      "Downloading pandas-2.2.2-cp311-cp311-win_amd64.whl (11.6 MB)\n",
      "   ---------------------------------------- 0.0/11.6 MB ? eta -:--:--\n",
      "   -------------------------- ------------- 7.6/11.6 MB 36.2 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 10.5/11.6 MB 27.3 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 11.0/11.6 MB 17.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 11.6/11.6 MB 16.2 MB/s eta 0:00:00\n",
      "Downloading huggingface_hub-0.26.2-py3-none-any.whl (447 kB)\n",
      "Downloading safetensors-0.4.5-cp311-none-win_amd64.whl (285 kB)\n",
      "Using cached thinc-8.2.5-cp311-cp311-win_amd64.whl (1.5 MB)\n",
      "Downloading tokenizers-0.20.3-cp311-none-win_amd64.whl (2.4 MB)\n",
      "   ---------------------------------------- 0.0/2.4 MB ? eta -:--:--\n",
      "   ---------------------------------------- 2.4/2.4 MB 45.1 MB/s eta 0:00:00\n",
      "Downloading filelock-3.16.1-py3-none-any.whl (16 kB)\n",
      "Downloading fsspec-2024.10.0-py3-none-any.whl (179 kB)\n",
      "Installing collected packages: safetensors, fsspec, filelock, pandas, huggingface-hub, tokenizers, transformers, thinc, spacy\n",
      "  Attempting uninstall: pandas\n",
      "    Found existing installation: pandas 2.2.3\n",
      "    Uninstalling pandas-2.2.3:\n",
      "      Successfully uninstalled pandas-2.2.3\n",
      "  Attempting uninstall: thinc\n",
      "    Found existing installation: thinc 8.1.12\n",
      "    Uninstalling thinc-8.1.12:\n",
      "      Successfully uninstalled thinc-8.1.12\n",
      "  Attempting uninstall: spacy\n",
      "    Found existing installation: spacy 3.6.1\n",
      "    Uninstalling spacy-3.6.1:\n",
      "      Successfully uninstalled spacy-3.6.1\n",
      "Successfully installed filelock-3.16.1 fsspec-2024.10.0 huggingface-hub-0.26.2 pandas-2.2.2 safetensors-0.4.5 spacy-3.7.5 thinc-8.2.5 tokenizers-0.20.3 transformers-4.46.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: Failed to remove contents in a temporary directory 'C:\\Users\\Lenovo\\anaconda3\\envs\\resume_parser_project\\Lib\\site-packages\\~andas.libs'.\n",
      "  You can safely remove it manually.\n",
      "  WARNING: Failed to remove contents in a temporary directory 'C:\\Users\\Lenovo\\anaconda3\\envs\\resume_parser_project\\Lib\\site-packages\\~andas'.\n",
      "  You can safely remove it manually.\n",
      "  WARNING: Failed to remove contents in a temporary directory 'C:\\Users\\Lenovo\\anaconda3\\envs\\resume_parser_project\\Lib\\site-packages\\~-inc'.\n",
      "  You can safely remove it manually.\n",
      "  WARNING: Failed to remove contents in a temporary directory 'C:\\Users\\Lenovo\\anaconda3\\envs\\resume_parser_project\\Lib\\site-packages\\~-acy'.\n",
      "  You can safely remove it manually.\n",
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "en-core-web-sm 3.6.0 requires spacy<3.7.0,>=3.6.0, but you have spacy 3.7.5 which is incompatible.\n",
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     C:\\Users\\Lenovo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\Lenovo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Lenovo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Lenovo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Lenovo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Lenovo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "!pip install -r requirements.txt\n",
    "%run prerequisite.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import constants as cs\n",
    "\n",
    "from spacy.matcher import Matcher\n",
    "import spacy\n",
    "\n",
    "import re\n",
    "from collections import defaultdict\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WrfB4co9dXen"
   },
   "source": [
    "## Extract name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 530,
     "status": "ok",
     "timestamp": 1731905651556,
     "user": {
      "displayName": "Giang Le Huynh",
      "userId": "12580968046014402630"
     },
     "user_tz": -420
    },
    "id": "t9IQHf-8nuB8",
    "outputId": "e7a7ffbd-f5b0-4d1d-9f42-c3ebd32a32af"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'POS': 'PROPN'}]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pattern = cs.NAME_PATTERN\n",
    "pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 340,
     "status": "ok",
     "timestamp": 1731905652231,
     "user": {
      "displayName": "Giang Le Huynh",
      "userId": "12580968046014402630"
     },
     "user_tz": -420
    },
    "id": "QwU_VDgH4P_t",
    "outputId": "b6224fc2-4990-4677-c5a2-ad9b50e20138"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['John Doe', 'Scatter William Smith']\n"
     ]
    }
   ],
   "source": [
    "def extract_name(doc, matcher):\n",
    "    '''\n",
    "    Helper function to extract name from spacy nlp text\n",
    "\n",
    "    :param nlp_text: object of `spacy.tokens.doc.Doc`\n",
    "    :param matcher: object of `spacy.matcher.Matcher`\n",
    "    :return: string of full name\n",
    "    '''\n",
    "    # Add a pattern to match proper nouns (PROPN)\n",
    "    pattern = cs.NAME_PATTERN\n",
    "    matcher.add('NAME', [pattern])\n",
    "\n",
    "    # Apply the matcher to the doc\n",
    "    matches = matcher(doc)\n",
    "\n",
    "    # List to store full names\n",
    "    full_names = []\n",
    "\n",
    "    # Temporary variable to track the end position of the last matched name\n",
    "    last_end_index = -1\n",
    "\n",
    "    # Iterate over the matches\n",
    "    for match_id, start, end in matches:\n",
    "        # If the current match is contiguous with the last match, append the name\n",
    "        if start == last_end_index:\n",
    "            full_names[-1] += \" \" + doc[start:end].text  # Merge the names\n",
    "        else:\n",
    "            full_names.append(doc[start:end].text)  # Add a new name\n",
    "\n",
    "        # Update the last_end_index to the end of the current match\n",
    "        last_end_index = end\n",
    "\n",
    "    return full_names\n",
    "\n",
    "\n",
    "text = \"John Doe is a software developer. And Scatter William Smith is an computer engineering.\"\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "matchers = Matcher(nlp.vocab)\n",
    "full_names = extract_name(nlp(text), matcher=matchers)\n",
    "print(full_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 347,
     "status": "ok",
     "timestamp": 1731903245924,
     "user": {
      "displayName": "Giang Le Huynh",
      "userId": "12580968046014402630"
     },
     "user_tz": -420
    },
    "id": "F8OUwQUAeG-5",
    "outputId": "5dcb52aa-1172-41ec-8940-75cc12b2d069"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(424143666773229379, 0, 1),\n",
       " (424143666773229379, 1, 2),\n",
       " (424143666773229379, 8, 9),\n",
       " (424143666773229379, 10, 11)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"John Doe is a software developer. And Scatter Will Smith is an computer engineering.\"\n",
    "matchers = Matcher(nlp.vocab)\n",
    "matchers.add('NAME', [[{'POS': 'PROPN'}]])\n",
    "matchers(nlp(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q2-Q2hoye3oJ"
   },
   "source": [
    "## Extract link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 342,
     "status": "ok",
     "timestamp": 1731906989351,
     "user": {
      "displayName": "Giang Le Huynh",
      "userId": "12580968046014402630"
     },
     "user_tz": -420
    },
    "id": "wp0up6rUm2Lw",
    "outputId": "edaffdd4-3895-4c0d-dc46-892b18e693e8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://example.com\n",
      "https://mywebsite.org/about\n",
      "https://subdomain.example.net/products?category=books\n",
      "https://another-site.co.uk/contact?ref=home\n",
      "ftp://fileserver.com/path/to/file\n"
     ]
    }
   ],
   "source": [
    "# Example text containing domain names with paths and some with protocols\n",
    "text = \"\"\"\n",
    "Here are some domain names with and without protocols:\n",
    "http://example.com\n",
    "https://mywebsite.org/about\n",
    "subdomain.example.net/products?category=books\n",
    "another-site.co.uk/contact?ref=home\n",
    "ftp://fileserver.com/path/to/file\n",
    "\"\"\"\n",
    "\n",
    "# Regular expression to match domain names with optional paths or query parameters\n",
    "domain_with_path_pattern = r'\\b(?:[A-Za-z][A-Za-z0-9+.-]*://)?[A-Za-z0-9.-]+\\.[a-zA-Z]{2,}(?:/[A-Za-z0-9&%_\\-.?=]*)*\\b'\n",
    "\n",
    "# Extracting all domain names with or without protocols\n",
    "domains_with_paths = re.findall(domain_with_path_pattern, text)\n",
    "\n",
    "# Prepare the final list with https:// if no protocol exists\n",
    "final_links = []\n",
    "for link in domains_with_paths:\n",
    "    # If the link doesn't have a protocol (i.e., it doesn't start with 'http://', 'https://', etc.)\n",
    "    if not re.match(r'^[a-zA-Z][a-zA-Z0-9+.-]*://', link):  # No protocol\n",
    "        link = 'https://' + link  # Add https:// as the default protocol\n",
    "    final_links.append(link)\n",
    "\n",
    "# Print the final list of links\n",
    "for link in final_links:\n",
    "    print(link)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 332
    },
    "executionInfo": {
     "elapsed": 381,
     "status": "error",
     "timestamp": 1731912246481,
     "user": {
      "displayName": "Giang Le Huynh",
      "userId": "12580968046014402630"
     },
     "user_tz": -420
    },
    "id": "IJTHB4jvsKMy",
    "outputId": "6b7ec85b-6bda-4012-ba9c-ba7563e3ab7b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "example.com:\n",
      "  http://example.com\n",
      "mywebsite.org:\n",
      "  https://mywebsite.org/about\n",
      "subdomain.example.net:\n",
      "  https://subdomain.example.net/products?category=books\n",
      "another-site.co.uk:\n",
      "  https://another-site.co.uk/contact?ref=home\n",
      "fileserver.com:\n",
      "  ftp://fileserver.com/path/to/file\n"
     ]
    }
   ],
   "source": [
    "def extract_links(text):\n",
    "    # Regular expression to match domain names with optional protocols and paths\n",
    "    domain_with_path_pattern = cs.LINK_PATTERN\n",
    "    # Extract all the links (with or without protocol)\n",
    "    links = re.findall(domain_with_path_pattern, text)\n",
    "\n",
    "    # Dictionary to hold domain as key and list of links as value\n",
    "    domain_dict = defaultdict(list)\n",
    "\n",
    "    # Iterate through each link\n",
    "    for link in links:\n",
    "        # If the link doesn't have a protocol, add https:// by default\n",
    "        if not re.match(r'^[a-zA-Z][a-zA-Z0-9+.-]*://', link):  # No protocol\n",
    "            link = 'https://' + link\n",
    "\n",
    "        # Parse the link to get the domain name (without protocol)\n",
    "        parsed_url = urlparse(link)\n",
    "        domain_name = parsed_url.netloc\n",
    "\n",
    "        # Append the link to the list of the corresponding domain\n",
    "        domain_dict[domain_name].append(link)\n",
    "\n",
    "    return domain_dict\n",
    "\n",
    "# Example text containing domain names with paths and some with protocols\n",
    "text = \"\"\"\n",
    "Here are some domain names with and without protocols:\n",
    "http://example.com\n",
    "https://mywebsite.org/about\n",
    "subdomain.example.net/products?category=books\n",
    "another-site.co.uk/contact?ref=home\n",
    "ftp://fileserver.com/path/to/file\n",
    "\"\"\"\n",
    "\n",
    "# Call the function and get the domain dictionary\n",
    "domain_links = extract_links(text)\n",
    "\n",
    "# Print the resulting dictionary\n",
    "for domain, links in domain_links.items():\n",
    "    print(f\"{domain}:\")\n",
    "    for link in links:\n",
    "        print(f\"  {link}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GMerCNRFx6Nf"
   },
   "source": [
    "## Extract phone number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 347,
     "status": "ok",
     "timestamp": 1731908333068,
     "user": {
      "displayName": "Giang Le Huynh",
      "userId": "12580968046014402630"
     },
     "user_tz": -420
    },
    "id": "KkhU2Q36r8lN",
    "outputId": "91ff7482-be19-4b37-cad8-a7f0f9a81333"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+1-800-5555555\n",
      "(800) 5555555\n",
      "555 555 5555\n",
      "+44 7911 123456\n",
      "123.456.7890\n",
      "800-123-4567\n"
     ]
    }
   ],
   "source": [
    "def extract_phone_numbers(text):\n",
    "    # Regular expression to match phone numbers (supports various formats)\n",
    "    phone_pattern = r'(\\+?\\d{1,3}[-.\\s]?)?(\\(?\\d{1,4}\\)?[-.\\s]?)?(\\d{1,4})[-.\\s]?(\\d{1,4})[-.\\s]?(\\d{1,4})'\n",
    "\n",
    "    # Find all matches in the text\n",
    "    phone_numbers = re.findall(phone_pattern, text)\n",
    "\n",
    "    # Clean up the result to return full phone numbers (combining the matched parts)\n",
    "    full_phone_numbers = [''.join(number) for number in phone_numbers]\n",
    "\n",
    "    return full_phone_numbers\n",
    "\n",
    "# Example text with phone numbers\n",
    "text = \"\"\"\n",
    "You can reach me at +1-800-555-5555 or (800) 555-5555.\n",
    "My office number is 555 555 5555. Call me at +44 7911 123456.\n",
    "Another contact: 123.456.7890. Or just use 800-123-4567.\n",
    "\"\"\"\n",
    "\n",
    "# Extract phone numbers from the text\n",
    "phones = extract_phone_numbers(text)\n",
    "\n",
    "# Print the extracted phone numbers\n",
    "for phone in phones:\n",
    "    print(phone)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v0nI6Df2x-p5"
   },
   "source": [
    "## Extract education"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('BACHELOR', '2015'), ('MASTERS', '2020')]\n"
     ]
    }
   ],
   "source": [
    "# Simulated constants\n",
    "class cs:\n",
    "    EDUCATION = {\"BACHELOR\", \"MASTERS\", \"PHD\", \"MBA\", \"BSC\", \"MSC\"}\n",
    "    STOPWORDS = {\"AND\", \"OF\", \"THE\"}\n",
    "    YEAR = r'\\b(19|20)\\d{2}\\b'  # Regex pattern to match years like 1999, 2020\n",
    "\n",
    "# Function to test\n",
    "def extract_education(nlp_text):\n",
    "    '''\n",
    "    Helper function to extract education from spacy nlp text\n",
    "\n",
    "    :param nlp_text: object of `spacy.tokens.doc.Doc`\n",
    "    :return: tuple of education degree and year if year is found\n",
    "             else only returns education degree\n",
    "    '''\n",
    "    edu = {}\n",
    "    # Extract education degree\n",
    "    try:\n",
    "        for index, token in enumerate(nlp_text):\n",
    "            text_cleaned = re.sub(r'[?|$|.|!|,]', r'', token.text)\n",
    "            if text_cleaned.upper() in cs.EDUCATION and text_cleaned.upper() not in cs.STOPWORDS:\n",
    "                # Collect surrounding context (current and next 5 tokens for simplicity)\n",
    "                context = \" \".join([t.text for t in nlp_text[index:index + 6]])\n",
    "                edu[text_cleaned.upper()] = context\n",
    "    except IndexError:\n",
    "        pass\n",
    "\n",
    "    # Extract year\n",
    "    education = []\n",
    "    for key in edu.keys():\n",
    "        year = re.search(re.compile(cs.YEAR), edu[key])\n",
    "        if year:\n",
    "            education.append((key, year.group(0)))  # Include degree and year\n",
    "        else:\n",
    "            education.append(key)  # Include degree only\n",
    "    return education\n",
    "\n",
    "# Test with spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "test_text = \"John completed his Bachelor of Science in 2015 and his Masters in 2020.\"\n",
    "nlp_text = nlp(test_text)\n",
    "\n",
    "# Execute the function\n",
    "result = extract_education(nlp_text)\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract skill"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted Skills: ['Data analysis', 'Python', 'Machine learning', 'Python']\n"
     ]
    }
   ],
   "source": [
    "# Mock skills data\n",
    "skills_data = {\"Skill\": [\"python\", \"data analysis\", \"machine learning\", \"nlp\", \"deep learning\"]}\n",
    "skills_file_path = \"skills.csv\"\n",
    "\n",
    "# Save the mock skills data to a CSV file\n",
    "pd.DataFrame(skills_data).to_csv(skills_file_path, index=False)\n",
    "\n",
    "# Function to extract skills\n",
    "def extract_skills(nlp_text, noun_chunks, skills_file=None):\n",
    "    '''\n",
    "    Helper function to extract skills from spacy nlp text\n",
    "\n",
    "    :param nlp_text: object of `spacy.tokens.doc.Doc`\n",
    "    :param noun_chunks: noun chunks extracted from nlp text\n",
    "    :param skills_file: path to skills file (optional)\n",
    "    :return: list of skills extracted\n",
    "    '''\n",
    "    tokens = [token.text for token in nlp_text if not token.is_stop]\n",
    "    if not skills_file:\n",
    "        raise ValueError(\"Skills file is required.\")\n",
    "    else:\n",
    "        data = pd.read_csv(skills_file)\n",
    "    skills = list(data['Skill'].str.lower())  # Convert skills to lowercase for matching\n",
    "    skillset = []\n",
    "    # Check for one-grams\n",
    "    for token in tokens:\n",
    "        if token.lower() in skills:\n",
    "            skillset.append(token)\n",
    "    # Check for bi-grams and tri-grams\n",
    "    for chunk in noun_chunks:\n",
    "        chunk_text = chunk.text.lower().strip()\n",
    "        if chunk_text in skills:\n",
    "            skillset.append(chunk_text)\n",
    "    return [i.capitalize() for i in set(skillset)]\n",
    "\n",
    "# Load SpaCy model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Test text\n",
    "test_text = \"I have experience in Python, data analysis, and machine learning.\"\n",
    "nlp_text = nlp(test_text)\n",
    "noun_chunks = list(nlp_text.noun_chunks)\n",
    "\n",
    "# Test the function\n",
    "result = extract_skills(nlp_text, noun_chunks, skills_file_path)\n",
    "print(\"Extracted Skills:\", result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use model pre-train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\Lenovo'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "RegistryError",
     "evalue": "[E892] Unknown function registry: 'vectors'.\n\nAvailable names: architectures, augmenters, batchers, callbacks, cli, datasets, displacy_colors, factories, initializers, languages, layers, lemmatizers, loggers, lookups, losses, misc, models, ops, optimizers, readers, schedules, scorers, tokenizers",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRegistryError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mspacy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlearning/Resume Analyser/4.version/trainning/_model/output/model-best\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\resume_parser_project\\Lib\\site-packages\\spacy\\__init__.py:51\u001b[0m, in \u001b[0;36mload\u001b[1;34m(name, vocab, disable, enable, exclude, config)\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload\u001b[39m(\n\u001b[0;32m     28\u001b[0m     name: Union[\u001b[38;5;28mstr\u001b[39m, Path],\n\u001b[0;32m     29\u001b[0m     \u001b[38;5;241m*\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     34\u001b[0m     config: Union[Dict[\u001b[38;5;28mstr\u001b[39m, Any], Config] \u001b[38;5;241m=\u001b[39m util\u001b[38;5;241m.\u001b[39mSimpleFrozenDict(),\n\u001b[0;32m     35\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Language:\n\u001b[0;32m     36\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Load a spaCy model from an installed package or a local path.\u001b[39;00m\n\u001b[0;32m     37\u001b[0m \n\u001b[0;32m     38\u001b[0m \u001b[38;5;124;03m    name (str): Package name or model path.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[38;5;124;03m    RETURNS (Language): The loaded nlp object.\u001b[39;00m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 51\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mutil\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     52\u001b[0m \u001b[43m        \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     53\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvocab\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvocab\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     54\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdisable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdisable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     55\u001b[0m \u001b[43m        \u001b[49m\u001b[43menable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     56\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexclude\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexclude\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     57\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     58\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\resume_parser_project\\Lib\\site-packages\\spacy\\util.py:467\u001b[0m, in \u001b[0;36mload_model\u001b[1;34m(name, vocab, disable, enable, exclude, config)\u001b[0m\n\u001b[0;32m    465\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m load_model_from_package(name, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[0;32m    466\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m Path(name)\u001b[38;5;241m.\u001b[39mexists():  \u001b[38;5;66;03m# path to model data directory\u001b[39;00m\n\u001b[1;32m--> 467\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mload_model_from_path\u001b[49m\u001b[43m(\u001b[49m\u001b[43mPath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[0;32m    468\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(name, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexists\u001b[39m\u001b[38;5;124m\"\u001b[39m):  \u001b[38;5;66;03m# Path or Path-like to model data\u001b[39;00m\n\u001b[0;32m    469\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m load_model_from_path(name, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\resume_parser_project\\Lib\\site-packages\\spacy\\util.py:539\u001b[0m, in \u001b[0;36mload_model_from_path\u001b[1;34m(model_path, meta, vocab, disable, enable, exclude, config)\u001b[0m\n\u001b[0;32m    537\u001b[0m overrides \u001b[38;5;241m=\u001b[39m dict_to_dot(config, for_overrides\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    538\u001b[0m config \u001b[38;5;241m=\u001b[39m load_config(config_path, overrides\u001b[38;5;241m=\u001b[39moverrides)\n\u001b[1;32m--> 539\u001b[0m nlp \u001b[38;5;241m=\u001b[39m \u001b[43mload_model_from_config\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    540\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    541\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvocab\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvocab\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    542\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdisable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdisable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    543\u001b[0m \u001b[43m    \u001b[49m\u001b[43menable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    544\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexclude\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexclude\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    545\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmeta\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmeta\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    546\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    547\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m nlp\u001b[38;5;241m.\u001b[39mfrom_disk(model_path, exclude\u001b[38;5;241m=\u001b[39mexclude, overrides\u001b[38;5;241m=\u001b[39moverrides)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\resume_parser_project\\Lib\\site-packages\\spacy\\util.py:587\u001b[0m, in \u001b[0;36mload_model_from_config\u001b[1;34m(config, meta, vocab, disable, enable, exclude, auto_fill, validate)\u001b[0m\n\u001b[0;32m    584\u001b[0m \u001b[38;5;66;03m# This will automatically handle all codes registered via the languages\u001b[39;00m\n\u001b[0;32m    585\u001b[0m \u001b[38;5;66;03m# registry, including custom subclasses provided via entry points\u001b[39;00m\n\u001b[0;32m    586\u001b[0m lang_cls \u001b[38;5;241m=\u001b[39m get_lang_class(nlp_config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlang\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m--> 587\u001b[0m nlp \u001b[38;5;241m=\u001b[39m \u001b[43mlang_cls\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_config\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    588\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    589\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvocab\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvocab\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    590\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdisable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdisable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    591\u001b[0m \u001b[43m    \u001b[49m\u001b[43menable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    592\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexclude\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexclude\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    593\u001b[0m \u001b[43m    \u001b[49m\u001b[43mauto_fill\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mauto_fill\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    594\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    595\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmeta\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmeta\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    596\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    597\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m nlp\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\resume_parser_project\\Lib\\site-packages\\spacy\\language.py:1794\u001b[0m, in \u001b[0;36mLanguage.from_config\u001b[1;34m(cls, config, vocab, disable, enable, exclude, meta, auto_fill, validate)\u001b[0m\n\u001b[0;32m   1792\u001b[0m     filled[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpretraining\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m orig_pretraining\n\u001b[0;32m   1793\u001b[0m     config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpretraining\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m orig_pretraining\n\u001b[1;32m-> 1794\u001b[0m resolved_nlp \u001b[38;5;241m=\u001b[39m \u001b[43mregistry\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresolve\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1795\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfilled\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mnlp\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mschema\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mConfigSchemaNlp\u001b[49m\n\u001b[0;32m   1796\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1797\u001b[0m create_tokenizer \u001b[38;5;241m=\u001b[39m resolved_nlp[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1798\u001b[0m before_creation \u001b[38;5;241m=\u001b[39m resolved_nlp[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbefore_creation\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\resume_parser_project\\Lib\\site-packages\\confection\\__init__.py:760\u001b[0m, in \u001b[0;36mregistry.resolve\u001b[1;34m(cls, config, schema, overrides, validate)\u001b[0m\n\u001b[0;32m    751\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[0;32m    752\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mresolve\u001b[39m(\n\u001b[0;32m    753\u001b[0m     \u001b[38;5;28mcls\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    758\u001b[0m     validate: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    759\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Dict[\u001b[38;5;28mstr\u001b[39m, Any]:\n\u001b[1;32m--> 760\u001b[0m     resolved, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    761\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mschema\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mschema\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moverrides\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moverrides\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresolve\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[0;32m    762\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    763\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m resolved\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\resume_parser_project\\Lib\\site-packages\\confection\\__init__.py:809\u001b[0m, in \u001b[0;36mregistry._make\u001b[1;34m(cls, config, schema, overrides, resolve, validate)\u001b[0m\n\u001b[0;32m    807\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_interpolated:\n\u001b[0;32m    808\u001b[0m     config \u001b[38;5;241m=\u001b[39m Config(orig_config)\u001b[38;5;241m.\u001b[39minterpolate()\n\u001b[1;32m--> 809\u001b[0m filled, _, resolved \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fill\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    810\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mschema\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moverrides\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moverrides\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresolve\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresolve\u001b[49m\n\u001b[0;32m    811\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    812\u001b[0m filled \u001b[38;5;241m=\u001b[39m Config(filled, section_order\u001b[38;5;241m=\u001b[39msection_order)\n\u001b[0;32m    813\u001b[0m \u001b[38;5;66;03m# Check that overrides didn't include invalid properties not in config\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\resume_parser_project\\Lib\\site-packages\\confection\\__init__.py:863\u001b[0m, in \u001b[0;36mregistry._fill\u001b[1;34m(cls, config, schema, validate, resolve, parent, overrides)\u001b[0m\n\u001b[0;32m    861\u001b[0m     field \u001b[38;5;241m=\u001b[39m schema\u001b[38;5;241m.\u001b[39m__fields__[key]\n\u001b[0;32m    862\u001b[0m     schema\u001b[38;5;241m.\u001b[39m__fields__[key] \u001b[38;5;241m=\u001b[39m copy_model_field(field, Any)\n\u001b[1;32m--> 863\u001b[0m promise_schema \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmake_promise_schema\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresolve\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresolve\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    864\u001b[0m filled[key], validation[v_key], final[key] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_fill(\n\u001b[0;32m    865\u001b[0m     value,\n\u001b[0;32m    866\u001b[0m     promise_schema,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    870\u001b[0m     overrides\u001b[38;5;241m=\u001b[39moverrides,\n\u001b[0;32m    871\u001b[0m )\n\u001b[0;32m    872\u001b[0m reg_name, func_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mget_constructor(final[key])\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\resume_parser_project\\Lib\\site-packages\\confection\\__init__.py:1069\u001b[0m, in \u001b[0;36mregistry.make_promise_schema\u001b[1;34m(cls, obj, resolve)\u001b[0m\n\u001b[0;32m   1067\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m resolve \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mhas(reg_name, func_name):\n\u001b[0;32m   1068\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m EmptySchema\n\u001b[1;32m-> 1069\u001b[0m func \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreg_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1070\u001b[0m \u001b[38;5;66;03m# Read the argument annotations and defaults from the function signature\u001b[39;00m\n\u001b[0;32m   1071\u001b[0m id_keys \u001b[38;5;241m=\u001b[39m [k \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m obj\u001b[38;5;241m.\u001b[39mkeys() \u001b[38;5;28;01mif\u001b[39;00m k\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m@\u001b[39m\u001b[38;5;124m\"\u001b[39m)]\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\resume_parser_project\\Lib\\site-packages\\spacy\\util.py:151\u001b[0m, in \u001b[0;36mregistry.get\u001b[1;34m(cls, registry_name, func_name)\u001b[0m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mcls\u001b[39m, registry_name):\n\u001b[0;32m    150\u001b[0m     names \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mget_registry_names()) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnone\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 151\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m RegistryError(Errors\u001b[38;5;241m.\u001b[39mE892\u001b[38;5;241m.\u001b[39mformat(name\u001b[38;5;241m=\u001b[39mregistry_name, available\u001b[38;5;241m=\u001b[39mnames))\n\u001b[0;32m    152\u001b[0m reg \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mcls\u001b[39m, registry_name)\n\u001b[0;32m    153\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[1;31mRegistryError\u001b[0m: [E892] Unknown function registry: 'vectors'.\n\nAvailable names: architectures, augmenters, batchers, callbacks, cli, datasets, displacy_colors, factories, initializers, languages, layers, lemmatizers, loggers, lookups, losses, misc, models, ops, optimizers, readers, schedules, scorers, tokenizers"
     ]
    }
   ],
   "source": [
    "model = spacy.load('learning/Resume Analyser/4.version/trainning/_model/output/model-best')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test example\n",
    "main_str = ['''Manufacturing Production Manager Resume\n",
    "Desired Industry Manufacturing\n",
    "SpiderID 78692\n",
    "Desired Job Location Windsor Colorado\n",
    "Date Posted 1 23 2017\n",
    "Type of Position Full Time Permanent\n",
    "Availability Date Immediately\n",
    "Desired Wage 95000\n",
    "U S Work Authorization Yes\n",
    "Job Level Management Manager Director\n",
    "Willing to Travel\n",
    "Highest Degree Attained Other\n",
    "Willing to Relocate Yes\n",
    "Objective Dynamic and growth driven professional offering hands on management experience and comprehensive background in manufacturing and engineering operations within highly competitive setting Adept at reengineering unproductive work processes as well as in planning and implementing various sustainable and cost effective work programs to drive continuous improvement of operations Armed with exceptional organizational and critical problem solving aptitudes to formulate effective solutions on complex production and quality issues Equipped with tactical leadership capabilities in supervising and guiding teams toward the successful and timely completion of projects Proficient with Microsoft Office applications Project Management and Microsoft Visio\n",
    "Experience Relevant ExperienceVestas Blades America Windsor COProduction Engineer Jan 2014 PresentContribute efforts in achieving production plan budget tooling equipment and bill of material along with quality control and safety Conceptualized new methods to optimize production levels while maintaining production costs yields quality and safety Identified and resolved process problems with effective solutions which decreased downtime and minimized costs Initiated plant trials to measure performance capabilities while ensuring updated documentation of process procedures Leveraged industry expertise in streamlining the manufacturing of turbine blades Production Supervisor Mar 2009 Jan 2014Rendered oversight to more than 70 employees to guarantee accordance of operation with production plan and goals Observed strict compliance with safety and quality guidelines and handled inventory control created schedules delegated work and facilitated training of staff Generated production reports for production and operations managers regarding production areas performance Served as a Shells Lighthouse project member while drafting and modifying all standard operating procedures Functioned as department lead for production quality training and process improvements Closely monitored operations productivity to determine areas for improvement in overall production process Pioneered the development of all shells production process job cards that decreased process times and improved efficiency and quality through changed production flow layout Anheuser Busch Fort Collis COBrew House Control Panel Operator Mar 2007 Mar 2009Efficiently administered beer brewing process from raw material selection and recipe formulation through the use of Siemens software Strictly enforced standard operating procedures and safe working practices Determined and evaluated all critical control points to achieve consistent product quality within allotted schedules Other ExperienceUnited States Airways Denver CoCustomer Service Representative Kroger Co King Soopers Smiths Food Drug Fort Collins CoGrocery Manager Head Clerk Night Crew Manager Front End ManagerInventory Control Manager Warehouse Manager\n",
    "Education EducationAssociate of Science with emphasis in chemistry and biologyFront Range Community College Fort Collins COPharmacy Pre Requisites for Doctor of Pharmacy Pharm D University of Wyoming Laramie WYPharmacy Pre Requisites for Doctor of Pharmacy Pharm D Western Wyoming Community College Rock Springs WY Deans Honor RollProfessional DevelopmentCertificationsSupervisor Certification Six Sigma Yellow Belt Project ManagementQuality Management Crucial Conversations Fort Lift License Crane License\n",
    "Affiliations\n",
    "Skills Manufacturing Production Manager Project Planning Cost Reduction and Budget Optimization Resource Allocation Six Sigma Quality ControlLean Manufacturing Plant Management Manufacturing Inspection Administration Cross functional Team Building\n",
    "Additional Information TrainingProduction Instructor Coordinator Planner Coordinator Wrote Training Document\n",
    "Reference Available upon request\n",
    "Candidate Contact Information\n",
    "JobSpider com has chosen not to make contact information available on this page Click Contact Candidate to send this candidate a response\n",
    "Manufacturing Production Manager Resume\n",
    "Desired Industry Manufacturing\n",
    "SpiderID 78692\n",
    "Desired Job Location Windsor Colorado\n",
    "Date Posted 1 23 2017\n",
    "Type of Position Full Time Permanent\n",
    "Availability Date Immediately\n",
    "Desired Wage 95000\n",
    "U S Work Authorization Yes\n",
    "Job Level Management Manager Director\n",
    "Willing to Travel\n",
    "Highest Degree Attained Other\n",
    "Willing to Relocate Yes\n",
    "Objective Dynamic and growth driven professional offering hands on management experience and comprehensive background in manufacturing and engineering operations within highly competitive setting Adept at reengineering unproductive work processes as well as in planning and implementing various sustainable and cost effective work programs to drive continuous improvement of operations Armed with exceptional organizational and critical problem solving aptitudes to formulate effective solutions on complex production and quality issues Equipped with tactical leadership capabilities in supervising and guiding teams toward the successful and timely completion of projects Proficient with Microsoft Office applications Project Management and Microsoft Visio\n",
    "Experience Relevant ExperienceVestas Blades America Windsor COProduction Engineer Jan 2014 PresentContribute efforts in achieving production plan budget tooling equipment and bill of material along with quality control and safety Conceptualized new methods to optimize production levels while maintaining production costs yields quality and safety Identified and resolved process problems with effective solutions which decreased downtime and minimized costs Initiated plant trials to measure performance capabilities while ensuring updated documentation of process procedures Leveraged industry expertise in streamlining the manufacturing of turbine blades Production Supervisor Mar 2009 Jan 2014Rendered oversight to more than 70 employees to guarantee accordance of operation with production plan and goals Observed strict compliance with safety and quality guidelines and handled inventory control created schedules delegated work and facilitated training of staff Generated production reports for production and operations managers regarding production areas performance Served as a Shells Lighthouse project member while drafting and modifying all standard operating procedures Functioned as department lead for production quality training and process improvements Closely monitored operations productivity to determine areas for improvement in overall production process Pioneered the development of all shells production process job cards that decreased process times and improved efficiency and quality through changed production flow layout Anheuser Busch Fort Collis COBrew House Control Panel Operator Mar 2007 Mar 2009Efficiently administered beer brewing process from raw material selection and recipe formulation through the use of Siemens software Strictly enforced standard operating procedures and safe working practices Determined and evaluated all critical control points to achieve consistent product quality within allotted schedules Other ExperienceUnited States Airways Denver CoCustomer Service Representative Kroger Co King Soopers Smiths Food Drug Fort Collins CoGrocery Manager Head Clerk Night Crew Manager Front End ManagerInventory Control Manager Warehouse Manager\n",
    "Education EducationAssociate of Science with emphasis in chemistry and biologyFront Range Community College Fort Collins COPharmacy Pre Requisites for Doctor of Pharmacy Pharm D University of Wyoming Laramie WYPharmacy Pre Requisites for Doctor of Pharmacy Pharm D Western Wyoming Community College Rock Springs WY Deans Honor RollProfessional DevelopmentCertificationsSupervisor Certification Six Sigma Yellow Belt Project ManagementQuality Management Crucial Conversations Fort Lift License Crane License\n",
    "Affiliations\n",
    "Skills Manufacturing Production Manager Project Planning Cost Reduction and Budget Optimization Resource Allocation Six Sigma Quality ControlLean Manufacturing Plant Management Manufacturing Inspection Administration Cross functional Team Building\n",
    "Additional Information TrainingProduction Instructor Coordinator Planner Coordinator Wrote Training Document\n",
    "Reference Available upon request\n",
    "Candidate Contact Information\n",
    "JobSpider com has chosen not to make contact information available on this page Click Contact Candidate to send this candidate a response''' ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for doc in model.pipe(main_str, disable=[\"tagger\", \"parser\"]):\n",
    "  for ent in doc.ents:\n",
    "    print((ent.text,ent.label_))\n",
    "    # print([(ent.text, ent.label_) for ent in doc.ents])\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPkAU8dUR8yw87X90Q5ysWj",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
